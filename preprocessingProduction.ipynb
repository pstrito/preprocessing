{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program gives the user 3 different cloud source data platforms to choose from:\n",
    "    finviz\n",
    "    stocktwits\n",
    "    reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File name: preprocessingProduction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import nltk\n",
    "#from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#import nltk.classify\n",
    "#from nltk import NaiveBayesClassifier\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import time\n",
    "#analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************\n",
    "Table of Contents\n",
    "#10* initializes the dataframe \"df\" and imports the csv into df; \n",
    "#20* calls getdata to import the csv into the dataframe, 'dfAPI'\n",
    "#30 removes any duplicate records; duplicate records imply bot records\n",
    "#40 finds certain words in the strings ('body') and deletes the entire record.  \n",
    "#50* Vader sentiment analyzer\n",
    "#60* creates a new column called 'compound_bin' from the raw_compound scores\n",
    "#70* converts the 'raw_compound' data to either a 1, 0 or -1. 1 if nltk sentiment number are >= .1; 0 if -.1 < x < .1 \n",
    "#80* Converts sentiment ratings into numerical values and put the value into 'sentiment_number'.\n",
    "#90 Determines the percent correct and incorrect for the Vader sentiment values vs the stocktwits sentiment values\n",
    "#100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "#110 This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "#115 Provides statistics on sentiments; bullish, none or bearish.\n",
    "#120 Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "#130 Loads a csv file into the df dfAPI and print out the first 21 records\n",
    "#140 This will change the modified rating to the nltk rating only when they are opposite to see if it improves \n",
    "the accuracy number \n",
    "#440 sets up stopword removal; returns stopWords\n",
    "#470 creates a list of new stopwords and then adds them to the set provided by nltk\n",
    "Note: it is case sensitive; Input is the nltk stopword list (\"stopWords\")\n",
    "#490 Checks to see of the words were removed from the stopWords list.\n",
    "inputs: stopword list: output from def remove_from_stopwords(sw); the word to be removed\n",
    "#510 Removes stopwords from all the \"body\" text (tweets); to do this it must tokenize the string which means it must parse \n",
    "the string into individual words. It then compares the words with the words in the stopwords list and if there is not \n",
    "match it puts the word into the \"wordsFiltered\" list. It keeps appending to the list until all of the words are checked.\n",
    "It then joins the individual words back into a string.\n",
    "There is a difference between \"deep\" copy and \"shallow\" copy. \"Deep\" copy make a copy where the index and data are\n",
    "separate from the original. \"Shallow\" copy is like a pointer where the two df share a common index and data\n",
    "dfScrubbed = df #This is a shallow copy\n",
    "#550 converts the scrubbed_compound scores into a 1 significant figure integer from a float number; rounding up\n",
    "this is only needed if you are going to uses the 'scrubbed_compound' value as the label.\n",
    "#550 converts the 'scrubbed_compound' (column 10) data to either a 1, 0 or -1.  \n",
    "if nltk sentiment number are >= .1; 0 if -.1 < x < .1 and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "creates a new column called 'compound_bin' from the raw_compound scores\n",
    "#640 compares the first record (index = 0) raw data (\"body\" column) with scrubbed (stopwords removed) data\n",
    "inputs: df - original df; dfs - scrubbed df (stopwords removed)\n",
    "#650 Loads and combines two different dataframes in df; this is to combine two input datasets where the 'none'\n",
    "values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "#660 Writes a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "\n",
    "class SentimentAnalysisPreprocessing():\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    # 10 initializes the dataframe \"df\" and imports the csv into df; \n",
    "    # the argument is the name/address of the file.\n",
    "    # https://stackoverflow.com/questions/33440805/pandas-dataframe-read-csv-on-bad-data\n",
    "    def getData(self, name):\n",
    "        df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "        df1 = pd.read_csv(name, header = 0)\n",
    "        return df1\n",
    "\n",
    "    # removes duplicate headers\n",
    "    def remove_duplicate_headers(self, df):\n",
    "        print('\\nDropping duplicate headers ...')\n",
    "        column = 'symbol'\n",
    "        %time df.drop(df[df['symbol'] == column].index, inplace=True)\n",
    "        df = df.reset_index(drop = True) # resets the index\n",
    "        return df\n",
    "\n",
    "    # 30 removes any duplicate records; duplicate records imply bot records\n",
    "    def remove_duplicates(self, df):\n",
    "        print('\\nDropping duplicates ...')\n",
    "        %time df = df.drop_duplicates()\n",
    "        df = df.reset_index(drop = True) # resets the index\n",
    "        len(df)\n",
    "        return df\n",
    "\n",
    "    # remove HTTP tags\n",
    "    def remove_http_tags(self, df):\n",
    "        print('\\nRemoving http tags ...')\n",
    "        %time df['body_processed'] = df['body'].map(lambda x : ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))\n",
    "        return df\n",
    "\n",
    "    # coverts to all lower case\n",
    "    def lower_case(self, df):\n",
    "        print('\\nConverting to lower case ...')\n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x: x.lower())\n",
    "        return df\n",
    "\n",
    "    # removes all punctuation\n",
    "    def remove_punctuation(self, df):\n",
    "        print('\\nRemoving punctuation ...')\n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "        return df\n",
    "\n",
    "    # removes unicodes (emojis)\n",
    "    def remove_unicode(self, df):\n",
    "        print('\\nRemoving unicode ...')\n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "        return df\n",
    "\n",
    "    def lemmatize(self, df, stop_words): #lemmer must be defined outside of the function and passed in\n",
    "        print('\\nLemmatizing ...')\n",
    "        \n",
    "        import nltk\n",
    "        import re\n",
    "        from bs4 import BeautifulSoup\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        \n",
    "        #from nltk.corpus import stopwords\n",
    "        #stop_words = stopwords.words('english')\n",
    "        #nltk.download('wordnet') #not in original code\n",
    "\n",
    "        # Lemmatize the text\n",
    "        lemmer = WordNetLemmatizer()\n",
    "        \n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Remove stopwords\n",
    "    def remove_stopwords(self, df, stop_words): #stop_words must be defined outside of the function and passed in\n",
    "        print('\\nRemoving stopwords ...')\n",
    "\n",
    "        #adds new stopwords to list\n",
    "\n",
    "        newStopWords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "        newStopWords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "        newStopWords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "        newStopWords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "        newStopWords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "        newStopWords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "        newStopWords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "        newStopWords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "        newStopWords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "        newStopWords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "        newStopWords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "        newStopWords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "        newStopWords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "        newStopWords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "        newStopWords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "        newStopWords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "        newStopWords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "        newStopWords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "        newStopWords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "        newStopWords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "        newStopWords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "        newStopWords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "        newStopWords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "        newStopWords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "        newStopWords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "        newStopWords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "        newStopWords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "        newStopWords += ['nevertheless', 'next', 'nine', 'nobody', 'none'] #removed 'no'\n",
    "        newStopWords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "        newStopWords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "        newStopWords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "        newStopWords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "        newStopWords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "        newStopWords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "        newStopWords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "        newStopWords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "        newStopWords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "        newStopWords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "        newStopWords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "        newStopWords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "        newStopWords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "        newStopWords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "        newStopWords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "        newStopWords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "        newStopWords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "        newStopWords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "        newStopWords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "        newStopWords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "        newStopWords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "        newStopWords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "        newStopWords += ['yours', 'yourself', 'yourselves'] #provided by Codecademy??\n",
    "\n",
    "        # additional stopwords:\n",
    "        newStopWords += ['[screenshot]', 'screenshot', '[screenshot]great', 'screenshot',\n",
    "                         'the', 'smart', 'yah', 'got', 'nutty', 'moving', 'weeks', 'Got', 'So', 'today', 'Been', 'or']\n",
    "\n",
    "        newStopWords += ['i', 'you', 'He', 'he', 'she', 'they', 'their', 'it'] # pronouns\n",
    "\n",
    "        newStopWords += ['amd','nvda', 'tsla', 'goog', 'ba', 'fb', 'googl', 'intc', 'intel', 'csco', 'mu',\n",
    "                         'smh', 'tsm','aapl', 'csco', 'poetf', 'photonics', 'dd', 'arwr', 't', 'infini', 'amc', 'arl',\n",
    "                         'gme', 'nio', 'qs', 'msft', 'adbe', 'unh'] # Stock symbols or names\n",
    "\n",
    "        newStopWords += [] # nouns\n",
    "\n",
    "        #newStopWords += ['.', '?', '!', ';', ',', \"'\", '.'] # punctuation\n",
    "\n",
    "        newStopWords += ['&', '#', '%', '$', '@', '/'] # symbols\n",
    "\n",
    "        newStopWords += ['41.75', '530.05', '39', 'Two', 'two', 'One', 'one', 'Three', 'three', 'Four', 'four',\n",
    "                        'Five', 'five', 'Six', 'six', 'Seven', 'seven', 'Eight', 'eight', 'Nine', 'nine', 'Ten',\n",
    "                        'ten', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '39', ' 270',\n",
    "                          '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "                          '1600', '993', '392', '98', '00', '1601'] # numbers\n",
    "\n",
    "        for w in newStopWords:\n",
    "            stop_words.append(w)\n",
    "\n",
    "        #print('stop_words: ', stop_words)\n",
    "\n",
    "        #removes the stopwords from the column body_processed\n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 40 finds certain words in the strings ('body') and deletes the entire record.\n",
    "    #Note: When the record is deleted the df is re-indexed. The index for the while statement is not so the result is\n",
    "    #that the record right after the deleted record is skipped. To remedy the problem the index (i) for the while statement \n",
    "    #is decremented by one.\n",
    "    #Also, the filtering terms are not case sensitive.\n",
    "    def filter_records(self, df):\n",
    "        import fnmatch\n",
    "\n",
    "        data = []\n",
    "        counter = 0\n",
    "        advert = ['* sec *', '* daily News *', '*Huge Print*', '* Form *', \n",
    "                  '*SweepCast*', '*Large Print*', '*Huge Print*', '*8-K*', \n",
    "                  '*SmartOptions*', '*Big Trade*', '*SEC Form*', '*Notice of Exempt*', \n",
    "                  '*created_at*', '*stock news*', '*Trading Zones*', '*Entry:*', \n",
    "                  '*New Article*', '*ooc.bz*', '*http*', 'Huge Trade', 'Trading is easy', \n",
    "                  'www.', '#wallstreetbets', 'wallstreetbets', 'Huge Trade', '#unitedtraders', \n",
    "                  'stockbeep.com', 'Big Trade'] # words or phrases whose records are to be removed; It is not case sensitive.\n",
    "\n",
    "        for a in advert:\n",
    "            i = 0\n",
    "            df = df.reset_index(drop = True) # resets the index before each iteration; removes the gaps; resets len(df)\n",
    "            while i < len(df):\n",
    "                dat = df.loc[i, ('body')] # 2 represents the 'body' column\n",
    "                data = [dat] # sets the string from the df into a list for the fnmatch.filter\n",
    "                #print('index = ', i)\n",
    "                filtered = fnmatch.filter(data, a) # compares the information in the 'body' column with the 'advert' list; it places the matched items in the 'filtered' variable.\n",
    "                #https://www.geeksforgeeks.org/fnmatch-unix-filename-pattern-matching-python/\n",
    "\n",
    "                if len(filtered) != 0: #if returns a True then record needs to be removed\n",
    "                    counter += 1\n",
    "\n",
    "                    df = df.drop(df.index[i]) # drops (deletes) the record\n",
    "                    df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "\n",
    "                    #print('after the record is dropped:', df..log[i,('body')], 'i = ', i)\n",
    "\n",
    "                    #Note: When the record is dropped there is a change in the 'index' number. after the drop index number\n",
    "                    #5 becomes index number 4. Since the counter increments one more time it skips the record right after\n",
    "                    #the record that was just checked. That is why it takes multiple runs to remove all of the target\n",
    "                    #records. To correct this decrement the index, i, by\n",
    "\n",
    "                    i -= 1\n",
    "\n",
    "                i += 1\n",
    "\n",
    "        df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "        len(df)\n",
    "        return df\n",
    "\n",
    "\n",
    "    # 120 Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "    # It counts every 20 edits and gives the user the option to quit. If the user chooses to quit\n",
    "    # it breaks from the while look and writes the df to a csv file so all work is saved up to that point.\n",
    "    # upon start up it ask if thie is the first time processing the raw data. If no it loads the csv file into\n",
    "    # the dataframe and starts where the previous session left off. If \"modified?\" is \"Yes and \"sentiment\" is \"None\"\n",
    "    # it skips the record. Therefore it will re-start at the first \"modified?\" is \"No\" and \"sentiment\" is \"None\"\n",
    "    def edit(self, df):\n",
    "\n",
    "        import copy\n",
    "\n",
    "        i = 0\n",
    "        counter = 0    # counter to see if user want to stop\n",
    "\n",
    "        while i < len(df):\n",
    "        #while i < 6:\n",
    "\n",
    "            if df.loc[i,('sentiment')] == 'None' and df.loc[i,('modified?')] == 'No': # Column 9 is 'modified?'\n",
    "                print('\\nindex number:', i, '\\n', df.loc[i, ('body')])\n",
    "                #print('This is the body of the tweet:\\n', df..log[i,('body')])\n",
    "                rating = int(input('Enter your rating (1, 0 or -1.):')) \n",
    "                df.loc[i,('modified_rating')] = copy.deepcopy(rating) # writes inputed number to the 'modified_rating'\n",
    "                df.loc[i,('modified?')] = 'Yes' # sets \"modified?\" equal to 'Yes' to identify which records have been modified; so that it can start at the next record at start up\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "            elif df.loc[i,('sentiment')] == 'Bearish':\n",
    "\n",
    "                df.loc[i,('modified_rating')] = df.loc[i,('sentiment_number')] #copies the stocktwits 'sentiment_number' (7) to the 'modified_rating(8)\n",
    "\n",
    "            elif df.loc[i,('sentiment')] == 'Bullish':\n",
    "\n",
    "                df.loc[i,('modified_rating')] = df.loc[i,('sentiment_number')] #copies the stocktwits 'sentiment_number' (7) to the 'modified_rating(8)\n",
    "\n",
    "            if counter == 20: # represents 20 edits\n",
    "                quit = input('Do you want to quit? (Enter either a \"y\" or \"Y\") ')\n",
    "                if quit == 'y' or quit == 'Y':\n",
    "                    print('You are exiting.')\n",
    "                    break\n",
    "                else:\n",
    "                    counter = 0 # resets the counter to 0 so there must be another 20 records reviewed and modified \n",
    "\n",
    "            i += 1\n",
    "\n",
    "        #df.to_csv(filename, index = False)\n",
    "        #print('The csv file was written. File name: ', filename)\n",
    "\n",
    "        return df\n",
    "\n",
    "    #480 This removes words from the list of stopwords and writes list to csv file\n",
    "    # https://stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python#:~:text=one%20more%20easy%20way%20to%20remove%20words%20from,%3D%20words%20-%20stopwords%20final_list%20%3D%20list%20%28final_list%29\n",
    "    #new_words = list(filter(lambda w: w not in stop_words, initial_words))\n",
    "    def remove_from_stopwords(self, sw, relevant_path):\n",
    "        WordsToBeRem = ['no']\n",
    "        stopWords = list(filter(lambda w: w not in WordsToBeRem, sw)) #It will retain anyword in sw that is not in WordsToBeRemoved\n",
    "\n",
    "        #converts the stopword list to a df so that it can then be written to a csv file\n",
    "        df_stopwords = pd.DataFrame(stopWords, columns = ['stopwords'])\n",
    "        name_of_csv_file = relevant_path + '/' + 'stopwords.csv'\n",
    "        df_stopwords.to_csv(name_of_csv_file, index = False) #writes stopwords to csv file\n",
    "\n",
    "        #print(stopWords)\n",
    "\n",
    "        return stopWords\n",
    "\n",
    "    #490 Checks to see of the words were removed from the stopWords list.\n",
    "    #inputs: stopword list (sw) and the word to be removed from the so (WordToBeRem):\n",
    "    def check_stopwords(self, sw, WordToBeRem):\n",
    "\n",
    "        r = 0\n",
    "\n",
    "        for w in sw:\n",
    "            #print(w)\n",
    "            if w == WordToBeRem:\n",
    "                print('The word ', w , ' is still in the stopWords list!')\n",
    "                r += 1\n",
    "\n",
    "        if r == 0:\n",
    "            print('It did remove the words from the stopWords list!')\n",
    "\n",
    "        #print(len(stopWords))\n",
    "\n",
    "    #510 Removes stopwords from all the \"body\" text (tweets); to do this it must tokenize the string which means it must parse \n",
    "    # the string into individual words. It then compares the words with the words in the stopwords list and if there is not \n",
    "    # match it puts the word into the \"wordsFiltered\" list. It keeps appending to the list until all of the words are checked.\n",
    "    # It then joins the individual words back into a string.\n",
    "    #There is a difference between \"deep\" copy and \"shallow\" copy. \"Deep\" copy make a copy where the index and data are\n",
    "    # separate from the original. \"Shallow\" copy is like a pointer where the two df share a common index and data\n",
    "    #dfScrubbed = df #This is a shallow copy\n",
    "    def rem_stopwords(self, df, stopWords):\n",
    "\n",
    "        from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "        dfScrubbed = df.copy() #This is a deep copy. df.copy(deep = True); deep = True is default\n",
    "\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "\n",
    "            data = df.loc[i,('body')]\n",
    "            words = word_tokenize(data) # separates the string into a individual words.\n",
    "            wordsFiltered = []\n",
    "\n",
    "            for w in words:\n",
    "                if w not in stopWords:\n",
    "                    wordsFiltered.append(w) # makes a new word list without the stopwords\n",
    "\n",
    "            joinedWordsFiltered = ' '.join(wordsFiltered)\n",
    "\n",
    "            dfScrubbed.loc[i,('body')] = joinedWordsFiltered # replaces the recorded in dfScrubbed with the stopWords removed\n",
    "            # from the 'body'\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        #print(wordsFiltered)\n",
    "\n",
    "        #### method removes empty body rows and reindexes\n",
    "        dfScrubbed = remove_empty_body_rows(dfScrubbed)\n",
    "\n",
    "        #### checks to see if there are any empty records left\n",
    "        print('Are there any empty body records?')\n",
    "        empty = np.where(pd.isnull(dfScrubbed['body'])) #checks to see if there are any empty records in the column 'body'\n",
    "        print(empty)\n",
    "\n",
    "        #print(dfScrubbed.head())\n",
    "\n",
    "        return dfScrubbed\n",
    "\n",
    "       #640 compares the first record (index = 0) raw data (\"body\" column) with scrubbed (stopwords removed) data\n",
    "    #inputs: df - original df; dfs - scrubbed df (stopwords removed)\n",
    "    def compare_scrubbed(self, df, dfs):\n",
    "        print(df.loc[0,('body')])\n",
    "        print(dfs.loc[0,('body')])\n",
    "\n",
    "    # 650 Loads and combines two different dataframes in df; this is to combine two input datasets where the 'none'\n",
    "    #values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "    def combine_dfs(self, df1, df2):\n",
    "\n",
    "        df = df1.append(df2)\n",
    "\n",
    "        print('The length of file 1 is:', len(df1))\n",
    "        print('The length of file 2 is:', len(df2))\n",
    "\n",
    "        print('The length of the combined dataframe is:', len(df))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 660 Writes a csv file\n",
    "    #input: df that is to be saved as a csv; output file name (eg 'tech stockTwit 03112021 dup advert stopwords.csv'\n",
    "    def write_csv(self, df, filename_output, relevant_path):\n",
    "\n",
    "        df.to_csv(relevant_path + '/' + filename_output, index = False, encoding = 'utf-8')\n",
    "        print('The csv file was written. File name: ', filename_output)\n",
    "\n",
    "    # displays a list of file with on a csv suffix       \n",
    "    def list_dir_files(self, relevant_path):\n",
    "        # https://clay-atlas.com/us/blog/2019/10/27/python-english-tutorial-solved-unicodeescape-error-escape-syntaxerror/?doing_wp_cron=1618286551.1528689861297607421875\n",
    "        #need to change \\ to /\n",
    "\n",
    "        import os\n",
    "\n",
    "        included_extensions = ['csv']\n",
    "        file_names = [fn for fn in os.listdir(relevant_path) # uses os.listdir to display only .csv files\n",
    "                  if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "        print('Path: ', relevant_path)\n",
    "\n",
    "        for f in file_names:\n",
    "            print(f)\n",
    "\n",
    "    # removes specific rows and resets the index\n",
    "    def remove_empty_body_rows(self, df):\n",
    "        df.dropna(subset=['body'], inplace=True) #drops empty body records\n",
    "        df = df.reset_index(drop = True) # resets the index\n",
    "        return df\n",
    "\n",
    "    #### checks to see if there are any empty records left\n",
    "    def empty_records_check(self, df):\n",
    "        print('Are there any empty body records?')\n",
    "        empty = np.where(pd.isnull(df['body'])) #checks to see if there are any empty records in the column 'body'\n",
    "\n",
    "        if empty[0].size == 0:\n",
    "            print('There are no empty records! \\n', empty)\n",
    "        else:\n",
    "            print('There are empty records ...\\n', empty)\n",
    "\n",
    "    #### Removes Imogis\n",
    "    def remove_emoji(self, string):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   u\"\\U0001f926-\\U0001f937\"\n",
    "                                   u\"\\U00010000-\\U0010ffff\"\n",
    "                                   u\"\\u2640-\\u2642\"\n",
    "                                   u\"\\u2600-\\u2B55\"\n",
    "                                   u\"\\u200d\"\n",
    "                                   u\"\\u23cf\"\n",
    "                                   u\"\\u23e9\"\n",
    "                                   u\"\\u231a\"\n",
    "                                   u\"\\ufe0f\"  # dingbats\n",
    "                                   u\"\\u3030\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', string)\n",
    "\n",
    "    # combines both names of file wanted to combing and writes csv file\n",
    "    def combine_two_files(self):\n",
    "        first_filename = input()\n",
    "\n",
    "    def rem_dup_adver_ever_oth_emoji(self, df):\n",
    "        #remove duplicates\n",
    "        r_d = input('Do you want to remove duplicates? [Press enter if no] ')\n",
    "        if r_d in yes_resp:\n",
    "            df = remove_duplicates(df) #return df; removes duplicates\n",
    "            remove_dupl = 'r_d '\n",
    "        else:\n",
    "            remove_dupl = ''\n",
    "\n",
    "        #remove advertisements\n",
    "        r_a = input('Do you want to remove advertisements? [Press enter if no] ')\n",
    "        if r_a in yes_resp:\n",
    "            df = filter_records(df) #returns df; removes addvertisements\n",
    "            remove_advertisements = 'r_a '\n",
    "        else:\n",
    "            remove_advertisements = ''\n",
    "\n",
    "        # remove emojis\n",
    "        r_emoj = input('Do you want to remove emojis from the body records: [Press enter if no] ')\n",
    "        if r_emoj in yes_resp:\n",
    "            #print('location1')\n",
    "            i = 0\n",
    "            #print('location2')\n",
    "            while i < len(df):\n",
    "                #print('location3', i)\n",
    "                string = df.loc[i, ('body')]\n",
    "                #print('location4')\n",
    "                #print('original string: ', string)\n",
    "                new_string = remove_emoji(string)\n",
    "                #print('location5')\n",
    "                #print('new string: ', new_string)\n",
    "                df.loc[i, ('body')] = new_string\n",
    "                #print(df['body'][i])\n",
    "\n",
    "                r_emoji = 'r_emoj '\n",
    "\n",
    "                i += 1\n",
    "        else:\n",
    "            r_emoji = ''\n",
    "\n",
    "        return df, r_emoji, rem_every_other, remove_advertisements, remove_dupl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#### MAIN                                                 ####\n",
    "##############################################################\n",
    "\n",
    "#Subreddit to query\n",
    "# data will return the number of submissions or posts in the subreddit.\n",
    "# query is a key word to be searched on in the subreddit\n",
    "# after is the starting date for the search\n",
    "# before is the ending date for the search\n",
    "# sub is the subreddit\n",
    "\n",
    "query = input('What stock symbol do you want to see? ')\n",
    "\n",
    "sub = 'wallstreetbets'\n",
    "\n",
    "unix_time = int(time.time()) # today's date in unix time\n",
    "before = unix_time\n",
    "#after = int(before - (60 * 60 * 24 * 7)) # one weeks prior to current date\n",
    "\n",
    "days = int(input('How many days back do you want to go? [Enter an integer between 1 and 14.] \\n'))\n",
    "\n",
    "after = int(before - (60 * 60 * 24 * days)) # one weeks prior to current date\n",
    "\n",
    "subCount = 0\n",
    "subStats = {}\n",
    "\n",
    "# modified by PH; added\n",
    "commentStats = {} # dictionary to hold comments\n",
    "\n",
    "%pwd # set path to present working directory\n",
    "\n",
    "data = getPushshiftData(query, after, before, sub)\n",
    "# Will run until all posts have been gathered from the 'after' date up until before date\n",
    "\n",
    "# while there is at lease one post the \n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        # collectSubData collects the:\n",
    "        collectSubData(submission)\n",
    "        \n",
    "        # \"subCount\" is the number of posts in a subreddit\n",
    "        subCount+=1\n",
    "    \n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    \n",
    "    # modified by PH\n",
    "    #print('length = ',len(data))\n",
    "    #print(\"submissions returned in getPushshiftData dictionary:\" + str(subCount))\n",
    "    #print(\"len dictonary returned in call:\"+str(len(data)))\n",
    "    \n",
    "    #print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    print(\"created timestamp:\"+str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "\n",
    "    #end\n",
    "    \n",
    "    after = data[-1]['created_utc']\n",
    "    \n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "    \n",
    "updateSubs_file()\n",
    "df = updateComs_file() #writes a csv file and also converts data to a df\n",
    "\n",
    "# modified by PH    \n",
    "\n",
    "#print(\"after while loop len data:\"+str(len(data)))\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting finVizFetchPkg\n",
      "  Downloading https://files.pythonhosted.org/packages/25/ec/92b3ea22d11da1242672815c751b4b8f771d3a7524a4e539234cc1989f86/finVizFetchPkg-0.0.2-py3-none-any.whl\n",
      "Installing collected packages: finVizFetchPkg\n",
      "Successfully installed finVizFetchPkg-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install finVizFetchPkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the stock symbol? (Please enter only one.)v\n",
      "ticker inside for loop:  v\n",
      "request url:https://www.finviz.com/quote.ashx?t=v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\finVizFetchPkg\\finVizScaper.py:45: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 45 of the file C:\\Users\\User\\Anaconda3\\lib\\site-packages\\finVizFetchPkg\\finVizScaper.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  html = BeautifulSoup(response, 'html')\n"
     ]
    }
   ],
   "source": [
    "import finVizFetchPkg.finVizScaper\n",
    "finvizdf = finVizFetchPkg.finVizScaper.finvizStreamer() #comment out to run in notebook\n",
    "df = finvizdf.scrape_finziz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>Sep-08-21</td>\n",
       "      <td>12:06AM</td>\n",
       "      <td>PayPal to Buy Japanese Unicorn Paidy in $2.7 B...</td>\n",
       "      <td>https://finance.yahoo.com/news/paypal-acquire-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>Sep-06-21</td>\n",
       "      <td>06:06AM</td>\n",
       "      <td>3 No-Brainer Stocks to Invest $300 in Right Now</td>\n",
       "      <td>https://www.fool.com/investing/2021/09/06/3-no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>Sep-05-21</td>\n",
       "      <td>01:19PM</td>\n",
       "      <td>While Visas (NYSE:V) Stock Price Appears Weak,...</td>\n",
       "      <td>https://finance.yahoo.com/news/while-visa-nyse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v</td>\n",
       "      <td>Sep-04-21</td>\n",
       "      <td>09:26AM</td>\n",
       "      <td>10 Financial Services Dividend Champions to Bu...</td>\n",
       "      <td>https://finance.yahoo.com/news/10-financial-se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>Sep-03-21</td>\n",
       "      <td>06:20PM</td>\n",
       "      <td>NFT marketplace OpenSea records $3.4 billion t...</td>\n",
       "      <td>https://www.marketwatch.com/story/nft-marketpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>v</td>\n",
       "      <td>Jul-21-21</td>\n",
       "      <td>10:03AM</td>\n",
       "      <td>Schwab (SCHW) Option Traders Betting on Bounce...</td>\n",
       "      <td>https://www.investopedia.com/schwab-schw-optio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>v</td>\n",
       "      <td>Jul-21-21</td>\n",
       "      <td>06:59AM</td>\n",
       "      <td>Meet Visa: Reintroducing the Iconic Visa Brand...</td>\n",
       "      <td>https://finance.yahoo.com/news/meet-visa-reint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>v</td>\n",
       "      <td>Jul-21-21</td>\n",
       "      <td>05:51AM</td>\n",
       "      <td>Got $5,000? 5 Brand-Name Stocks That'll Make Y...</td>\n",
       "      <td>https://www.fool.com/investing/2021/07/21/got-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>v</td>\n",
       "      <td>Jul-20-21</td>\n",
       "      <td>05:45PM</td>\n",
       "      <td>Visa (V) Gains But Lags Market: What You Shoul...</td>\n",
       "      <td>https://finance.yahoo.com/news/visa-v-gains-la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>v</td>\n",
       "      <td>Jul-20-21</td>\n",
       "      <td>03:02PM</td>\n",
       "      <td>Visa (V) Earnings Expected to Grow: What to Kn...</td>\n",
       "      <td>https://finance.yahoo.com/news/visa-v-earnings...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker       date       time  \\\n",
       "0       v  Sep-08-21  12:06AM     \n",
       "1       v  Sep-06-21  06:06AM     \n",
       "2       v  Sep-05-21  01:19PM     \n",
       "3       v  Sep-04-21  09:26AM     \n",
       "4       v  Sep-03-21  06:20PM     \n",
       "..    ...        ...        ...   \n",
       "95      v  Jul-21-21  10:03AM     \n",
       "96      v  Jul-21-21  06:59AM     \n",
       "97      v  Jul-21-21  05:51AM     \n",
       "98      v  Jul-20-21  05:45PM     \n",
       "99      v  Jul-20-21  03:02PM     \n",
       "\n",
       "                                                title  \\\n",
       "0   PayPal to Buy Japanese Unicorn Paidy in $2.7 B...   \n",
       "1     3 No-Brainer Stocks to Invest $300 in Right Now   \n",
       "2   While Visas (NYSE:V) Stock Price Appears Weak,...   \n",
       "3   10 Financial Services Dividend Champions to Bu...   \n",
       "4   NFT marketplace OpenSea records $3.4 billion t...   \n",
       "..                                                ...   \n",
       "95  Schwab (SCHW) Option Traders Betting on Bounce...   \n",
       "96  Meet Visa: Reintroducing the Iconic Visa Brand...   \n",
       "97  Got $5,000? 5 Brand-Name Stocks That'll Make Y...   \n",
       "98  Visa (V) Gains But Lags Market: What You Shoul...   \n",
       "99  Visa (V) Earnings Expected to Grow: What to Kn...   \n",
       "\n",
       "                                                 link  \n",
       "0   https://finance.yahoo.com/news/paypal-acquire-...  \n",
       "1   https://www.fool.com/investing/2021/09/06/3-no...  \n",
       "2   https://finance.yahoo.com/news/while-visa-nyse...  \n",
       "3   https://finance.yahoo.com/news/10-financial-se...  \n",
       "4   https://www.marketwatch.com/story/nft-marketpl...  \n",
       "..                                                ...  \n",
       "95  https://www.investopedia.com/schwab-schw-optio...  \n",
       "96  https://finance.yahoo.com/news/meet-visa-reint...  \n",
       "97  https://www.fool.com/investing/2021/07/21/got-...  \n",
       "98  https://finance.yahoo.com/news/visa-v-gains-la...  \n",
       "99  https://finance.yahoo.com/news/visa-v-earnings...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "####                     MAIN                      ####\n",
    "#######################################################\n",
    "\n",
    "yes_resp = ['yes', 'YES', 'y', 'Y', 'Yes']\n",
    "no_resp = ['no', 'NO', 'n', 'N', 'No']\n",
    "\n",
    "#need to accommodate for three different types of inputs. \n",
    "#         - finviz\n",
    "#         - stocktwits\n",
    "#         - reddit\n",
    "\n",
    "# step one is to either scrape or parse the data into a dataframe, df.\n",
    "#############\n",
    "# Finviz scraper\n",
    "#############\n",
    "#finvizdf = finvizStreamer()\n",
    "import finVizFetchPkg()\n",
    "finvizdf = finVizFetchPkg.finVizScaper.finvizStreamer() #comment out to run in notebook\n",
    "df = finvizdf.scrape_finziz()\n",
    "\n",
    "#############\n",
    "# stocktwits API\n",
    "#############\n",
    "\n",
    "\n",
    "#############\n",
    "# reddit API\n",
    "#############\n",
    "\n",
    "# step two is to convert each of the dataframes into a common format with the same column names (data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if finviz_resp in yes_resp:\n",
    "    df.rename(columns = {'ticker':'symbol', 'title':'body', 'Sentiment':'sentiment'}, inplace = True) #renames the columns to match the stocktwits names\n",
    "    \n",
    "####################################    \n",
    "'''finviz output columns:\n",
    "ticker,date,time,title,Sentiment\n",
    "\n",
    "stocktwits parser output columsn:\n",
    "symbol,messageID ,created_at,body,followers,sentiment,date,time\n",
    "\n",
    "ticker = symbol\n",
    "title = body\n",
    "Sentiment = sentiment\n",
    "date = date\n",
    "time = time\n",
    "\n",
    "\"messageID\", \"created_at\", \"followers\" do not exist in scraped finviz csv '''\n",
    "\n",
    "######################################\n",
    "\n",
    "#step 3 is to scrub the df to optimize the natural Language sentiment classification model development and accuracy\n",
    "\n",
    "df = remove_duplicate_headers(df)\n",
    "\n",
    "df = remove_duplicates(df)\n",
    "\n",
    "df = remove_http_tags(df)\n",
    "\n",
    "df = remove_punctuation(df)\n",
    "\n",
    "df = remove_unicode(df)\n",
    "\n",
    "df = lower_case(df)\n",
    "\n",
    "#df = convert_sentiment_to_numerical(df) #line 240\n",
    "\n",
    "#df = compound_binning(df) #line 210\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "df = remove_stopwords(df, stop_words)\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "df = lemmatize(df, lemmer)\n",
    "\n",
    "\n",
    "#### checks to see if there are any empty records\n",
    "print('Test empty records before writing the csv file')\n",
    "empty_records_check(df)\n",
    "\n",
    "df = remove_empty_body_rows(df)\n",
    "\n",
    "# Writes a csv file; input  df that is to be saved as a csv; output file name is combination of types of editing\n",
    "w_csv = input('Do you want to write a csv file? [Press enter if no] ')\n",
    "if w_csv in yes_resp:\n",
    "    new_name = name.replace('.csv', '') #removes the .csv from the input file's name\n",
    "    print(new_name)\n",
    "    processed = 'preprocessed'\n",
    "    # creates a file name that is a combination of all the different scrubbing types\n",
    "    #filename_output = processed + remove_dupl + remove_advertisements + rem_every_other + swords + ed + r_emoji + vader_run + name\n",
    "    filename_output = new_name + '_' + processed + '_lemmatized.csv'\n",
    "\n",
    "    if name == filename_output: #Checks to see if the file already exists\n",
    "        os.remove(filename_output) #If the file already exists it deletes the original file\n",
    "        print('The old file was deleted.\\n')\n",
    "    \n",
    "    write_csv(df, filename_output, relevant_path) #Writes the df to a new file\n",
    "    print('The file was written with the filename of: ', filename_output, '\\n')\n",
    "\n",
    "    # NOTE TO SELF - When there is a record that has spaces only, it is encoded as a 'NaN' or empty record\n",
    "    #when encoded as a utf-8 csv file. It will cause the postprocessing Vader app to crash. Importing the csv file\n",
    "    #and then removing the 'NaN' and then rewriting the csv file should take care of the problem.\n",
    "\n",
    "    final_name =  relevant_path + '/' + filename_output\n",
    "    print('The filename is: \\n', final_name)\n",
    "    dftest = getData(final_name)\n",
    "    print('csv file read into df to see if all of the empty records are removed.')\n",
    "    empty_records_check(dftest)\n",
    "    df_final = remove_empty_body_rows(dftest)\n",
    "    empty_records_check(df_final)\n",
    "\n",
    "    os.remove(final_name) #If the file already exists it deletes the original file\n",
    "    write_csv(df_final, filename_output, relevant_path) #Writes the df to a new file\n",
    "\n",
    "# combines two dfs\n",
    "c_t_dfs = input('Do you want to combine two files? [Press enter if no] ')\n",
    "if c_t_dfs in yes_resp:\n",
    "    \n",
    "    print('Here is a list of the csv files to choose from: \\n')\n",
    "    list_dir_files(relevant_path)\n",
    "    first_name = input('\\nWhat is the first file you want to combine? ')\n",
    "    df = getData(relevant_path + '/' + first_name) #returns df; reads csv file into df\n",
    "    print('Imported the csv file.')\n",
    "    \n",
    "    second_name = input('What is the second file you want to add? ')\n",
    "    df2 = getData(relevant_path + '/' + second_name)\n",
    "\n",
    "    # 650 Loads and combines two different dataframes in dfAPI; this is to combine two input datasets where the 'none'\n",
    "    #values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "    df = combine_dfs(df1, df2)\n",
    "    \n",
    "    w_csv = input('Do you want to write a csv file? [Press enter if no] ')\n",
    "    if w_csv in yes_resp:\n",
    "        first_name_no_csv = first_name.replace('.csv', ' + ') \n",
    "        duo_name = first_name_no_csv + second\n",
    "        write_csv(df, duo_name, relevant_path, encoding = 'UNICODE') #Writes the df to a new file\n",
    "        print('The file was written with the filename of: ', duo_name, '\\n')\n",
    "\n",
    "\n",
    "print('\\nAll done ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = 'test_name.csv'\n",
    "new_name = testname.replace('.csv', '') #removes the .csv from the input file's name\n",
    "new_name = new_name + '_test.csv'\n",
    "print(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df):\n",
    "    print('\\nLemmatizing ...')\n",
    "    \n",
    "    import nltk\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    nltk.download('wordnet') #not in original code\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    %time df['body_processed'] = df['body_processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nLemmatizing ...')\n",
    "    \n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('wordnet') #not in original code\n",
    "    \n",
    "# Lemmatize the text\n",
    "lemmer = WordNetLemmatizer()\n",
    "%time df['body_processed'] = df['body_processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df):\n",
    "    print('\\nLemmatizing ...')\n",
    "    \n",
    "    import nltk\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    nltk.download('wordnet') #not in original code\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    %time df['body_processed'] = df['body_processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lemmatize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df):\n",
    "\n",
    "    import nltk\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    #adds new stopwords to list\n",
    "    new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270',\n",
    "                      '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "                      '1600', '993', '392', '98', '00', '1601', 'amd', 'aapl']\n",
    "    for w in new_stop_words:\n",
    "        stop_words.append(w)\n",
    "\n",
    "    print('stop_words: ', stop_words)\n",
    "\n",
    "    #removes the stopwords from the column body_processed\n",
    "    %time df['body_processed'] = df['body_processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#adds new stopwords to list\n",
    "new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270',\n",
    "                      '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "                      '1600', '993', '392', '98', '00', '1601', 'amd', 'aapl']\n",
    "for w in new_stop_words:\n",
    "    stop_words.append(w)\n",
    "\n",
    "print('stop_words: ', stop_words)\n",
    "\n",
    "#removes the stopwords from the column body_processed\n",
    "%time df['body_processed'] = df['body_processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = getData('preprocessed tech stockTwit 03112021.csv')\n",
    "print(dftest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.reset_index(drop = True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "none_count_raw(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_resp = ['yes', 'YES', 'y', 'Y', 'Yes']\n",
    "no_resp = ['no', 'NO', 'n', 'N', 'No']\n",
    "\n",
    "test = input('do you want to test? ')\n",
    "if test in yes_resp:\n",
    "    print('yes I do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = 'output.csv'\n",
    "remove_dupl = 'a '\n",
    "remove_advertisements = 'b '\n",
    "remove_every_other = ''\n",
    "ed = 'd '\n",
    "\n",
    "filename_output = remove_dupl + remove_advertisements + remove_every_other + ed + name1\n",
    "\n",
    "print(filename_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to determine if column exists\n",
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame([[10, 20, 30, 40], [7, 14, 21, 28], [55, 15, 8, 12]],\n",
    "                  columns=['Apple', 'Orange', 'Banana', 'Pear'],\n",
    "                  index=['Basket1', 'Basket2', 'Basket3'])\n",
    " \n",
    "if 'apple' not in df.columns:\n",
    "    print(\"in - no\")\n",
    "else:\n",
    "    print(\"notin - yes\")\n",
    " \n",
    " \n",
    "if set(['Apple','Orange']).issubset(df.columns):\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_duplicate_headers(df):\n",
    "    column = 'symbol'\n",
    "    df.drop(df[df['symbol'] == column].index, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('Here is a list of the csv files to choose from: \\n')\n",
    "list_dir_files()\n",
    "name = input('\\nWhat file do you want to use? ')\n",
    "df = getData(name) #returns df; reads csv file into df\n",
    "print('Imported the csv file.')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "i = 0\n",
    "while i < len(df):\n",
    "    if df.iloc[i , 0] == \"symbol\":\n",
    "        print('The index is: ', i)\n",
    "    i += 1\n",
    "\n",
    "print('starting to remove headers')\n",
    "df = remove_duplicate_headers(df)\n",
    "print('done removing headers')\n",
    "\n",
    "\n",
    "i = 0\n",
    "while i < len(df):\n",
    "    if df.iloc[i , 0] == \"symbol\":\n",
    "        print('The index is: ', i)\n",
    "    i += 1\n",
    "else:\n",
    "    print('They are all gone!')\n",
    "    \n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def remove_duplicate_headers(df):\n",
    "    column = 'symbol'\n",
    "    df.drop(df[df['symbol'] == column].index, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Post Processing'\n",
    "included_extensions = ['csv']\n",
    "file_names = [fn for fn in os.listdir(relevant_path)\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "for f in file_names:\n",
    "    print(f)\n",
    "    \n",
    "name = input('What file do you want: ')\n",
    "df = getData(relevant_path + '/' + name)\n",
    "\n",
    "print(df.head(120))\n",
    "\n",
    "print('before:')\n",
    "empty = np.where(pd.isnull(df['body']))\n",
    "print('empty')\n",
    "\n",
    "df = remove_duplicate_headers(df)\n",
    "\n",
    "df = df.fillna(value ={'body':' '}) #replaces any empty 'body' records with a space\n",
    "\n",
    "print('after:')\n",
    "np.where(pd.isnull(df['body']))\n",
    "\n",
    "print(df.head(120))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding and removing empty records in a df\n",
    "dftest = getData(relevant_path + '/' + filename_output)\n",
    "\n",
    "print(relevant_path + '/' + filename_output)\n",
    "print('csv file read into df to see if all of the empty records are removed.')\n",
    "\n",
    "#finds empty records\n",
    "empty = np.where(pd.isnull(dftest['body'])) #checks to see if there are any empty records in the column 'body'; empty is a tuple where the first element is the array, the second is dtype of the array\n",
    "if empty[0].size == 0:\n",
    "    print('There are no empty records: \\n', empty)\n",
    "else:\n",
    "    print('There are empty records: \\n', empty, '\\n')\n",
    "    \n",
    "print(dftest.iloc[110:125,])\n",
    "\n",
    "#drops empty records\n",
    "dftest.dropna(subset=['body'], inplace=True) #drops empty body records\n",
    "dftest = dftest.reset_index(drop = True) # resets the index\n",
    "\n",
    "empty = np.where(pd.isnull(dftest['body'])) #checks to see if there are any empty records in the column 'body'; empty is a tuple where the first element is the array, the second is dtype of the array\n",
    "print('\\nAFTER DROP: \\n', empty, '\\n')\n",
    "\n",
    "print(dftest.iloc[110:125,])\n",
    "\n",
    "# removes specific rows and resets the index\n",
    "def remove_empty_body_rows(df):\n",
    "    df.dropna(subset=['body'], inplace=True) #drops empty body records\n",
    "    df = df.reset_index(drop = True) # resets the index\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['body'][10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes emojis \n",
    "\n",
    "# Inports the csv file of choice\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files'\n",
    "\n",
    "print('Here is a list of the csv files to choose from: \\n')\n",
    "list_dir_files(relevant_path)\n",
    "name = input('\\nWhat file do you want to use? ')\n",
    "df = getData(relevant_path + '/' + name) #returns df; reads csv file into df\n",
    "print('Imported the csv file.')\n",
    "\n",
    "\n",
    "\n",
    "def remove_emoji(string):\n",
    "    import re\n",
    "    import sys\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "i = 0\n",
    "\n",
    "yes_dec = ['yes', 'y']\n",
    "\n",
    "decision = input('decide: ')\n",
    "\n",
    "if decision in yes_dec:\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        string = df.loc[i, ('body')]\n",
    "        #print('original string: ', string)\n",
    "        new_string = remove_emoji(string)\n",
    "        #print('new string: ', new_string)\n",
    "        df.loc[i, ('body')] = new_string\n",
    "        #print(df['body'][i])\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "print('all done')\n",
    "\n",
    "print(df.loc[13,'body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[13,'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "while i < 2:\n",
    "    #string = df.iloc[i,2]\n",
    "    string = df.loc[i, ('body')]\n",
    "    \n",
    "    dat = df.loc[i, ('body')] \n",
    "    data = [dat] # \n",
    "    \n",
    "    print('original string: ', dat)\n",
    "    print(data)\n",
    "    i += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manipulating two names and then adding them together\n",
    "\n",
    "first = 'first.csv'\n",
    "second = 'second.csv'\n",
    "first_no_csv = first.replace('.csv', ' + ') \n",
    "\n",
    "first_second = first_no_csv + second\n",
    "print(first_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = -1.9\n",
    "rounding = int(negative)\n",
    "print(rounding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 finds certain words in the strings ('body') and deletes the entire record.\n",
    "#Note: When the record is deleted the df is re-indexed. The index for the while statement is not so the result is\n",
    "#that the record right after the deleted record is skipped. To remedy the problem the index (i) for the while statement \n",
    "#is decremented by one.\n",
    "#Also, the filtering terms are not case sensitive.\n",
    "def filter_records(df):\n",
    "    import fnmatch\n",
    "\n",
    "    data = []\n",
    "    counter = 0\n",
    "    advert = ['* sec *', '* daily News *', '*Huge Print*', '* Form *', '*SweepCast*', '*Large Print*', \n",
    "          '*Huge Print*', '*8-K*', '*SmartOptions*', '*Big Trade*', '*SEC Form*', '*Notice of Exempt*', \n",
    "          '*created_at*', '*stock news*', '*Trading Zones*', '*Entry:*', '*New Article*', '*ooc.bz*', \n",
    "          '*http*', 'Huge Trade', 'Trading is easy', 'www.', '#wallstreetbets', 'wallstreetbets',\n",
    "          'Huge Trade', '#unitedtraders', 'stockbeep.com', 'Big Trade'] # words or phrases whose records are to be removed; It is not case sensitive.\n",
    "\n",
    "    for a in advert:\n",
    "        i = 0\n",
    "        df = df.reset_index(drop = True) # resets the index before each iteration; removes the gaps; resets len(df)\n",
    "        while i < len(df):\n",
    "            dat = df.loc[i, ('body')] # 2 represents the 'body' column\n",
    "            data = [dat] # sets the string from the df into a list for the fnmatch.filter\n",
    "            #print('index = ', i)\n",
    "            filtered = fnmatch.filter(data, a) # compares the information in the 'body' column with the 'advert' list; it places the matched items in the 'filtered' variable.\n",
    "            #https://www.geeksforgeeks.org/fnmatch-unix-filename-pattern-matching-python/\n",
    "\n",
    "            if len(filtered) != 0: #if returns a True then record needs to be removed\n",
    "                counter += 1\n",
    "            \n",
    "                df = df.drop(df.index[i]) # drops (deletes) the record\n",
    "            \n",
    "                #print('after the record is dropped:', df..log[i,('body')], 'i = ', i)\n",
    "                \n",
    "                #Note: When the record is dropped there is a change in the 'index' number. after the drop index number\n",
    "                #5 becomes index number 4. Since the counter increments one more time it skips the record right after\n",
    "                #the record that was just checked. That is why it takes multiple runs to remove all of the target\n",
    "                #records. To correct this decrement the index, i, by\n",
    "                \n",
    "                i -= 1\n",
    "    \n",
    "            i += 1\n",
    "\n",
    "    df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "    len(df)\n",
    "    return df\n",
    "\n",
    "df = filter_records(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 40 finds certain words in the strings ('body') and deletes the entire record.\n",
    "#Note: When the record is deleted the df is re-indexed. The index for the while statement is not so the result is\n",
    "#that the record right after the deleted record is skipped. To remedy the problem the index (i) for the while statement \n",
    "#is decremented by one.\n",
    "#Also, the filtering terms are not case sensitive.\n",
    "\n",
    "import fnmatch\n",
    "df = df.reset_index(drop = True) # resets the index; removes the gaps  \n",
    "data = []\n",
    "counter = 0\n",
    "advert = ['* sec *', '* daily News *', '*Huge Print*', '* Form *', '*SweepCast*', '*Large Print*', \n",
    "          '*Huge Print*', '*8-K*', '*SmartOptions*', '*Big Trade*', '*SEC Form*', '*Notice of Exempt*', \n",
    "          '*created_at*', '*stock news*', '*Trading Zones*', '*Entry:*', '*New Article*', '*ooc.bz*', \n",
    "          '*http*', 'Huge Trade', 'Trading is easy', 'www.', '#wallstreetbets', 'wallstreetbets',\n",
    "          'Huge Trade', '#unitedtraders', 'stockbeep.com', 'Big Trade'] # words or phrases whose records are to be removed; It is not case sensitive.\n",
    "\n",
    "for a in advert:\n",
    "    i = 0\n",
    "    df = df.reset_index(drop = True) # resets the index before each iteration; removes the gaps; resets len(df)\n",
    "    while i < len(df):\n",
    "        dat = df.loc[i, ('body')] # 2 represents the 'body' column\n",
    "        #print('index =', i)\n",
    "        #print(dat)\n",
    "        #print(a)\n",
    "        data = [dat] # sets the string from the df into a list for the fnmatch.filter\n",
    "        #print('index = ', i)\n",
    "        filtered = fnmatch.filter(data, a) # compares the information in the 'body' column with the 'advert' list; it places the matched items in the 'filtered' variable.\n",
    "        #https://www.geeksforgeeks.org/fnmatch-unix-filename-pattern-matching-python/\n",
    "\n",
    "        if len(filtered) != 0: #if returns a True then record needs to be removed\n",
    "            counter += 1\n",
    "            \n",
    "            df = df.drop(df.index[i]) # drops (deletes) the record\n",
    "            df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "            #print('after the record is dropped:', df..log[i,('body')], 'i = ', i)\n",
    "                \n",
    "            #Note: When the record is dropped there is a change in the 'index' number. after the drop index number\n",
    "            #5 becomes index number 4. Since the counter increments one more time it skips the record right after\n",
    "            #the record that was just checked. That is why it takes multiple runs to remove all of the target\n",
    "            #records. To correct this decrement the index, i, by\n",
    "                \n",
    "            i -= 1\n",
    "   \n",
    "        i += 1\n",
    "\n",
    "df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "len(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[340:350,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 110 This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "#the 'None' proportions more equal. It also prints the ratios of each sentiment response to the total number\n",
    "#of responses.\n",
    "def remove_every_other(df):\n",
    "    i = 0\n",
    "    counter = 0\n",
    "    df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "\n",
    "    while i < len(df):\n",
    "        print('index =', i, i % 2, df.loc[i,('sentiment')])\n",
    "\n",
    "        if df.loc[i,('sentiment')] == 'None': #column 4 is sentiment\n",
    "            if i % 2 == 0: #identifies every even index where the sentiment is \"None\"\n",
    "                df.drop(df.index[i]) #drops (deletes) the record\n",
    "                print('index =', i, df.loc[i,('sentiment')])\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.loc[i,('sentiment')] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('\\nThe total number of records is: ', len(df))\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.loc[i,('sentiment')] == 'Bullish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bullish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bullish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "            \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.loc[i,('sentiment')] == 'Bearish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bearish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bearish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '% \\n')\n",
    "            \n",
    "    return df    \n",
    "\n",
    "remove_every_other(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 110 This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "#the 'None' proportions more equal. It also prints the ratios of each sentiment response to the total number\n",
    "#of responses.\n",
    "def remove_every_other(df):\n",
    "    i = 1\n",
    "    counter = 0\n",
    "    df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "    \n",
    "    print(len(df))\n",
    "\n",
    "    while i < len(df):\n",
    "        print('index =', i, i % 2, df.loc[i,('sentiment')])\n",
    "\n",
    "        if df.loc[i,('sentiment')] == 'None': #column 4 is sentiment\n",
    "            if i % 2 == 0: #identifies every even index where the sentiment is \"None\"\n",
    "                print('inside :',i, i % 2)\n",
    "                print(df.loc[i], '\\n right before drop')\n",
    "                df = df.drop(df.index[i]) #drops (deletes) the record\n",
    "                df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "\n",
    "                #df.drop([i]) #drops (deletes) the record\n",
    "                print('index =', i, df.loc[i,('sentiment')])\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "    \n",
    "    print(len(df))\n",
    "    \n",
    "    return df    \n",
    "\n",
    "remove_every_other(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "df = df.drop(df.index[i]) #drops (deletes) the record\n",
    "#print('index =', i, df.loc[i,('sentiment')])\n",
    "\n",
    "print(df.loc[0:15,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "d = { 'Name':['Alisa','raghu','jodha','jodha','raghu','Cathrine', 'Alisa','Bobby','Bobby','Alisa','raghu','Cathrine'],\n",
    "     'Age':[26,23,23,23,23,24,26,24,22,26,23,24], \n",
    "     'Score':[85,31,55,55,31,77,85,63,42,85,31,np.nan]}\n",
    "\n",
    "df = pd.DataFrame(d,columns=['Name','Age','Score'])\n",
    "\n",
    "df\n",
    "\n",
    "df.drop([1,2])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_resp = ['yes', 'YES', 'y', 'Y', 'Yes']\n",
    "no_resp = ['no', 'NO', 'n', 'N', 'No']\n",
    "\n",
    "finviz_resp = input('Is this a file from scraping Finviz? ')\n",
    "\n",
    "if finviz_resp in yes_resp:\n",
    "    print('It is in there')\n",
    "else:\n",
    "    print('It is not in there')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if 'raw_compound' not in df.columns:\n",
    "    before_scrubbing = input('Do you want to run the Vader analysis before scrubbing? \\n')\n",
    "    if before_scrubbing in yes_resp:\n",
    "        vader_run = 'v_b '\n",
    "        print('\\nThis is the first time this file has been preprocessed.\\n')\n",
    "        print('Performing Vader sentiment analysis before scrubbing... \\n')\n",
    "    \n",
    "        df = vader_analysis(df)\n",
    "        \n",
    "        df, r_emoji, rem_every_other, remove_advertisements, remove_dupl = rem_dup_adver_ever_oth_emoji(df)\n",
    "        df, swords = remove_stopwords(df)\n",
    "        \n",
    "    else:\n",
    "        vader_run = 'v_a '\n",
    "        print('\\n Performing Vader sentiment analysis after scrubbing... \\n')\n",
    "        \n",
    "        df, r_emoji, rem_every_other, remove_advertisements, remove_dupl = rem_dup_adver_ever_oth_emoji(df)\n",
    "        df, swords = remove_stopwords(df)\n",
    "    \n",
    "        df = vader_analysis(df)   \n",
    "else:\n",
    "    print('\\nThis file has been preprocessed before. There is no need to run the VADER analysis.\\n')\n",
    "'''\n",
    "\n",
    "'''# 90 OPTIONAL Compares the Vader sentiment numbers with the Stocktwits sentiment ratings.\n",
    "v_c = input('Do you want to compare the Vader sentiment numbers with the Stocktwits sentiment ratings? [Press enter if no] ')\n",
    "if v_c in yes_resp:\n",
    "        if 'raw_compound' in df.columns: #checks to see if this file have been prepocessed before by seeing if the column 'raw_compond' exists\n",
    "            vader_correct(df) \n",
    "\n",
    "# 100 OPTIONAL: Counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "c_n_s = input('Do you want to count the \"None\" sentiment values for the Stocktwits sentiments before any edits? [Press enter if no] ')\n",
    "if c_n_s in yes_resp:\n",
    "    none_count_raw(df) \n",
    "\n",
    "# 115 OPTIONAL: Provides statistics on Stocktwits sentiments; bullish, none or bearish.\n",
    "s_o_s = input('Do you want to see the statistics on the Stocktwits sentiments? [Press enter if no] ')\n",
    "if s_o_s in yes_resp:\n",
    "    stats(df) \n",
    "\n",
    "# 120 OPTIONAL: Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "# It counts every 20 edits and gives the user the option to quit. If the user chooses to quit\n",
    "# it breaks from the while look and writes the df to a csv file so all work is saved up to that point.\n",
    "# upon start up it ask if thie is the first time processing the raw data. If no it loads the csv file into\n",
    "# the dataframe and starts where the previous session left off. If \"modified?\" is \"Yes and \"sentiment\" is \"None\"\n",
    "# it skips the record. Therefore it will re-start at the first \"modified?\" is \"No\" and \"sentiment\" is \"None\"\n",
    "\n",
    "e_n = input('Do you want to edit the \"None\" records? [Press enter if no] ')\n",
    "if e_n in yes_resp:\n",
    "    df = edit(df) #returns df\n",
    "    ed = 'edited '\n",
    "else:\n",
    "    ed = ''\n",
    "\n",
    "# 180 OPTIONAL: counts how many \"None\" sentiment values are there for the stocktwits sentiment values after the edit\n",
    "n_r_a_e = input('Do you want to see how many \"None\" records there are after the edits? [Press enter if no] ')\n",
    "if n_r_a_e in yes_resp:\n",
    "    none_count(df) \n",
    "\n",
    "# 140 OPTIONAL: This will change the modified rating to the nltk rating only when they are opposite to see if it improves \n",
    "#the accuracy number \n",
    "# flip vader rating if opposite to stocktwits sentiment\n",
    "f_v_r = input('Do you want to flip the Vader sentiment rating when it is the opposite of the Stocktwits sentiment rating? [Press enter if no] ')\n",
    "if f_v_r in yes_resp:\n",
    "    df = change_opp_nltk(df) #returns df\n",
    "\n",
    "# 180 OPTIONAL: counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "n_c_a_e = input('Do you want to see the number of \"None\" sentiments after the edit? [Press enter if no] ')\n",
    "if n_c_a_e in yes_resp:\n",
    "    none_count(df) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# methods\n",
    "\n",
    "class SentimentAnalysisPreprocessing(self):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    # 10 initializes the dataframe \"df\" and imports the csv into df; \n",
    "    # the argument is the name/address of the file.\n",
    "    # https://stackoverflow.com/questions/33440805/pandas-dataframe-read-csv-on-bad-data\n",
    "    def getData(name):\n",
    "        df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "        df1 = pd.read_csv(name, header = 0)\n",
    "        return df1\n",
    "\n",
    "    # removes duplicate headers\n",
    "    def remove_duplicate_headers(df):\n",
    "        print('\\nDropping duplicate headers ...')\n",
    "        column = 'symbol'\n",
    "        %time df.drop(df[df['symbol'] == column].index, inplace=True)\n",
    "        df = df.reset_index(drop = True) # resets the index\n",
    "        return df\n",
    "\n",
    "    # 30 removes any duplicate records; duplicate records imply bot records\n",
    "    def remove_duplicates(df):\n",
    "        print('\\nDropping duplicates ...')\n",
    "        %time df = df.drop_duplicates()\n",
    "        df = df.reset_index(drop = True) # resets the index\n",
    "        len(df)\n",
    "        return df\n",
    "\n",
    "    # remove HTTP tags\n",
    "    def remove_http_tags(df):\n",
    "        print('\\nRemoving http tags ...')\n",
    "        %time df['body_processed'] = df['body'].map(lambda x : ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))\n",
    "        return df\n",
    "\n",
    "    # coverts to all lower case\n",
    "    def lower_case(df):\n",
    "        print('\\nConverting to lower case ...')\n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x: x.lower())\n",
    "        return df\n",
    "\n",
    "    # removes all punctuation\n",
    "    def remove_punctuation(df):\n",
    "        print('\\nRemoving punctuation ...')\n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "        return df\n",
    "\n",
    "    # removes unicodes (emojis)\n",
    "    def remove_unicode(df):\n",
    "        print('\\nRemoving unicode ...')\n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "        return df\n",
    "\n",
    "    def lemmatize(df, lemmer): #lemmer must be defined outside of the function and passed in\n",
    "        print('\\nLemmatizing ...')\n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "        return df\n",
    "\n",
    "    # Remove stopwords\n",
    "    def remove_stopwords(df, stop_words): #stop_words must be defined outside of the function and passed in\n",
    "        print('\\nRemoving stopwords ...')\n",
    "\n",
    "        #adds new stopwords to list\n",
    "\n",
    "        newStopWords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "        newStopWords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "        newStopWords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "        newStopWords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "        newStopWords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "        newStopWords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "        newStopWords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "        newStopWords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "        newStopWords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "        newStopWords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "        newStopWords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "        newStopWords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "        newStopWords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "        newStopWords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "        newStopWords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "        newStopWords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "        newStopWords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "        newStopWords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "        newStopWords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "        newStopWords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "        newStopWords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "        newStopWords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "        newStopWords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "        newStopWords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "        newStopWords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "        newStopWords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "        newStopWords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "        newStopWords += ['nevertheless', 'next', 'nine', 'nobody', 'none'] #removed 'no'\n",
    "        newStopWords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "        newStopWords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "        newStopWords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "        newStopWords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "        newStopWords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "        newStopWords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "        newStopWords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "        newStopWords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "        newStopWords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "        newStopWords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "        newStopWords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "        newStopWords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "        newStopWords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "        newStopWords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "        newStopWords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "        newStopWords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "        newStopWords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "        newStopWords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "        newStopWords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "        newStopWords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "        newStopWords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "        newStopWords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "        newStopWords += ['yours', 'yourself', 'yourselves'] #provided by Codecademy??\n",
    "\n",
    "        # additional stopwords:\n",
    "        newStopWords += ['[screenshot]', 'screenshot', '[screenshot]great', 'screenshot',\n",
    "                         'the', 'smart', 'yah', 'got', 'nutty', 'moving', 'weeks', 'Got', 'So', 'today', 'Been', 'or']\n",
    "\n",
    "        newStopWords += ['i', 'you', 'He', 'he', 'she', 'they', 'their', 'it'] # pronouns\n",
    "\n",
    "        newStopWords += ['amd','nvda', 'tsla', 'goog', 'ba', 'fb', 'googl', 'intc', 'intel', 'csco', 'mu',\n",
    "                         'smh', 'tsm','aapl', 'csco', 'poetf', 'photonics', 'dd', 'arwr', 't', 'infini', 'amc', 'arl',\n",
    "                         'gme', 'nio', 'qs', 'msft', 'adbe', 'unh'] # Stock symbols or names\n",
    "\n",
    "        newStopWords += [] # nouns\n",
    "\n",
    "        #newStopWords += ['.', '?', '!', ';', ',', \"'\", '.'] # punctuation\n",
    "\n",
    "        newStopWords += ['&', '#', '%', '$', '@', '/'] # symbols\n",
    "\n",
    "        newStopWords += ['41.75', '530.05', '39', 'Two', 'two', 'One', 'one', 'Three', 'three', 'Four', 'four',\n",
    "                        'Five', 'five', 'Six', 'six', 'Seven', 'seven', 'Eight', 'eight', 'Nine', 'nine', 'Ten',\n",
    "                        'ten', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '39', ' 270',\n",
    "                          '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "                          '1600', '993', '392', '98', '00', '1601'] # numbers\n",
    "\n",
    "        for w in newStopWords:\n",
    "            stop_words.append(w)\n",
    "\n",
    "        #print('stop_words: ', stop_words)\n",
    "\n",
    "        #removes the stopwords from the column body_processed\n",
    "        %time df['body_processed'] = df['body_processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 40 finds certain words in the strings ('body') and deletes the entire record.\n",
    "    #Note: When the record is deleted the df is re-indexed. The index for the while statement is not so the result is\n",
    "    #that the record right after the deleted record is skipped. To remedy the problem the index (i) for the while statement \n",
    "    #is decremented by one.\n",
    "    #Also, the filtering terms are not case sensitive.\n",
    "    def filter_records(df):\n",
    "        import fnmatch\n",
    "\n",
    "        data = []\n",
    "        counter = 0\n",
    "        advert = ['* sec *', '* daily News *', '*Huge Print*', '* Form *', \n",
    "                  '*SweepCast*', '*Large Print*', '*Huge Print*', '*8-K*', \n",
    "                  '*SmartOptions*', '*Big Trade*', '*SEC Form*', '*Notice of Exempt*', \n",
    "                  '*created_at*', '*stock news*', '*Trading Zones*', '*Entry:*', \n",
    "                  '*New Article*', '*ooc.bz*', '*http*', 'Huge Trade', 'Trading is easy', \n",
    "                  'www.', '#wallstreetbets', 'wallstreetbets', 'Huge Trade', '#unitedtraders', \n",
    "                  'stockbeep.com', 'Big Trade'] # words or phrases whose records are to be removed; It is not case sensitive.\n",
    "\n",
    "        for a in advert:\n",
    "            i = 0\n",
    "            df = df.reset_index(drop = True) # resets the index before each iteration; removes the gaps; resets len(df)\n",
    "            while i < len(df):\n",
    "                dat = df.loc[i, ('body')] # 2 represents the 'body' column\n",
    "                data = [dat] # sets the string from the df into a list for the fnmatch.filter\n",
    "                #print('index = ', i)\n",
    "                filtered = fnmatch.filter(data, a) # compares the information in the 'body' column with the 'advert' list; it places the matched items in the 'filtered' variable.\n",
    "                #https://www.geeksforgeeks.org/fnmatch-unix-filename-pattern-matching-python/\n",
    "\n",
    "                if len(filtered) != 0: #if returns a True then record needs to be removed\n",
    "                    counter += 1\n",
    "\n",
    "                    df = df.drop(df.index[i]) # drops (deletes) the record\n",
    "                    df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "\n",
    "                    #print('after the record is dropped:', df..log[i,('body')], 'i = ', i)\n",
    "\n",
    "                    #Note: When the record is dropped there is a change in the 'index' number. after the drop index number\n",
    "                    #5 becomes index number 4. Since the counter increments one more time it skips the record right after\n",
    "                    #the record that was just checked. That is why it takes multiple runs to remove all of the target\n",
    "                    #records. To correct this decrement the index, i, by\n",
    "\n",
    "                    i -= 1\n",
    "\n",
    "                i += 1\n",
    "\n",
    "        df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "        len(df)\n",
    "        return df\n",
    "\n",
    "    #50 Vader sentiment analyzer\n",
    "    def vader_sentiment(df):\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "        f = lambda tweet: vader.polarity_scores(tweet)['compound']\n",
    "\n",
    "        df['raw_compound'] = df['body'].apply(f)\n",
    "\n",
    "        print('The number of clean records in the df are: ', len(df) , '\\n')\n",
    "        #print(df.head())\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 60 creates a new column called 'compound_bin' from the raw_compound scores. This creates a column that the raw \n",
    "    #where the translated raw compound scores will be placed (either a -1, 0, 1.)\n",
    "    def compound_binning(df):\n",
    "        df['compound_bin'] = df['raw_compound'] # Creates a column called 'compound_bin'\n",
    "\n",
    "        #del df['Unnamed: 0'] # deletes the column named 'Unnamed: 0'\n",
    "\n",
    "        #print(df.head())\n",
    "\n",
    "        # 70 converts the 'raw_compound' data to either a 1, 0 or -1. 1 if nltk sentiment number are >= .1; 0 if -.1 < x < .1 \n",
    "        #and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('raw_compound')] >= 0.1: # column 5 is 'raw_compound'\n",
    "                df.loc[i, ('compound_bin')] =  np.int(df.loc[i, ('raw_compound')] + .9) # column 6 is 'compound_bin'\n",
    "\n",
    "            if df.loc[i,('raw_compound')]  < .1 and df.loc[i,('raw_compound')] > -.1:\n",
    "                df.loc[i, ('compound_bin')] = 0   \n",
    "\n",
    "            if df.loc[i,('raw_compound')]  <= -.1:\n",
    "                df.loc[i, ('compound_bin')] =  np.int(df.loc[i,('raw_compound')] - .9)\n",
    "            i += 1\n",
    "\n",
    "        #print(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 80 Converts sentiment ratings into numerical values and put the value into 'sentiment_number'.\n",
    "    #Stocktwits sentiment rating (bullish or Bearish) is used as the standard;\n",
    "    #Stocktwits sentiment rating of 'None' is not used as a standard because people could have simply elected to not enter it.\n",
    "    #https://www.dataquest.io/blog/tutorial-add-column-pandas-dataframe-based-on-if-else-condition/\n",
    "    def convert_sentiment_to_numerical(df):\n",
    "\n",
    "        print('\\nconverting sentiment values to numerical values ...')\n",
    "        import numpy as np\n",
    "\n",
    "        conditions = [(df['sentiment'] == 'Bullish'), (df['sentiment'] == 'None'), (df['sentiment'] == 'Bearish')] #column 4 is 'sentiment'\n",
    "\n",
    "        values = [1.0, 0.0, -1.0]\n",
    "\n",
    "        %time df['sentiment_number'] = np.select(conditions, values)\n",
    "\n",
    "        df['modified_rating'] = 0 # adds a column \"modified_rating\" and sets it equal to 0\n",
    "        df['modified?'] = 'No' # adds a column \"modified?\" and sets it equal to 'No'\n",
    "\n",
    "\n",
    "        #print(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 90 Determines the percent correct and incorrect for the Vader sentiment values vs the stocktwits sentiment values\n",
    "    def vader_correct(df):\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        total = len(df)\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "            if df.loc[i, ('compound_bin')] == df.loc[i, ('sentiment_number')]: # column 6 is 'compound_bin' and column 7 is 'sentiment_number'\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1 \n",
    "\n",
    "            i += 1\n",
    "\n",
    "        print('The Vader percent correct to stocktwits raw data is:', int(100 * correct/total), '%')\n",
    "        print('The Vader percent incorrect to stocktwits raw data is:', int(100 * incorrect/total), '%')\n",
    "\n",
    "        #return df\n",
    "\n",
    "    # 100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "    def none_count_raw(df):\n",
    "        i = 0\n",
    "        sentiment_number = 0\n",
    "\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('sentiment')] == 'None': # column 4 is 'sentiment'\n",
    "                sentiment_number += 1\n",
    "            i += 1\n",
    "\n",
    "        print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "        print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "    # 110 This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "    #the 'None' proportions more equal. It also prints the ratios of each sentiment response to the total number\n",
    "    #of responses.\n",
    "    def remove_every_other(df):\n",
    "        i = 0\n",
    "        counter_before = 0\n",
    "        counter_after = 0\n",
    "        df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "\n",
    "        while i < len(df): #count the 'None' records before the drop.\n",
    "            if df.loc[i,('sentiment')] == 'None':\n",
    "                counter_before += 1\n",
    "            i += 1\n",
    "\n",
    "        print('\\nThe total number of records is: ', len(df))\n",
    "        print('The number of \"None\" stocktwits sentiment values before removal is:', counter_before)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('sentiment')] == 'None': #column 4 is sentiment\n",
    "                if i % 2 == 0: #identifies every even index where the sentiment is \"None\"\n",
    "                    #df = df.drop(df.index[i]) #drops (deletes) the record\n",
    "                    df = df.drop(df.index[i])\n",
    "                    df = df.reset_index(drop = True) #resets the index to be continuous\n",
    "\n",
    "                    i -= 1\n",
    "\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "\n",
    "        i = 0\n",
    "        counter_after = 0\n",
    "\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('sentiment')] == 'None':\n",
    "                counter_after += 1\n",
    "            i += 1\n",
    "\n",
    "        print('\\nThe total number of records is: ', len(df))\n",
    "        print('The number of \"None\" stocktwits sentiment values after removal is:', counter_after)\n",
    "        print('The percentage of \"None\" values is:', (int(counter_after/len(df) * 1000)/10), '%')\n",
    "\n",
    "        i = 0\n",
    "        sentiment_number = 0\n",
    "\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('sentiment')] == 'Bullish':\n",
    "                sentiment_number += 1\n",
    "            i += 1\n",
    "\n",
    "        print('The number of \"Bullish\" stocktwits sentiment values is:', sentiment_number)\n",
    "        print('The percentage of \"Bullish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "        i = 0\n",
    "        sentiment_number = 0\n",
    "\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('sentiment')] == 'Bearish':\n",
    "                sentiment_number += 1\n",
    "            i += 1\n",
    "\n",
    "        print('The number of \"Bearish\" stocktwits sentiment values is:', sentiment_number)\n",
    "        print('The percentage of \"Bearish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '% \\n')\n",
    "\n",
    "        return df    \n",
    "\n",
    "    # 115 Provides statistics on sentiments; bullish, none or bearish.\n",
    "    def stats(df):\n",
    "\n",
    "        i = 0\n",
    "        sentiment_number = 0\n",
    "\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('sentiment')] == 'None':\n",
    "                sentiment_number += 1\n",
    "            i += 1\n",
    "\n",
    "        print('The total number of records is: ', len(df))\n",
    "        print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "        print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "        i = 0\n",
    "        sentiment_number = 0\n",
    "\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('sentiment')] == 'Bullish':\n",
    "                sentiment_number += 1\n",
    "            i += 1\n",
    "\n",
    "        print('The number of \"Bullish\" stocktwits sentiment values is:', sentiment_number)\n",
    "        print('The percentage of \"Bullish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "        i = 0\n",
    "        sentiment_number = 0\n",
    "\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('sentiment')] == 'Bearish':\n",
    "                sentiment_number += 1\n",
    "            i += 1\n",
    "\n",
    "        print('The number of \"Bearish\" stocktwits sentiment values is:', sentiment_number)\n",
    "        print('The percentage of \"Bearish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "    # 120 Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "    # It counts every 20 edits and gives the user the option to quit. If the user chooses to quit\n",
    "    # it breaks from the while look and writes the df to a csv file so all work is saved up to that point.\n",
    "    # upon start up it ask if thie is the first time processing the raw data. If no it loads the csv file into\n",
    "    # the dataframe and starts where the previous session left off. If \"modified?\" is \"Yes and \"sentiment\" is \"None\"\n",
    "    # it skips the record. Therefore it will re-start at the first \"modified?\" is \"No\" and \"sentiment\" is \"None\"\n",
    "    def edit(df):\n",
    "\n",
    "        import copy\n",
    "\n",
    "        i = 0\n",
    "        counter = 0    # counter to see if user want to stop\n",
    "\n",
    "        while i < len(df):\n",
    "        #while i < 6:\n",
    "\n",
    "            if df.loc[i,('sentiment')] == 'None' and df.loc[i,('modified?')] == 'No': # Column 9 is 'modified?'\n",
    "                print('\\nindex number:', i, '\\n', df.loc[i, ('body')])\n",
    "                #print('This is the body of the tweet:\\n', df..log[i,('body')])\n",
    "                rating = int(input('Enter your rating (1, 0 or -1.):')) \n",
    "                df.loc[i,('modified_rating')] = copy.deepcopy(rating) # writes inputed number to the 'modified_rating'\n",
    "                df.loc[i,('modified?')] = 'Yes' # sets \"modified?\" equal to 'Yes' to identify which records have been modified; so that it can start at the next record at start up\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "            elif df.loc[i,('sentiment')] == 'Bearish':\n",
    "\n",
    "                df.loc[i,('modified_rating')] = df.loc[i,('sentiment_number')] #copies the stocktwits 'sentiment_number' (7) to the 'modified_rating(8)\n",
    "\n",
    "            elif df.loc[i,('sentiment')] == 'Bullish':\n",
    "\n",
    "                df.loc[i,('modified_rating')] = df.loc[i,('sentiment_number')] #copies the stocktwits 'sentiment_number' (7) to the 'modified_rating(8)\n",
    "\n",
    "            if counter == 20: # represents 20 edits\n",
    "                quit = input('Do you want to quit? (Enter either a \"y\" or \"Y\") ')\n",
    "                if quit == 'y' or quit == 'Y':\n",
    "                    print('You are exiting.')\n",
    "                    break\n",
    "                else:\n",
    "                    counter = 0 # resets the counter to 0 so there must be another 20 records reviewed and modified \n",
    "\n",
    "            i += 1\n",
    "\n",
    "        #df.to_csv(filename, index = False)\n",
    "        #print('The csv file was written. File name: ', filename)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 140 This will change the modified rating (8) to the nltk rating (6) only when they are opposite to see if it improves \n",
    "    #the accuracy number \n",
    "    def change_opp_nltk(df):\n",
    "\n",
    "        filename = 'tech stockTwit 02232021 opposite compound_bin vs modified_rating.csv'\n",
    "\n",
    "        print('The name of the csv file that will be written to is: ', filename)\n",
    "\n",
    "        correct_name = input('Is this the correct filename? (enter \"N\" or \"n\" for no)')\n",
    "\n",
    "        if correct_name == 'N' or correct_name == 'n':\n",
    "              new_name = input('What is the correct name?')\n",
    "              filename = new_name\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        import copy\n",
    "\n",
    "        counter = 0    # counter to see if user want to stop\n",
    "\n",
    "        while i < len(df):\n",
    "\n",
    "            if df.loc[i,('sentiment')] == -1 and df.loc[i,('modified_rating')] == 1:\n",
    "                df.loc[i,('modified_rating')] = copy.deepcopy(df.loc[i,('sentiment')]) # change \"modified_rating\" (8) to \"compound_bin\" (6)      \n",
    "\n",
    "            elif df.loc[i,('sentiment')] == 1 and df.loc[i,('modified_rating')] == -1:\n",
    "                df.loc[i,('modified_rating')] = copy.deepcopy(df.loc[i,('sentiment')]) # change \"modified_rating\" to \"compound_bin\"     \n",
    "\n",
    "            i += 1\n",
    "\n",
    "        df.to_csv(filename, index = False)\n",
    "        print('The csv file was written. File name: ', filename)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 180 counts how many \"None\" sentiment values are there for the stocktwits sentiment modified rating values\n",
    "    def none_count(df):\n",
    "        i = 0\n",
    "        sentiment_number = 0\n",
    "\n",
    "        while i < len(df):\n",
    "            if df.loc[i,('modified_rating')] == 0.0: # column #8 is 'modified_rating'\n",
    "                sentiment_number += 1\n",
    "            i +=1\n",
    "\n",
    "        print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "        print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "\n",
    "    #480 This removes words from the list of stopwords and writes list to csv file\n",
    "    # https://stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python#:~:text=one%20more%20easy%20way%20to%20remove%20words%20from,%3D%20words%20-%20stopwords%20final_list%20%3D%20list%20%28final_list%29\n",
    "    #new_words = list(filter(lambda w: w not in stop_words, initial_words))\n",
    "    def remove_from_stopwords(sw, relevant_path):\n",
    "        WordsToBeRem = ['no']\n",
    "        stopWords = list(filter(lambda w: w not in WordsToBeRem, sw)) #It will retain anyword in sw that is not in WordsToBeRemoved\n",
    "\n",
    "        #converts the stopword list to a df so that it can then be written to a csv file\n",
    "        df_stopwords = pd.DataFrame(stopWords, columns = ['stopwords'])\n",
    "        name_of_csv_file = relevant_path + '/' + 'stopwords.csv'\n",
    "        df_stopwords.to_csv(name_of_csv_file, index = False) #writes stopwords to csv file\n",
    "\n",
    "        #print(stopWords)\n",
    "\n",
    "        return stopWords\n",
    "\n",
    "    #490 Checks to see of the words were removed from the stopWords list.\n",
    "    #inputs: stopword list (sw) and the word to be removed from the so (WordToBeRem):\n",
    "    def check_stopwords(sw, WordToBeRem):\n",
    "\n",
    "        r = 0\n",
    "\n",
    "        for w in sw:\n",
    "            #print(w)\n",
    "            if w == WordToBeRem:\n",
    "                print('The word ', w , ' is still in the stopWords list!')\n",
    "                r += 1\n",
    "\n",
    "        if r == 0:\n",
    "            print('It did remove the words from the stopWords list!')\n",
    "\n",
    "        #print(len(stopWords))\n",
    "\n",
    "    #510 Removes stopwords from all the \"body\" text (tweets); to do this it must tokenize the string which means it must parse \n",
    "    # the string into individual words. It then compares the words with the words in the stopwords list and if there is not \n",
    "    # match it puts the word into the \"wordsFiltered\" list. It keeps appending to the list until all of the words are checked.\n",
    "    # It then joins the individual words back into a string.\n",
    "    #There is a difference between \"deep\" copy and \"shallow\" copy. \"Deep\" copy make a copy where the index and data are\n",
    "    # separate from the original. \"Shallow\" copy is like a pointer where the two df share a common index and data\n",
    "    #dfScrubbed = df #This is a shallow copy\n",
    "    def rem_stopwords(df, stopWords):\n",
    "\n",
    "        from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "        dfScrubbed = df.copy() #This is a deep copy. df.copy(deep = True); deep = True is default\n",
    "\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "\n",
    "            data = df.loc[i,('body')]\n",
    "            words = word_tokenize(data) # separates the string into a individual words.\n",
    "            wordsFiltered = []\n",
    "\n",
    "            for w in words:\n",
    "                if w not in stopWords:\n",
    "                    wordsFiltered.append(w) # makes a new word list without the stopwords\n",
    "\n",
    "            joinedWordsFiltered = ' '.join(wordsFiltered)\n",
    "\n",
    "            dfScrubbed.loc[i,('body')] = joinedWordsFiltered # replaces the recorded in dfScrubbed with the stopWords removed\n",
    "            # from the 'body'\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        #print(wordsFiltered)\n",
    "\n",
    "        #### method removes empty body rows and reindexes\n",
    "        dfScrubbed = remove_empty_body_rows(dfScrubbed)\n",
    "\n",
    "        #### checks to see if there are any empty records left\n",
    "        print('Are there any empty body records?')\n",
    "        empty = np.where(pd.isnull(dfScrubbed['body'])) #checks to see if there are any empty records in the column 'body'\n",
    "        print(empty)\n",
    "\n",
    "        #print(dfScrubbed.head())\n",
    "\n",
    "        return dfScrubbed\n",
    "\n",
    "    #550 converts the scrubbed_compound scores into a 1 significant figure integer from a float number; rounding up\n",
    "    # this is only needed if you are going to uses the 'scrubbed_compound' value as the label.\n",
    "    def int_conversion(dfs):\n",
    "        dfs['scrubbed_compound'] =  np.int64((dfs['scrubbed_compound'] + .05) * 10)\n",
    "\n",
    "    # 550 converts the 'scrubbed_compound' (column 10) data to either a 1, 0 or -1.  \n",
    "    # if nltk sentiment number are >= .1; 0 if -.1 < x < .1 and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "    # creates a new column called 'compound_bin' from the raw_compound scores\n",
    "    def bin_sentiment(dfs):\n",
    "        dfs['scrubbed_compound_bin'] = dfs['scrubbed_compound'] # creates a new column 'scrubbed_compound_bin' (column 11)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "            if dfs.loc[i,('scrubbed_compound')] >= 0.1: # column 10 is 'scrubbed_compound'\n",
    "                dfs.loc[i, ('scrubbed_compound_bin')] =  np.int(dfs.loc[i,('scrubbed_compound')] + .9) # column 11 is 'scurbbed_compound_bin'\n",
    "\n",
    "            if dfs.loc[i,('scrubbed_compound')] < .1 and dfs.loc[i,('scrubbed_compound')] > -.1:\n",
    "                dfs.iloc[i, 11] = 0   \n",
    "\n",
    "            if dfs.loc[i,('scrubbed_compound')] <= -.1:\n",
    "                dfs.loc[i, ('scrubbed_compound_bin')] =  np.int(dfs.loc[i,('scrubbed_compound')]  - .9)\n",
    "            i += 1\n",
    "\n",
    "        print(dfs)\n",
    "\n",
    "    #640 compares the first record (index = 0) raw data (\"body\" column) with scrubbed (stopwords removed) data\n",
    "    #inputs: df - original df; dfs - scrubbed df (stopwords removed)\n",
    "    def compare_scrubbed(df, dfs):\n",
    "        print(df.loc[0,('body')])\n",
    "        print(dfs.loc[0,('body')])\n",
    "\n",
    "    # 650 Loads and combines two different dataframes in df; this is to combine two input datasets where the 'none'\n",
    "    #values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "    def combine_dfs(df1, df2):\n",
    "\n",
    "        df = df1.append(df2)\n",
    "\n",
    "        print('The length of file 1 is:', len(df1))\n",
    "        print('The length of file 2 is:', len(df2))\n",
    "\n",
    "        print('The length of the combined dataframe is:', len(df))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 660 Writes a csv file\n",
    "    #input: df that is to be saved as a csv; output file name (eg 'tech stockTwit 03112021 dup advert stopwords.csv'\n",
    "    def write_csv(df, filename_output, relevant_path):\n",
    "\n",
    "        df.to_csv(relevant_path + '/' + filename_output, index = False, encoding = 'utf-8')\n",
    "        print('The csv file was written. File name: ', filename_output)\n",
    "\n",
    "    # displays a list of file with on a csv suffix       \n",
    "    def list_dir_files(relevant_path):\n",
    "        # https://clay-atlas.com/us/blog/2019/10/27/python-english-tutorial-solved-unicodeescape-error-escape-syntaxerror/?doing_wp_cron=1618286551.1528689861297607421875\n",
    "        #need to change \\ to /\n",
    "\n",
    "        import os\n",
    "\n",
    "        included_extensions = ['csv']\n",
    "        file_names = [fn for fn in os.listdir(relevant_path) # uses os.listdir to display only .csv files\n",
    "                  if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "        print('Path: ', relevant_path)\n",
    "\n",
    "        for f in file_names:\n",
    "            print(f)\n",
    "\n",
    "    # removes specific rows and resets the index\n",
    "    def remove_empty_body_rows(df):\n",
    "        df.dropna(subset=['body'], inplace=True) #drops empty body records\n",
    "        df = df.reset_index(drop = True) # resets the index\n",
    "        return df\n",
    "\n",
    "    #### checks to see if there are any empty records left\n",
    "    def empty_records_check(df):\n",
    "        print('Are there any empty body records?')\n",
    "        empty = np.where(pd.isnull(df['body'])) #checks to see if there are any empty records in the column 'body'\n",
    "\n",
    "        if empty[0].size == 0:\n",
    "            print('There are no empty records! \\n', empty)\n",
    "        else:\n",
    "            print('There are empty records ...\\n', empty)\n",
    "\n",
    "    #### Removes Imogis\n",
    "    def remove_emoji(string):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   u\"\\U0001f926-\\U0001f937\"\n",
    "                                   u\"\\U00010000-\\U0010ffff\"\n",
    "                                   u\"\\u2640-\\u2642\"\n",
    "                                   u\"\\u2600-\\u2B55\"\n",
    "                                   u\"\\u200d\"\n",
    "                                   u\"\\u23cf\"\n",
    "                                   u\"\\u23e9\"\n",
    "                                   u\"\\u231a\"\n",
    "                                   u\"\\ufe0f\"  # dingbats\n",
    "                                   u\"\\u3030\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', string)\n",
    "\n",
    "    # combines both names of file wanted to combing and writes csv file\n",
    "    def combine_two_files():\n",
    "        first_filename = input()\n",
    "\n",
    "    def rem_dup_adver_ever_oth_emoji(df):\n",
    "        #remove duplicates\n",
    "        r_d = input('Do you want to remove duplicates? [Press enter if no] ')\n",
    "        if r_d in yes_resp:\n",
    "            df = remove_duplicates(df) #return df; removes duplicates\n",
    "            remove_dupl = 'r_d '\n",
    "        else:\n",
    "            remove_dupl = ''\n",
    "\n",
    "        #remove advertisements\n",
    "        r_a = input('Do you want to remove advertisements? [Press enter if no] ')\n",
    "        if r_a in yes_resp:\n",
    "            df = filter_records(df) #returns df; removes addvertisements\n",
    "            remove_advertisements = 'r_a '\n",
    "        else:\n",
    "            remove_advertisements = ''\n",
    "\n",
    "        # 110 OPTIONAL: This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "        #the 'None' proportions more equal. It also prints the ratios of each sentiment response to the total number\n",
    "        #of responses.\n",
    "        r_e_o = input('Do you want to remove every other neutral sentiment record: [Press enter if no] ')\n",
    "        if r_e_o in yes_resp:\n",
    "            df = remove_every_other(df) #returns df\n",
    "            rem_every_other = 'r_e_o '\n",
    "        else:\n",
    "            rem_every_other = ''\n",
    "\n",
    "        # remove emojis\n",
    "        r_emoj = input('Do you want to remove emojis from the body records: [Press enter if no] ')\n",
    "        if r_emoj in yes_resp:\n",
    "            #print('location1')\n",
    "            i = 0\n",
    "            #print('location2')\n",
    "            while i < len(df):\n",
    "                #print('location3', i)\n",
    "                string = df.loc[i, ('body')]\n",
    "                #print('location4')\n",
    "                #print('original string: ', string)\n",
    "                new_string = remove_emoji(string)\n",
    "                #print('location5')\n",
    "                #print('new string: ', new_string)\n",
    "                df.loc[i, ('body')] = new_string\n",
    "                #print(df['body'][i])\n",
    "\n",
    "                r_emoji = 'r_emoj '\n",
    "\n",
    "                i += 1\n",
    "        else:\n",
    "            r_emoji = ''\n",
    "\n",
    "        return df, r_emoji, rem_every_other, remove_advertisements, remove_dupl\n",
    "\n",
    "\n",
    "    def vader_analysis(df): #performs Vader sentiment analysis and adds to df the compound binning and converts the stocktwits string value to a numerical value.\n",
    "        df = vader_sentiment(df) #returns df; adds column with Vader sentiment values ('raw_compound') from the 'body' column.\n",
    "        print('Produced Vader sentiment values.')\n",
    "\n",
    "        df = compound_binning(df) #returns df; adds a column where the raw_compound scores are translated into 1, 0 or -1 'compound_bin'\n",
    "        print('Completed the Vader compound binning.')\n",
    "\n",
    "        df = convert_sentiment_to_numerical(df) #returns df\n",
    "        print('Converted the Stocktwits sentiments to a numberical value (1,0,-1).')\n",
    "        print('\\nAll finished with the Vader sentiment analysis.\\n')\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files'\n",
    "#relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Preprocessing'\n",
    "\n",
    "\n",
    "print('Here is a list of the csv files to choose from: \\n')\n",
    "list_dir_files(relevant_path) # gives all of the file options in the relevant path.\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "name = input('\\nWhat file do you want to use? \\n')\n",
    "df = getData(relevant_path + '/' + name) #returns df; reads csv file into df\n",
    "print('Imported the csv file.')\n",
    "\n",
    "finviz_resp = input('Is this a file from scraping Finviz?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
