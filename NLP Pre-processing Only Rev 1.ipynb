{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 initializes the dataframe \"df\" and imports the csv into df; \n",
    "# 20 calls getdata to import the csv into the dataframe, 'dfAPI'\n",
    "# 30 removes any duplicate records; duplicate records imply bot records\n",
    "# 40 finds certain words in the strings ('body') and deletes the entire record.  \n",
    "# 50Vader sentiment analyzer\n",
    "# 60 creates a new column called 'compound_bin' from the raw_compound scores\n",
    "# 70 converts the 'raw_compound' data to either a 1, 0 or -1. 1 if nltk sentiment number are >= .1; 0 if -.1 < x < .1 \n",
    "# 80 Converts sentiment ratings into numerical values and put the value into 'sentiment_number'.\n",
    "# 90 Determines the percent correct and incorrect for the Vader sentiment values vs the stocktwits sentiment values\n",
    "# 100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "# 110 This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "# 115 Provides statistics on sentiments; bullish, none or bearish.\n",
    "# 120 Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "# 130 Loads a csv file into the df dfAPI and print out the first 21 records\n",
    "# 140 This will change the modified rating to the nltk rating only when they are opposite to see if it improves the accuracy\n",
    "#number \n",
    "# 150 imports the csv into the dataframe, 'dfAPI'\n",
    "# 160 converts the df columns of body and the label (compound or sentiment_number) into one list for each column\n",
    "#this is needed to be able to create the \n",
    "# 170 divides the whole data set into a 80/20 split into a training set and a test set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 210 takes the test_data list, the test_labels list and the predictions list and puts them into a df\n",
    "\n",
    "\n",
    "# 650 Loads and combines two different dataframes into dfAPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "\n",
    "# 10 initializes the dataframe \"df\" and imports the csv into df; \n",
    "# the argument is the name/address of the file.\n",
    "# https://stackoverflow.com/questions/33440805/pandas-dataframe-read-csv-on-bad-data\n",
    "def getData(name):\n",
    "    df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "    df1 = pd.read_csv(name, header = 0)\n",
    "    return df1\n",
    "\n",
    "    #df1 = pd.read_csv(name, warn_bad_lines=True, error_bad_lines=False)\n",
    "    #df1 = pd.read_csv(name, nrows = 150, warn_bad_lines=True, error_bad_lines=False)\n",
    "\n",
    "# 30 removes any duplicate records; duplicate records imply bot records\n",
    "def remove_duplicates(df):\n",
    "    df = df.drop_duplicates()\n",
    "    len(df)\n",
    "    return df\n",
    "\n",
    "# 40 finds certain words in the strings ('body') and deletes the entire record.\n",
    "#Note: When the record is deleted the df is re-indexed. The index for the while statement is not so the result is\n",
    "#that the record right after the deleted record is skipped. To remedy the problem the index (i) for the while statement \n",
    "#is decremented by one.\n",
    "#Also, the filtering terms are not case sensitive.\n",
    "def filter_records(df):\n",
    "    import fnmatch\n",
    "\n",
    "    data = []\n",
    "    counter = 0\n",
    "    advert = ['* sec *', '* daily News *', '*Huge Print*', '* Form *', '*SweepCast*', '*Large Print*', \n",
    "          '*Huge Print*', '*8-K*', '*SmartOptions*', '*Big Trade*', '*SEC Form*', '*Notice of Exempt*', \n",
    "          '*created_at*', '*stock news*', '*Trading Zones*', '*Entry:*', '*New Article*', '*ooc.bz*', \n",
    "          '*http*', 'Huge Trade', 'Trading is easy', 'www.', '#wallstreetbets', 'wallstreetbets',\n",
    "          'Huge Trade', '#unitedtraders', 'stockbeep.com'] # words or phrases whose records are to be removed; It is not case sensitive.\n",
    "\n",
    "    for a in advert:\n",
    "        i = 0\n",
    "        df = df.reset_index(drop = True) # resets the index before each iteration; removes the gaps; resets len(df)\n",
    "        while i < len(df):\n",
    "            dat = df.iloc[i,2] # 2 represents the 'body' column\n",
    "            data = [dat] # sets the string from the df into a list for the fnmatch.filter\n",
    "            #print('index = ', i)\n",
    "            filtered = fnmatch.filter(data, a) # compares the information in the 'body' column with the 'advert' list; it places the matched items in the 'filtered' variable.\n",
    "            #https://www.geeksforgeeks.org/fnmatch-unix-filename-pattern-matching-python/\n",
    "\n",
    "            if len(filtered) != 0: #if returns a True then record needs to be removed\n",
    "                counter += 1\n",
    "                #print('index:', i, df.iloc[i,2]) # prints the index number and record\n",
    "                #print(filtered, '\\n') # prints the entire record where there was a match (not wildcards were used)    \n",
    "                #print('before drop the next record is:', df.iloc[i+1, 2], 'i+1 = ', i + 1)\n",
    "            \n",
    "                df = df.drop(df.index[i]) # drops (deletes) the record\n",
    "            \n",
    "                #print('after the record is dropped:', df.iloc[i,2], 'i = ', i)\n",
    "                \n",
    "                #Note: When the record is dropped there is a change in the 'index' number. after the drop index number\n",
    "                #5 becomes index number 4. Since the counter increments one more time it skips the record right after\n",
    "                #the record that was just checked. That is why it takes multiple runs to remove all of the target\n",
    "                #records. To correct this decrement the index, i, by one\n",
    "                \n",
    "                i -= 1\n",
    "    \n",
    "            i += 1\n",
    "\n",
    "    df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "    len(df)\n",
    "    return df\n",
    "\n",
    "#50 Vader sentiment analyzer\n",
    "def vader_sentiment(df):\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "    f = lambda tweet: vader.polarity_scores(tweet)['compound']\n",
    "\n",
    "    df['raw_compound'] = df['body'].apply(f)\n",
    "\n",
    "    print('The number of clean records in the df are: ', len(df) , '\\n')\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 60 creates a new column called 'compound_bin' from the raw_compound scores. This creates a column that the raw \n",
    "#where the translated raw compound scores will be placed (either a -1, 0, 1.)\n",
    "def compound_binning(df):\n",
    "    df['compound_bin'] = df['raw_compound'] \n",
    "    \n",
    "    #del df['Unnamed: 0'] # deletes the column named 'Unnamed: 0'\n",
    "    \n",
    "    print(df.head())\n",
    "    \n",
    "    # 70 converts the 'raw_compound' data to either a 1, 0 or -1. 1 if nltk sentiment number are >= .1; 0 if -.1 < x < .1 \n",
    "    #and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,5] >= 0.1: # column 5 is 'raw_compound'\n",
    "            df.iloc[i, 6] =  np.int(df.iloc[i, 5] + .9) # column 6 is 'compound_bin'\n",
    "        \n",
    "        if df.iloc[i,5] < .1 and df.iloc[i, 5] > -.1:\n",
    "            df.iloc[i, 6] = 0   \n",
    "        \n",
    "        if df.iloc[i,5] <= -.1:\n",
    "            df.iloc[i, 6] =  np.int(df.iloc[i, 5] - .9)\n",
    "        i += 1\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 80 Converts sentiment ratings into numerical values and put the value into 'sentiment_number'.\n",
    "#Stocktwits sentiment rating (bullish or Bearish) is used as the standard;\n",
    "#Stocktwits sentiment rating of 'None' is not used as a standard because people could have simply elected to not enter it.\n",
    "#https://www.dataquest.io/blog/tutorial-add-column-pandas-dataframe-based-on-if-else-condition/\n",
    "def convert_sentiment_to_numerical(df):\n",
    "    import numpy as np\n",
    "\n",
    "    conditions = [(df['sentiment'] == 'Bullish'),\n",
    "                  (df['sentiment'] == 'None'),\n",
    "                  (df['sentiment'] == 'Bearish')]\n",
    "\n",
    "    values = [1.0, 0.0, -1.0]\n",
    "\n",
    "    df['sentiment_number'] = np.select(conditions, values)\n",
    "\n",
    "    df['modified_rating'] = 0 # adds a column \"modified_rating\" and sets it equal to 0\n",
    "    df['modified?'] = 'No' # adds a column \"modified?\" and sets it equal to 'No'\n",
    "\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 90 Determines the percent correct and incorrect for the Vader sentiment values vs the stocktwits sentiment values\n",
    "def vader_correct(df):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    total = len(df)\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if df.iloc[i, 6] == df.iloc[i, 7]: # column 6 is 'compound_bin' and column 7 is 'sentiment_number'\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1 \n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    print('The Vader percent correct to stocktwits raw data is:', int(100 * correct/total), '%')\n",
    "    print('The Vader percent incorrect to stocktwits raw data is:', int(100 * incorrect/total), '%')\n",
    "\n",
    "    #return df\n",
    "\n",
    "# 100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "def none_count_raw(df):\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "        \n",
    "# 110 This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "#the 'None' proportions more equal. It also prints the ratios of each sentiment response to the total number\n",
    "#of responses.\n",
    "def remove_every_other(df):\n",
    "    i = 0\n",
    "    counter = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'None':\n",
    "            if i % 2 == 0: #identifies every even index where the sentiment is \"None\"\n",
    "                df = df.drop(df.index[i]) #drops (deletes) the record\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('\\nThe total number of records is: ', len(df))\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'Bullish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bullish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bullish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "            \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'Bearish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bearish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bearish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '% \\n')\n",
    "            \n",
    "    return df    \n",
    "\n",
    "# 115 Provides statistics on sentiments; bullish, none or bearish.\n",
    "def stats(df):\n",
    "    \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The total number of records is: ', len(df))\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'Bullish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bullish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bullish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "            \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'Bearish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bearish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bearish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "            \n",
    "# 120 Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "# It counts every 20 edits and gives the user the option to quit. If the user chooses to quit\n",
    "# it breaks from the while look and writes the df to a csv file so all work is saved up to that point.\n",
    "# upon start up it ask if thie is the first time processing the raw data. If no it loads the csv file into\n",
    "# the dataframe and starts where the previous session left off. If \"modified?\" is \"Yes and \"sentiment\" is \"None\"\n",
    "# it skips the record. Therefore it will re-start at the first \"modified?\" is \"No\" and \"sentiment\" is \"None\"\n",
    "def edit(df):\n",
    "\n",
    "    import copy\n",
    "\n",
    "    filename = \"tech stockTwit 03112021 adjusted Rev1.csv\"\n",
    "    \n",
    "    print('The name of the csv file that will be written to is: ', filename)\n",
    "    \n",
    "    correct_name = input('Is this the correct filename? (enter \"N\" or \"n\" for no)')\n",
    "          \n",
    "    if correct_name == 'N' or correct_name == 'n':\n",
    "          new_name = input('What is the correct name?')\n",
    "          filename = new_name\n",
    "          \n",
    "    load = input('Is this the first time processing the raw stocktwits data (enter \"n\"/\"N\" or \"y\"/\"Y\")? ')\n",
    "    if load == 'n' or load == 'N' or load == 'no' or load == 'No':\n",
    "        df = getData(filename)\n",
    "        print('Loaded filename:', filename)\n",
    "    else:\n",
    "    \n",
    "        print('ok')\n",
    "    \n",
    "    i = 0\n",
    "    counter = 0    # counter to see if user want to stop\n",
    "\n",
    "    while i < len(df):\n",
    "    #while i < 6:\n",
    "\n",
    "        if df.iloc[i,4] == 'None' and df.iloc[i,9] == 'No':\n",
    "            print('\\nindex number:', i, '\\n', df.iloc[i, 2])\n",
    "            #print('This is the body of the tweet:\\n', df.iloc[i, 2])\n",
    "            rating = int(input('Enter your rating (1, 0 or -1.):')) \n",
    "            df.iloc[i,8] = copy.deepcopy(rating) # writes inputed number to the 'modified_rating'\n",
    "            df.iloc[i,9] = 'Yes' # sets \"modified?\" equal to 'Yes' to identify which records have been modified; so that it can start at the next record at start up\n",
    "        \n",
    "            counter += 1\n",
    "        \n",
    "        elif df.iloc[i,4] == 'Bearish':\n",
    "        #elif df.iloc[i,4] == 'Bearish' and df.iloc[i,9] == 'No': # the second condition is not needed\n",
    "\n",
    "            df.iloc[i,8] = df.iloc[i,7] #copies the stocktwits 'sentiment_number' to the 'modified_rating'\n",
    "        \n",
    "        elif df.iloc[i,4] == 'Bullish':\n",
    "        #elif df.iloc[i,4] == 'Bullish' and df.iloc[i,9] == 'No': # the second condition is not needed\n",
    "        \n",
    "            df.iloc[i,8] = df.iloc[i,7] #copies the stocktwits 'sentiment_number' to the 'modified_rating'\n",
    "\n",
    "        if counter == 20: # represents 20 edits\n",
    "            quit = input('Do you want to quit? (Enter either a \"y\" or \"Y\") ')\n",
    "            if quit == 'y' or quit == 'Y':\n",
    "                print('You are exiting.')\n",
    "                break\n",
    "            else:\n",
    "                counter = 0 # resets the counter to 0 so there must be another 20 records reviewed and modified \n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    df.to_csv(filename, index = False)\n",
    "    print('The csv file was written. File name: ', filename)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 140 This will change the modified rating to the nltk rating only when they are opposite to see if it improves \n",
    "#the accuracy number \n",
    "def change_opp_nltk(df):\n",
    "    \n",
    "    filename = 'tech stockTwit 02232021 opposite compound_bin vs modified_rating.csv'\n",
    "    \n",
    "    print('The name of the csv file that will be written to is: ', filename)\n",
    "    \n",
    "    correct_name = input('Is this the correct filename? (enter \"N\" or \"n\" for no)')\n",
    "          \n",
    "    if correct_name == 'N' or correct_name == 'n':\n",
    "          new_name = input('What is the correct name?')\n",
    "          filename = new_name\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    import copy\n",
    "\n",
    "    counter = 0    # counter to see if user want to stop\n",
    "\n",
    "    while i < len(df):\n",
    "\n",
    "        if df.iloc[i,6] == -1 and df.iloc[i, 8] == 1:\n",
    "            df.iloc[i,8] = copy.deepcopy(df.iloc[i, 6]) # change \"modified rating\" to \"compound_bin\"       \n",
    "        \n",
    "        elif df.iloc[i,6] == 1 and df.iloc[i, 8] == -1:\n",
    "            df.iloc[i,8] = copy.deepcopy(df.iloc[i, 6]) # change \"modified rating\" to \"compound_bin\"     \n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    df.to_csv(filename, index = False)\n",
    "    print('The csv file was written. File name: ', filename)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 180 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "def none_count(df):\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,8] == 0.0:\n",
    "            sentiment_number += 1\n",
    "        i +=1\n",
    "        \n",
    "    '''\n",
    "    while i < len(test_labels):\n",
    "        if test_labels[i] == 0.0:\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "    '''\n",
    "    \n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "    \n",
    "#440 sets up stopword removal; returns stopWords\n",
    "def set_up_nltk_stopword_removal():\n",
    "    #from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "\n",
    "    print(len(stopWords))\n",
    "    return stopWords\n",
    "\n",
    "#470 creates a list of new stopwords and then adds them to the set provided by nltk\n",
    "#Note: it is case sensitive\n",
    "#Input is the nltk stopword list (\"stopWords\")\n",
    "def add_new_stopwords(sw):\n",
    "    newStopWords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "    newStopWords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "    newStopWords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "    newStopWords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "    newStopWords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "    newStopWords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "    newStopWords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "    newStopWords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "    newStopWords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "    newStopWords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "    newStopWords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "    newStopWords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "    newStopWords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "    newStopWords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "    newStopWords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "    newStopWords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "    newStopWords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "    newStopWords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "    newStopWords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "    newStopWords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "    newStopWords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "    newStopWords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "    newStopWords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "    newStopWords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "    newStopWords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "    newStopWords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "    newStopWords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "    newStopWords += ['nevertheless', 'next', 'nine', 'nobody', 'none'] #removed 'no'\n",
    "    newStopWords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "    newStopWords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "    newStopWords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "    newStopWords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "    newStopWords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "    newStopWords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "    newStopWords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "    newStopWords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "    newStopWords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "    newStopWords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "    newStopWords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "    newStopWords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "    newStopWords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "    newStopWords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "    newStopWords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "    newStopWords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "    newStopWords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "    newStopWords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "    newStopWords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "    newStopWords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "    newStopWords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "    newStopWords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "    newStopWords += ['yours', 'yourself', 'yourselves'] #provided by Codecademy??\n",
    "\n",
    "    # additional stopwords:\n",
    "    newStopWords += ['[Screenshot]', '[screenshot]', 'Screenshot', '[Screenshot]Great', '[SCREENSHOT]', 'screenshot', \n",
    "                 'The', 'the', 'SMART', 'yah', 'got', 'nutty', 'moving', 'weeks', 'Got', 'So', 'today', 'Been', 'or']\n",
    "\n",
    "    newStopWords += ['I', 'it', 'It'] # pronouns\n",
    "\n",
    "    newStopWords += ['AMD', 'NVDA','NVDA', 'TSLA', 'GOOG', 'BA', 'FB', 'GOOGL', 'INTC', 'intel', 'Intel', 'CSCO', 'MU', \n",
    "                 'SMH', 'TSM','AAPL', 'TSLA', 'CSCO', 'POETF', 'PHOTONICS', 'DD', 'ARWR', 'T', 'INFI', 'AMC', 'ARK',\n",
    "                'GME', 'NIO', 'QS'] # Stock symbols or names\n",
    "\n",
    "    newStopWords += ['Readytogo123', 'Maddog68','Stocktwits'] # nouns\n",
    "\n",
    "    newStopWords += ['.', '?', '!', ';', ',', \"'\"] # punctuation\n",
    "\n",
    "    newStopWords += ['&', '#', '%', '$', '@'] # symbols\n",
    "\n",
    "    newStopWords += ['41.75', '530.05', '39', 'Two', 'two',] # numbers\n",
    "\n",
    "    #adds them to the stopWords list provided by nltk\n",
    "    for i in newStopWords:\n",
    "        sw.add(i) #stopWords is defined as a \"set\" in #450 when inputed as english words from nltk;\n",
    "        # sets cannot be ordered so it must be converted back to a list to be ordered or alphabetized. A set has no duplicate elements.\n",
    "\n",
    "    print(len(sw))\n",
    "    #print(stopWords)\n",
    "\n",
    "    #converts the set to a list\n",
    "    stopWords_list = list(sw)\n",
    "\n",
    "    #sorts the stopword list\n",
    "    stopWords_list.sort(key = lambda k : k.lower())\n",
    "    print(stopWords_list)\n",
    "    \n",
    "    return stopWords_list\n",
    "\n",
    "#480 This removes words from the list of stopwords and writes list to csv file\n",
    "# https://stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python#:~:text=one%20more%20easy%20way%20to%20remove%20words%20from,%3D%20words%20-%20stopwords%20final_list%20%3D%20list%20%28final_list%29\n",
    "#new_words = list(filter(lambda w: w not in stop_words, initial_words))\n",
    "def remove_from_stopwords(sw):\n",
    "    WordsToBeRem = ['no']\n",
    "    stopWords = list(filter(lambda w: w not in WordsToBeRem, sw)) #sw has been sorted in #470\n",
    "\n",
    "    #converts the stopword list to a df and then outputs the df to a csv file\n",
    "    df_stopwords = pd.DataFrame(stopWords, columns = ['stopwords'])\n",
    "    df_stopwords.to_csv('stopwords.csv', index = False)\n",
    "\n",
    "    print(stopWords)\n",
    "    \n",
    "    return stopWords\n",
    "\n",
    "#490 Checks to see of the words were removed from the stopWords list.\n",
    "#inputs: stopword list: output from def remove_from_stopwords(sw); the word to be removed\n",
    "def check_stopwords(sw, WordToBeRem):\n",
    "    \n",
    "    r = 0\n",
    "\n",
    "    for w in sw:\n",
    "        #print(w)\n",
    "        if w in WordToBeRem:\n",
    "            print('The word ', w , ' is still in the stopWords list!')\n",
    "            r += 1\n",
    "\n",
    "    if r == 0:\n",
    "        print('It did remove the words from the stopWords list!')\n",
    "    \n",
    "    #print(len(stopWords))\n",
    "\n",
    "#510 Removes stopwords from all the \"body\" text (tweets); to do this it must tokenize the string which means it must parse \n",
    "# the string into individual words. It then compares the words with the words in the stopwords list and if there is not \n",
    "# match it puts the word into the \"wordsFiltered\" list. It keeps appending to the list until all of the words are checked.\n",
    "# It then joins the individual words back into a string.\n",
    "\n",
    "#There is a difference between \"deep\" copy and \"shallow\" copy. \"Deep\" copy make a copy where the index and data are\n",
    "# separate from the original. \"Shallow\" copy is like a pointer where the two df share a common index and data\n",
    "#dfScrubbed = df #This is a shallow copy\n",
    "def rem_stopwords(df, stopWords):\n",
    "    \n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    \n",
    "    dfScrubbed = df.copy() #This is a deep copy. df.copy(deep = True); deep = True is default\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "    \n",
    "        data = df.iloc[i,2]\n",
    "        words = word_tokenize(data)\n",
    "        wordsFiltered = []\n",
    "\n",
    "        for w in words:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "    \n",
    "        joinedWordsFiltered = ' '.join(wordsFiltered)\n",
    "    \n",
    "        dfScrubbed.iloc[i,2] = joinedWordsFiltered # replaces the recorded in dfScrubbed with the stopWords removed\n",
    "        # from the 'body'\n",
    "    \n",
    "        i += 1\n",
    "    \n",
    "    #print(wordsFiltered)\n",
    "\n",
    "    print(dfScrubbed.head())\n",
    "\n",
    "    #print(joinedWordsFiltered)\n",
    "    \n",
    "    return dfScrubbed\n",
    "\n",
    "#550 converts the scrubbed_compound scores into a 1 significant figure integer from a float number; rounding up\n",
    "# this is only needed if you are going to uses the 'scrubbed_compound' value as the label.\n",
    "def int_conversion(dfs):\n",
    "    dfs['scrubbed_compound'] =  np.int64((dfs['scrubbed_compound'] + .05) * 10)\n",
    "\n",
    "# 550 converts the 'scrubbed_compound' (column 10) data to either a 1, 0 or -1.  \n",
    "# if nltk sentiment number are >= .1; 0 if -.1 < x < .1 and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "# creates a new column called 'compound_bin' from the raw_compound scores\n",
    "def bin_sentiment(dfs):\n",
    "    dfs['scrubbed_compound_bin'] = dfs['scrubbed_compound'] # creates a new column 'scrubbed_compound_bin' (column 11)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if dfs.iloc[i,10] >= 0.1: # column 10 is 'scrubbed_compound'\n",
    "            dfs.iloc[i, 11] =  np.int(dfs.iloc[i, 10] + .9) # column 11 is 'scurbbed_compound_bin'\n",
    "        \n",
    "        if dfs.iloc[i,10] < .1 and dfs.iloc[i, 10] > -.1:\n",
    "            dfs.iloc[i, 11] = 0   \n",
    "        \n",
    "        if dfs.iloc[i,10] <= -.1:\n",
    "            dfs.iloc[i, 11] =  np.int(dfs.iloc[i, 10] - .9)\n",
    "        i += 1\n",
    "    \n",
    "    print(dfs)\n",
    "\n",
    "# compares the first record (index = 0) raw data (\"body\" column) with scrubbed (stopwords removed) data\n",
    "#inputs: df - original df; dfs - scrubbed df (stopwords removed)\n",
    "def compare_scrubbed(df, dfs):\n",
    "    print(df.iloc[0,2])\n",
    "    print(dfs.iloc[0,2])\n",
    "\n",
    "# 650 Loads and combines two different dataframes in df; this is to combine two input datasets where the 'none'\n",
    "#values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "def combine_dfs():\n",
    "\n",
    "    filename1 = \"tech stockTwit 03112021 adjusted-Copy1.csv\"\n",
    "    filename2 = \"tech stockTwit 02232021 adjusted-Copy1.csv\"\n",
    "\n",
    "    df1 = getData(filename1)\n",
    "    df2 = getData(filename2)\n",
    "\n",
    "    df = df1.append(df2)\n",
    "\n",
    "    print('The length of file 1 is:', len(df1))\n",
    "    print('The length of file 2 is:', len(df2))\n",
    "\n",
    "    print('The length of the combined dataframe is:', len(df))\n",
    "\n",
    "# Writes a csv file\n",
    "#input: df that is to be saved as a csv; output file name (eg 'tech stockTwit 03112021 dup advert stopwords.csv'\n",
    "def write_csv(df, filename_output):\n",
    "    df.to_csv(filename_output, index = False)\n",
    "    #df.to_csv(filename_output, index = False)\n",
    "    print('The csv file was written. File name: ', filename_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of clean records in the df are:  2418 \n",
      "\n",
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T21:01:03Z   \n",
      "1   INTC  2021-03-05T21:01:03Z   \n",
      "2   INTC  2021-03-05T21:00:02Z   \n",
      "3   INTC  2021-03-05T20:51:14Z   \n",
      "4   INTC  2021-03-05T20:06:56Z   \n",
      "\n",
      "                                                body followers sentiment  \\\n",
      "0  $INTC Big Trade - $16 399 800.270 000 shares a...       862      None   \n",
      "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None   \n",
      "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None   \n",
      "3               $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "\n",
      "   raw_compound  \n",
      "0        0.2960  \n",
      "1        0.0000  \n",
      "2        0.3182  \n",
      "3        0.0000  \n",
      "4        0.0000  \n",
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T21:01:03Z   \n",
      "1   INTC  2021-03-05T21:01:03Z   \n",
      "2   INTC  2021-03-05T21:00:02Z   \n",
      "3   INTC  2021-03-05T20:51:14Z   \n",
      "4   INTC  2021-03-05T20:06:56Z   \n",
      "\n",
      "                                                body followers sentiment  \\\n",
      "0  $INTC Big Trade - $16 399 800.270 000 shares a...       862      None   \n",
      "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None   \n",
      "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None   \n",
      "3               $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "\n",
      "   raw_compound  compound_bin  \n",
      "0        0.2960        0.2960  \n",
      "1        0.0000        0.0000  \n",
      "2        0.3182        0.3182  \n",
      "3        0.0000        0.0000  \n",
      "4        0.0000        0.0000  \n",
      "     symbol            created_at  \\\n",
      "0      INTC  2021-03-05T21:01:03Z   \n",
      "1      INTC  2021-03-05T21:01:03Z   \n",
      "2      INTC  2021-03-05T21:00:02Z   \n",
      "3      INTC  2021-03-05T20:51:14Z   \n",
      "4      INTC  2021-03-05T20:06:56Z   \n",
      "...     ...                   ...   \n",
      "2495     MU  2021-02-24T12:44:23Z   \n",
      "2496     MU  2021-02-24T12:42:03Z   \n",
      "2497     MU  2021-02-24T12:38:21Z   \n",
      "2498     MU  2021-02-24T12:10:44Z   \n",
      "2499     MU  2021-02-24T12:10:09Z   \n",
      "\n",
      "                                                   body followers sentiment  \\\n",
      "0     $INTC Big Trade - $16 399 800.270 000 shares a...       862      None   \n",
      "1     Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None   \n",
      "2     Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None   \n",
      "3                  $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "4                   $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "...                                                 ...       ...       ...   \n",
      "2495    $MU 90 w ay to easy，let&#39;s see 100 this week         1      None   \n",
      "2496  $AMD $NVDA $INTC $MU $QCOM..BUY STHC reverse m...       191   Bullish   \n",
      "2497  $MU Premarket looking promising  bears can get...        55   Bullish   \n",
      "2498                                 $MU DXI up over 3%        55   Bullish   \n",
      "2499  $MU Looking very good again  buy with conviction.        55   Bullish   \n",
      "\n",
      "      raw_compound  compound_bin  \n",
      "0           0.2960           1.0  \n",
      "1           0.0000           0.0  \n",
      "2           0.3182           1.0  \n",
      "3           0.0000           0.0  \n",
      "4           0.0000           0.0  \n",
      "...            ...           ...  \n",
      "2495        0.0000           0.0  \n",
      "2496        0.7249           1.0  \n",
      "2497        0.4574           1.0  \n",
      "2498        0.0000           0.0  \n",
      "2499        0.4927           1.0  \n",
      "\n",
      "[2418 rows x 7 columns]\n",
      "     symbol            created_at  \\\n",
      "0      INTC  2021-03-05T21:01:03Z   \n",
      "1      INTC  2021-03-05T21:01:03Z   \n",
      "2      INTC  2021-03-05T21:00:02Z   \n",
      "3      INTC  2021-03-05T20:51:14Z   \n",
      "4      INTC  2021-03-05T20:06:56Z   \n",
      "...     ...                   ...   \n",
      "2495     MU  2021-02-24T12:44:23Z   \n",
      "2496     MU  2021-02-24T12:42:03Z   \n",
      "2497     MU  2021-02-24T12:38:21Z   \n",
      "2498     MU  2021-02-24T12:10:44Z   \n",
      "2499     MU  2021-02-24T12:10:09Z   \n",
      "\n",
      "                                                   body followers sentiment  \\\n",
      "0     $INTC Big Trade - $16 399 800.270 000 shares a...       862      None   \n",
      "1     Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None   \n",
      "2     Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None   \n",
      "3                  $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "4                   $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "...                                                 ...       ...       ...   \n",
      "2495    $MU 90 w ay to easy，let&#39;s see 100 this week         1      None   \n",
      "2496  $AMD $NVDA $INTC $MU $QCOM..BUY STHC reverse m...       191   Bullish   \n",
      "2497  $MU Premarket looking promising  bears can get...        55   Bullish   \n",
      "2498                                 $MU DXI up over 3%        55   Bullish   \n",
      "2499  $MU Looking very good again  buy with conviction.        55   Bullish   \n",
      "\n",
      "      raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "0           0.2960           1.0               0.0                0        No  \n",
      "1           0.0000           0.0               0.0                0        No  \n",
      "2           0.3182           1.0               0.0                0        No  \n",
      "3           0.0000           0.0               1.0                0        No  \n",
      "4           0.0000           0.0               1.0                0        No  \n",
      "...            ...           ...               ...              ...       ...  \n",
      "2495        0.0000           0.0               0.0                0        No  \n",
      "2496        0.7249           1.0               1.0                0        No  \n",
      "2497        0.4574           1.0               1.0                0        No  \n",
      "2498        0.0000           0.0               1.0                0        No  \n",
      "2499        0.4927           1.0               1.0                0        No  \n",
      "\n",
      "[2418 rows x 10 columns]\n",
      "179\n",
      "442\n",
      "['!', '#', '$', '%', '&', \"'\", ',', '.', '39', '41.75', '530.05', ';', '?', '@', '[SCREENSHOT]', '[screenshot]', '[Screenshot]', '[Screenshot]Great', 'a', 'AAPL', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'ain', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'AMC', 'AMD', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'aren', \"aren't\", 'ARK', 'around', 'ARWR', 'as', 'at', 'BA', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'Been', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldn', \"couldn't\", 'couldnt', 'cry', 'CSCO', 'd', 'DD', 'de', 'describe', 'detail', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'FB', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'GME', 'go', 'GOOG', 'GOOGL', 'got', 'Got', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'hasnt', 'have', 'haven', \"haven't\", 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'I', 'ie', 'if', 'in', 'inc', 'indeed', 'INFI', 'INTC', 'Intel', 'intel', 'interest', 'into', 'is', 'isn', \"isn't\", 'It', 'it', \"it's\", 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'll', 'ltd', 'm', 'ma', 'Maddog68', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mightn', \"mightn't\", 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'moving', 'MU', 'much', 'must', 'mustn', \"mustn't\", 'my', 'myself', 'name', 'namely', 'needn', \"needn't\", 'neither', 'never', 'nevertheless', 'next', 'nine', 'NIO', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'nutty', 'NVDA', 'o', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'PHOTONICS', 'please', 'POETF', 'put', 'QS', 'rather', 're', 'Readytogo123', 's', 'same', 'screenshot', 'Screenshot', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'SMART', 'SMH', 'So', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'Stocktwits', 'such', 'system', 't', 'T', 'take', 'ten', 'than', 'that', \"that'll\", 'The', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'today', 'together', 'too', 'top', 'toward', 'towards', 'TSLA', 'TSM', 'twelve', 'twenty', 'two', 'Two', 'un', 'under', 'until', 'up', 'upon', 'us', 've', 'very', 'via', 'was', 'wasn', \"wasn't\", 'we', 'weeks', 'well', 'were', 'weren', \"weren't\", 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'won', \"won't\", 'would', 'wouldn', \"wouldn't\", 'y', 'yah', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "['!', '#', '$', '%', '&', \"'\", ',', '.', '39', '41.75', '530.05', ';', '?', '@', '[SCREENSHOT]', '[screenshot]', '[Screenshot]', '[Screenshot]Great', 'a', 'AAPL', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'ain', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'AMC', 'AMD', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'aren', \"aren't\", 'ARK', 'around', 'ARWR', 'as', 'at', 'BA', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'Been', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldn', \"couldn't\", 'couldnt', 'cry', 'CSCO', 'd', 'DD', 'de', 'describe', 'detail', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'FB', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'GME', 'go', 'GOOG', 'GOOGL', 'got', 'Got', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'hasnt', 'have', 'haven', \"haven't\", 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'I', 'ie', 'if', 'in', 'inc', 'indeed', 'INFI', 'INTC', 'Intel', 'intel', 'interest', 'into', 'is', 'isn', \"isn't\", 'It', 'it', \"it's\", 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'll', 'ltd', 'm', 'ma', 'Maddog68', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mightn', \"mightn't\", 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'moving', 'MU', 'much', 'must', 'mustn', \"mustn't\", 'my', 'myself', 'name', 'namely', 'needn', \"needn't\", 'neither', 'never', 'nevertheless', 'next', 'nine', 'NIO', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'nutty', 'NVDA', 'o', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'PHOTONICS', 'please', 'POETF', 'put', 'QS', 'rather', 're', 'Readytogo123', 's', 'same', 'screenshot', 'Screenshot', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'SMART', 'SMH', 'So', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'Stocktwits', 'such', 'system', 't', 'T', 'take', 'ten', 'than', 'that', \"that'll\", 'The', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'today', 'together', 'too', 'top', 'toward', 'towards', 'TSLA', 'TSM', 'twelve', 'twenty', 'two', 'Two', 'un', 'under', 'until', 'up', 'upon', 'us', 've', 'very', 'via', 'was', 'wasn', \"wasn't\", 'we', 'weeks', 'well', 'were', 'weren', \"weren't\", 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'won', \"won't\", 'would', 'wouldn', \"wouldn't\", 'y', 'yah', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "The word  o  is still in the stopWords list!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T20:51:14Z   \n",
      "1   INTC  2021-03-05T20:06:56Z   \n",
      "2   INTC  2021-03-05T19:57:20Z   \n",
      "3   INTC  2021-03-05T19:52:43Z   \n",
      "4   INTC  2021-03-05T19:36:13Z   \n",
      "\n",
      "                                                body followers sentiment  \\\n",
      "0                              common follow ur sibs        48   Bullish   \n",
      "1                                 ITT ADBE OPTT GLBS       575   Bullish   \n",
      "2  Should thankful bull market Dow couple reachin...        21   Bullish   \n",
      "3  ButterFingerDROPs sell October Still probably ...        77      None   \n",
      "4                       At rate left like flew - : (        11      None   \n",
      "\n",
      "   raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "0        0.0000           0.0               1.0              1.0        No  \n",
      "1        0.0000           0.0               1.0              1.0        No  \n",
      "2        0.6996           1.0               1.0              1.0        No  \n",
      "3        0.0000           0.0               0.0              1.0       Yes  \n",
      "4        0.5574           1.0               0.0              1.0       Yes  \n",
      "$AMD common follow ur sibs $INTC $MU\n",
      "common follow ur sibs\n",
      "The csv file was written. File name:  tech stockTwit 03112021 adjusted Rev1.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "name = 'tech stockTwit 03112021.csv'\n",
    "df = getData(name) #returns df; reads csv file into df\n",
    "\n",
    "#OPTIONAL:\n",
    "df = remove_duplicates(df) #return df; removes duplicates\n",
    "\n",
    "#OPTIONAL:\n",
    "#df = filter_records(df) #returns df; removes addvertisements\n",
    "\n",
    "# NOT OPTIONAL\n",
    "df = vader_sentiment(df) #returns df; adds column with Vader sentiment values ('raw_compound') from the 'body' column.\n",
    "\n",
    "# NOT OPTIONAL\n",
    "df = compound_binning(df) #returns df; adds a column where the raw_compound scores are translated into 1, 0 or -1 'compound_bin'\n",
    "\n",
    "#80 NOT OPTIONAL\n",
    "df = convert_sentiment_to_numerical(df) #returns df\n",
    "\n",
    "# 90 OPTIONAL\n",
    "#vader_correct(df) \n",
    "\n",
    "# 100 OPTIONAL: Counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "#none_count_raw(df) \n",
    "\n",
    "# 110 OPTIONAL: This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "#the 'None' proportions more equal. It also prints the ratios of each sentiment response to the total number\n",
    "#of responses.\n",
    "#df = remove_every_other(df) #returns df\n",
    "\n",
    "# 115 OPTIONAL: Provides statistics on sentiments; bullish, none or bearish.\n",
    "#stats(df) \n",
    "\n",
    "\n",
    "# 120 OPTIONAL: Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "# It counts every 20 edits and gives the user the option to quit. If the user chooses to quit\n",
    "# it breaks from the while look and writes the df to a csv file so all work is saved up to that point.\n",
    "# upon start up it ask if thie is the first time processing the raw data. If no it loads the csv file into\n",
    "# the dataframe and starts where the previous session left off. If \"modified?\" is \"Yes and \"sentiment\" is \"None\"\n",
    "# it skips the record. Therefore it will re-start at the first \"modified?\" is \"No\" and \"sentiment\" is \"None\"\n",
    "#df = edit(df) #returns df\n",
    "\n",
    "# OPTIONAL:\n",
    "csv_name = 'tech stockTwit 03112021 adjusted Rev1.csv'\n",
    "df = getData(csv_name)\n",
    "\n",
    "# 180 OPTIONAL: counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "#none_count(df) \n",
    "\n",
    "# 140 OPTIONAL: This will change the modified rating to the nltk rating only when they are opposite to see if it improves \n",
    "#the accuracy number \n",
    "#df = change_opp_nltk(df) #returns df\n",
    "\n",
    "# 180 OPTIONAL: counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "#none_count(df) \n",
    "\n",
    "#440 imports the nltk stopword list that holds the stopwords that will be removed from the text ('body.') \n",
    "sw = set_up_nltk_stopword_removal() \n",
    "\n",
    "#470 creates a list of new stopwords and then adds them to the set provided by nltk\n",
    "#Note  it is case sensitive\n",
    "#Input is the nltk stopword list (\"stopWords\")\n",
    "sw = add_new_stopwords(sw) \n",
    "\n",
    "#480 This removes words from the list of stopwords and writes list to csv file\n",
    "# https //stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python# ~ text=one%20more%20easy%20way%20to%20remove%20words%20from,%3D%20words%20-%20stopwords%20final_list%20%3D%20list%20%28final_list%29\n",
    "#new_words = list(filter(lambda w  w not in stop_words, initial_words))\n",
    "sw = remove_from_stopwords(sw) \n",
    "#return stopWords\n",
    "\n",
    "#490 Checks to see of the words were removed from the stopWords list.\n",
    "#inputs  stopword list  output from  remove_from_stopwords(sw); the word to be removed\n",
    "check_stopwords(sw, 'no') \n",
    "\n",
    "#510 Removes stopwords from all the \"body\" text (tweets); to do this it must tokenize the string which means it must parse \n",
    "# the string into individual words. It then compares the words with the words in the stopwords list and if there is not \n",
    "# match it puts the word into the \"wordsFiltered\" list. It keeps appending to the list until all of the words are checked.\n",
    "# It then joins the individual words back into a string.\n",
    "#There is a difference between \"deep\" copy and \"shallow\" copy. \"Deep\" copy make a copy where the index and data are\n",
    "# separate from the original. \"Shallow\" copy is like a pointer where the two df share a common index and data\n",
    "#dfAPIScrubbed = dfAPI #This is a shallow copy\n",
    "dfScrubbed = rem_stopwords(df, sw) \n",
    "#return dfScrubbed\n",
    "\n",
    "#550 converts the scrubbed_compound scores into a 1 significant figure integer from a float number; rounding up\n",
    "# this is only needed if you are going to uses the 'scrubbed_compound' value as the label.\n",
    "#int_conversion(dfs) #return df\n",
    "\n",
    "# compares the first record (index = 0) raw data (\"body\" column) with scrubbed (stopwords removed) data\n",
    "#inputs  df - original df; dfs - scrubbed df (stopwords removed)\n",
    "dfs = dfScrubbed\n",
    "compare_scrubbed(df, dfs) \n",
    "\n",
    "# 650 Loads and combines two different dataframes in dfAPI; this is to combine two input datasets where the 'none'\n",
    "#values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "#combine_dfs() \n",
    "\n",
    "# Writes a csv file\n",
    "#input  df that is to be saved as a csv; output file name (eg 'tech stockTwit 03112021 dup advert stopwords.csv'\n",
    "#filename_output = 'tech stockTwit 03112021 dup.csv'\n",
    "filename_output = 'tech stockTwit 03112021 adjusted Rev1.csv'\n",
    "\n",
    "write_csv(df, filename_output) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     symbol            created_at  \\\n",
      "0      INTC  2021-03-05T20:51:14Z   \n",
      "1      INTC  2021-03-05T20:06:56Z   \n",
      "2      INTC  2021-03-05T19:57:20Z   \n",
      "3      INTC  2021-03-05T19:52:43Z   \n",
      "4      INTC  2021-03-05T19:36:13Z   \n",
      "...     ...                   ...   \n",
      "1286     MU  2021-02-24T13:22:35Z   \n",
      "1287     MU  2021-02-24T12:48:31Z   \n",
      "1288     MU  2021-02-24T12:38:21Z   \n",
      "1289     MU  2021-02-24T12:10:44Z   \n",
      "1290     MU  2021-02-24T12:10:09Z   \n",
      "\n",
      "                                                   body followers sentiment  \\\n",
      "0                  $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "1                   $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "2     $INTC Should be thankful we are in this bull m...        21   Bullish   \n",
      "3     @ButterFingerDROPs $INTC had its sell off back...        77      None   \n",
      "4     $AMD At this rate  this will be left behind by...        11      None   \n",
      "...                                                 ...       ...       ...   \n",
      "1286  $MU bought on 89$.My only regret i miss averag...         2   Bullish   \n",
      "1287  $MU today is going to be another back breaker🚀...        37   Bullish   \n",
      "1288  $MU Premarket looking promising  bears can get...        55   Bullish   \n",
      "1289                                 $MU DXI up over 3%        55   Bullish   \n",
      "1290  $MU Looking very good again  buy with conviction.        55   Bullish   \n",
      "\n",
      "      raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "0           0.0000           0.0               1.0                0        No  \n",
      "1           0.0000           0.0               1.0                0        No  \n",
      "2           0.6996           1.0               1.0                0        No  \n",
      "3           0.0000           0.0               0.0                0        No  \n",
      "4           0.5574           1.0               0.0                0        No  \n",
      "...            ...           ...               ...              ...       ...  \n",
      "1286       -0.5267          -1.0               1.0                0        No  \n",
      "1287        0.0000           0.0               1.0                0        No  \n",
      "1288        0.4574           1.0               1.0                0        No  \n",
      "1289        0.0000           0.0               1.0                0        No  \n",
      "1290        0.4927           1.0               1.0                0        No  \n",
      "\n",
      "[1291 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#df = df.reset_index(drop = True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of \"None\" stocktwits sentiment values is: 481\n",
      "The percentage of \"None\" values is: 37.2 %\n"
     ]
    }
   ],
   "source": [
    "# 100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "none_count_raw(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T21:01:03Z   \n",
      "1   INTC  2021-03-05T21:01:03Z   \n",
      "2   INTC  2021-03-05T21:00:02Z   \n",
      "3   INTC  2021-03-05T20:51:14Z   \n",
      "4   INTC  2021-03-05T20:06:56Z   \n",
      "5   INTC  2021-03-05T19:57:20Z   \n",
      "6   INTC  2021-03-05T19:52:43Z   \n",
      "7   INTC  2021-03-05T19:44:47Z   \n",
      "8   INTC  2021-03-05T19:36:13Z   \n",
      "9   INTC  2021-03-05T19:27:49Z   \n",
      "\n",
      "                                                body followers sentiment  \n",
      "0  $INTC Big Trade - $16 399 800.270 000 shares a...       862      None  \n",
      "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None  \n",
      "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None  \n",
      "3               $AMD common follow ur sibs $INTC $MU        48   Bullish  \n",
      "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish  \n",
      "5  $INTC Should be thankful we are in this bull m...        21   Bullish  \n",
      "6  @ButterFingerDROPs $INTC had its sell off back...        77      None  \n",
      "7  $INTC  Trading is easy with Buy and Short sign...       162      None  \n",
      "8  $AMD At this rate  this will be left behind by...        11      None  \n",
      "9  I sold all my $AMD shares and moved the money ...       134   Bullish  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHODS IN INDIVIDUAL CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 initializes the dataframe \"df\" and imports the csv into df; \n",
    "# the argument is the name/address of the file.\n",
    "# https://stackoverflow.com/questions/33440805/pandas-dataframe-read-csv-on-bad-data\n",
    "def getData(name):\n",
    "    df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "    df1 = pd.read_csv(name, header = 0)\n",
    "    return df1\n",
    "\n",
    "    #df1 = pd.read_csv(name, warn_bad_lines=True, error_bad_lines=False)\n",
    "    #df1 = pd.read_csv(name, nrows = 150, warn_bad_lines=True, error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 removes any duplicate records; duplicate records imply bot records\n",
    "def remove_duplicates(df):\n",
    "    df = df.drop_duplicates()\n",
    "    len(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 finds certain words in the strings ('body') and deletes the entire record.\n",
    "#Note: When the record is deleted the df is re-indexed. The index for the while statement is not so the result is\n",
    "#that the record right after the deleted record is skipped. To remedy the problem the index (i) for the while statement \n",
    "#is decremented by one.\n",
    "#Also, the filtering terms are not case sensitive.\n",
    "\n",
    "def filter_records(df):\n",
    "    import fnmatch\n",
    "\n",
    "    data = []\n",
    "    counter = 0\n",
    "    advert = ['* sec *', '* daily News *', '*Huge Print*', '* Form *', '*SweepCast*', '*Large Print*', \n",
    "          '*Huge Print*', '*8-K*', '*SmartOptions*', '*Big Trade*', '*SEC Form*', '*Notice of Exempt*', \n",
    "          '*created_at*', '*stock news*', '*Trading Zones*', '*Entry:*', '*New Article*', '*ooc.bz*', \n",
    "          '*http*', 'Huge Trade', 'Trading is easy', 'www.', '#wallstreetbets', 'wallstreetbets',\n",
    "          'Huge Trade', '#unitedtraders', 'stockbeep.com'] # words or phrases whose records are to be removed; It is not case sensitive.\n",
    "\n",
    "    for a in advert:\n",
    "        i = 0\n",
    "        df = df.reset_index(drop = True) # resets the index before each iteration; removes the gaps; resets len(dfAPI)\n",
    "        while i < len(df):\n",
    "            dat = df.iloc[i,2] # 2 represents the 'body' column\n",
    "            data = [dat] # sets the string from the df into a list for the fnmatch.filter\n",
    "            #print('index = ', i)\n",
    "            filtered = fnmatch.filter(data, a) # compares the information in the 'body' column with the 'advert' list; it places the matched items in the 'filtered' variable.\n",
    "            #https://www.geeksforgeeks.org/fnmatch-unix-filename-pattern-matching-python/\n",
    "\n",
    "            if len(filtered) != 0: #if returns a True then record needs to be removed\n",
    "                counter += 1\n",
    "                #print('index:', i, df.iloc[i,2]) # prints the index number and record\n",
    "                #print(filtered, '\\n') # prints the entire record where there was a match (not wildcards were used)    \n",
    "                #print('before drop the next record is:', df.iloc[i+1, 2], 'i+1 = ', i + 1)\n",
    "            \n",
    "                df = df.drop(df.index[i]) # drops (deletes) the record\n",
    "            \n",
    "                #print('after the record is dropped:', df.iloc[i,2], 'i = ', i)\n",
    "                \n",
    "                #Note: When the record is dropped there is a change in the 'index' number. after the drop index number\n",
    "                #5 becomes index number 4. Since the counter increments one more time it skips the record right after\n",
    "                #the record that was just checked. That is why it takes multiple runs to remove all of the target\n",
    "                #records. To correct this decrement the index, i, by one\n",
    "                \n",
    "                i -= 1\n",
    "    \n",
    "            i += 1\n",
    "\n",
    "    df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "    len(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50 Vader sentiment analyzer\n",
    "\n",
    "def vader_sentiment(df):\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "    f = lambda tweet: vader.polarity_scores(tweet)['compound']\n",
    "\n",
    "    df['raw_compound'] = df['body'].apply(f)\n",
    "\n",
    "    print('The number of clean records in the df are: ', len(df) , '\\n')\n",
    "    print(df.head())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60 creates a new column called 'compound_bin' from the raw_compound scores. This creates a column that the raw \n",
    "#where the translated raw compound scores will be placed (either a -1, 0, 1.)\n",
    "\n",
    "def compound_binning(df):\n",
    "    df['compound_bin'] = df['raw_compound'] \n",
    "    \n",
    "    #del dfAPI['Unnamed: 0'] # deletes the column named 'Unnamed: 0'\n",
    "    \n",
    "    print(dfAPI.head())\n",
    "    \n",
    "    # 70 converts the 'raw_compound' data to either a 1, 0 or -1. 1 if nltk sentiment number are >= .1; 0 if -.1 < x < .1 \n",
    "    #and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "\n",
    "    i = 0\n",
    "    while i < len(dfAPI):\n",
    "        if df.iloc[i,5] >= 0.1: # column 5 is 'raw_compound'\n",
    "            df.iloc[i, 6] =  np.int(df.iloc[i, 5] + .9) # column 6 is 'compound_bin'\n",
    "        \n",
    "        if df.iloc[i,5] < .1 and df.iloc[i, 5] > -.1:\n",
    "            df.iloc[i, 6] = 0   \n",
    "        \n",
    "        if df.iloc[i,5] <= -.1:\n",
    "            df.iloc[i, 6] =  np.int(df.iloc[i, 5] - .9)\n",
    "        i += 1\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80 Converts sentiment ratings into numerical values and put the value into 'sentiment_number'.\n",
    "#Stocktwits sentiment rating (bullish or Bearish) is used as the standard;\n",
    "#Stocktwits sentiment rating of 'None' is not used as a standard because people could have simply elected to not enter it.\n",
    "#https://www.dataquest.io/blog/tutorial-add-column-pandas-dataframe-based-on-if-else-condition/\n",
    "\n",
    "def convert_sentiment_to_numerical(df):\n",
    "    import numpy as np\n",
    "\n",
    "    conditions = [(df['sentiment'] == 'Bullish'),\n",
    "                  (df['sentiment'] == 'None'),\n",
    "                  (df['sentiment'] == 'Bearish')]\n",
    "\n",
    "    values = [1.0, 0.0, -1.0]\n",
    "\n",
    "    df['sentiment_number'] = np.select(conditions, values)\n",
    "\n",
    "    df['modified_rating'] = 0 # adds a column \"modified_rating\" and sets it equal to 0\n",
    "    df['modified?'] = 'No' # adds a column \"modified?\" and sets it equal to 'No'\n",
    "\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Vader percent correct to stocktwits raw data is: 40 %\n",
      "The Vader percent incorrect to stocktwits raw data is: 59 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ncorrect = 0\\nincorrect = 0\\ntotal = len(dfAPI)\\ni = 0\\nwhile i < len(dfAPI):\\n    if dfAPI.iloc[i, 7] == dfAPI.iloc[i, 9]:\\n        correct += 1\\n    else:\\n        incorrect += 1 \\n        \\n    i += 1\\n\\nprint('The Vader percent correct compared to stocktwit enhanced is:', int(100 * correct/total), '%')\\nprint('The Vader percent incorrect compared to stocktwits enhanced is:', int(100 * incorrect/total), '%')\\n\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90 Determines the percent correct and incorrect for the Vader sentiment values vs the stocktwits sentiment values\n",
    "\n",
    "def vader_correct(df):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    total = len(df)\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if df.iloc[i, 6] == df.iloc[i, 7]: # column 6 is 'compound_bin' and column 7 is 'sentiment_number'\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1 \n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    print('The Vader percent correct to stocktwits raw data is:', int(100 * correct/total), '%')\n",
    "    print('The Vader percent incorrect to stocktwits raw data is:', int(100 * incorrect/total), '%')\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of \"None\" stocktwits sentiment values is: 1408\n",
      "The percentage of \"None\" values is: 56.3 %\n"
     ]
    }
   ],
   "source": [
    "# 100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "\n",
    "def none_count(df):\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(dfAPI):\n",
    "        if df.iloc[i,4] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(dfAPI) * 1000)/10), '%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 110 This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "#the 'None' proportions more equal. It also prints the ratios of each sentiment response to the total number\n",
    "#of responses.\n",
    "def remove_every_other(df):\n",
    "    i = 0\n",
    "    counter = 0\n",
    "\n",
    "    while i < len(dfAPI):\n",
    "        if dfAPI.iloc[i,4] == 'None':\n",
    "            if i % 2 == 0: #identifies every even index where the sentiment is \"None\"\n",
    "                dfAPI = dfAPI.drop(dfAPI.index[i]) #drops (deletes) the record\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "    \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(dfAPI):\n",
    "        if dfAPI.iloc[i,4] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('\\n The total number of records is: ', len(dfAPI))\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(dfAPI) * 1000)/10), '%')\n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(dfAPI):\n",
    "        if dfAPI.iloc[i,4] == 'Bullish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bullish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bullish\" values is:', (int(sentiment_number/len(dfAPI) * 1000)/10), '%')\n",
    "            \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(dfAPI):\n",
    "        if dfAPI.iloc[i,4] == 'Bearish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bearish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bearish\" values is:', (int(sentiment_number/len(dfAPI) * 1000)/10), '%')\n",
    "            \n",
    "    return df    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to methods below this point ******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of records is:  2500\n",
      "The number of \"None\" stocktwits sentiment values is: 1408\n",
      "The percentage of \"None\" values is: 56.3 %\n",
      "The number of \"Bullish\" stocktwits sentiment values is: 840\n",
      "The percentage of \"Bullish\" values is: 33.6 %\n",
      "The number of \"Bearish\" stocktwits sentiment values is: 169\n",
      "The percentage of \"Bearish\" values is: 6.7 %\n"
     ]
    }
   ],
   "source": [
    "# 115 Provides statistics on sentiments; bullish, none or bearish.\n",
    "\n",
    "def stats(df):\n",
    "    \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(dfAPI):\n",
    "        if dfAPI.iloc[i,4] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The total number of records is: ', len(dfAPI))\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(dfAPI) * 1000)/10), '%')\n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(dfAPI):\n",
    "        if dfAPI.iloc[i,4] == 'Bullish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bullish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bullish\" values is:', (int(sentiment_number/len(dfAPI) * 1000)/10), '%')\n",
    "            \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(dfAPI):\n",
    "        if dfAPI.iloc[i,4] == 'Bearish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bearish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bearish\" values is:', (int(sentiment_number/len(dfAPI) * 1000)/10), '%')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 120 Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "# It counts every 20 edits and gives the user the option to quit. If the user chooses to quit\n",
    "# it breaks from the while look and writes the df to a csv file so all work is saved up to that point.\n",
    "# upon start up it ask if thie is the first time processing the raw data. If no it loads the csv file into\n",
    "# the dataframe and starts where the previous session left off. If \"modified?\" is \"Yes and \"sentiment\" is \"None\"\n",
    "# it skips the record. Therefore it will re-start at the first \"modified?\" is \"No\" and \"sentiment\" is \"None\"\n",
    "\n",
    "def edit(df):\n",
    "\n",
    "    import copy\n",
    "\n",
    "    filename = \"tech stockTwit 03112021 adjusted Rev1.csv\"\n",
    "    \n",
    "    print('The name of the csv file that will be written to is: ', filename)\n",
    "    \n",
    "    correct_name = input('Is this the correct filename? (enter \"N\" or \"n\" for no)')\n",
    "          \n",
    "    if correct_name == 'N' or correct_name == 'n':\n",
    "          new_name = input('What is the correct name?')\n",
    "          filename = new_name\n",
    "\n",
    "    load = input('Is this the first time processing the raw stocktwits data (enter \"n\"/\"N\" or \"y\"/\"Y\")? ')\n",
    "    if load == 'n' or load == 'N' or load == 'no' or load == 'No':\n",
    "        df = getData(filename)\n",
    "        print('Loaded filename:', filename)\n",
    "    else:\n",
    "    \n",
    "        print('ok')\n",
    "    \n",
    "    i = 0\n",
    "    counter = 0    # counter to see if user want to stop\n",
    "\n",
    "    while i < len(df):\n",
    "    #while i < 6:\n",
    "\n",
    "        if df.iloc[i,4] == 'None' and df.iloc[i,9] == 'No':\n",
    "            print('\\nindex number:', i, '\\n', df.iloc[i, 2])\n",
    "            #print('This is the body of the tweet:\\n', df.iloc[i, 2])\n",
    "            rating = int(input('Enter your rating (1, 0 or -1.):')) \n",
    "            df.iloc[i,8] = copy.deepcopy(rating) # writes inputed number to the 'modified_rating'\n",
    "            df.iloc[i,9] = 'Yes' # sets \"modified?\" equal to 'Yes' to identify which records have been modified; so that it can start at the next record at start up\n",
    "        \n",
    "            counter += 1\n",
    "        \n",
    "        elif df.iloc[i,4] == 'Bearish':\n",
    "        #elif df.iloc[i,4] == 'Bearish' and df.iloc[i,9] == 'No': # the second condition is not needed\n",
    "\n",
    "            df.iloc[i,8] = df.iloc[i,7] #copies the stocktwits 'sentiment_number' to the 'modified_rating'\n",
    "        \n",
    "        elif df.iloc[i,4] == 'Bullish':\n",
    "        #elif df.iloc[i,4] == 'Bullish' and df.iloc[i,9] == 'No': # the second condition is not needed\n",
    "        \n",
    "            df.iloc[i,8] = df.iloc[i,7] #copies the stocktwits 'sentiment_number' to the 'modified_rating'\n",
    "\n",
    "        if counter == 20: # represents 20 edits\n",
    "            quit = input('Do you want to quit? (Enter either a \"y\" or \"Y\") ')\n",
    "            if quit == 'y' or quit == 'Y':\n",
    "                print('You are exiting.')\n",
    "                break\n",
    "            else:\n",
    "                counter = 0 # resets the counter to 0 so there must be another 20 records reviewed and modified \n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    df.to_csv(filename, index = False)\n",
    "    print('The csv file was written. File name: ', filename)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 140 This will change the modified rating to the nltk rating only when they are opposite to see if it improves \n",
    "#the accuracy number \n",
    "\n",
    "def change_opp_nltk(df):\n",
    "    filename = 'tech stockTwit 02232021 opposite compound_bin vs modified_rating.csv'\n",
    "\n",
    "    print('The name of the csv file that will be written to is: ', filename)\n",
    "    \n",
    "    correct_name = input('Is this the correct filename? (enter \"N\" or \"n\" for no)')\n",
    "          \n",
    "    if correct_name == 'N' or correct_name == 'n':\n",
    "          new_name = input('What is the correct name?')\n",
    "          filename = new_name\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    import copy\n",
    "\n",
    "    counter = 0    # counter to see if user want to stop\n",
    "\n",
    "    while i < len(df):\n",
    "\n",
    "        if df.iloc[i,6] == -1 and df.iloc[i, 8] == 1:\n",
    "            df.iloc[i,8] = copy.deepcopy(df.iloc[i, 6]) # change \"modified rating\" to \"compound_bin\"       \n",
    "        \n",
    "        elif df.iloc[i,6] == 1 and df.iloc[i, 8] == -1:\n",
    "            df.iloc[i,8] = copy.deepcopy(df.iloc[i, 6]) # change \"modified rating\" to \"compound_bin\"     \n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    df.to_csv(filename, index = False)\n",
    "    print('The csv file was written. File name: ', filename)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 180 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "\n",
    "def none_count(df):\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,8] == 0.0:\n",
    "            sentiment_number += 1\n",
    "        i +=1\n",
    "        \n",
    "    '''\n",
    "    while i < len(test_labels):\n",
    "        if test_labels[i] == 0.0:\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "    '''\n",
    "    \n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#440 sets up stopword removal; returns stopWords\n",
    "def set_up_nltk_stopword_removal():\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "\n",
    "    print(len(stopWords))\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "['!', '#', '$', '%', '&', \"'\", ',', '.', '39', '41.75', '530.05', ';', '?', '@', '[Screenshot]', '[SCREENSHOT]', '[screenshot]', '[Screenshot]Great', 'a', 'AAPL', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'ain', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'AMC', 'AMD', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'aren', \"aren't\", 'ARK', 'around', 'ARWR', 'as', 'at', 'BA', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'Been', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldn', \"couldn't\", 'couldnt', 'cry', 'CSCO', 'd', 'DD', 'de', 'describe', 'detail', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'FB', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'GME', 'go', 'GOOG', 'GOOGL', 'got', 'Got', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'hasnt', 'have', 'haven', \"haven't\", 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'I', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'INFI', 'INTC', 'Intel', 'intel', 'interest', 'into', 'is', 'isn', \"isn't\", 'It', 'it', \"it's\", 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'll', 'ltd', 'm', 'ma', 'Maddog68', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mightn', \"mightn't\", 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'moving', 'MU', 'much', 'must', 'mustn', \"mustn't\", 'my', 'myself', 'name', 'namely', 'needn', \"needn't\", 'neither', 'never', 'nevertheless', 'next', 'nine', 'NIO', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'nutty', 'NVDA', 'o', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'PHOTONICS', 'please', 'POETF', 'put', 'QS', 'rather', 're', 'Readytogo123', 's', 'same', 'Screenshot', 'screenshot', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'SMART', 'SMH', 'So', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'Stocktwits', 'such', 'system', 't', 'T', 'take', 'ten', 'than', 'that', \"that'll\", 'The', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'today', 'together', 'too', 'top', 'toward', 'towards', 'TSLA', 'TSM', 'twelve', 'twenty', 'two', 'Two', 'un', 'under', 'until', 'up', 'upon', 'us', 've', 'very', 'via', 'was', 'wasn', \"wasn't\", 'we', 'weeks', 'well', 'were', 'weren', \"weren't\", 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'won', \"won't\", 'would', 'wouldn', \"wouldn't\", 'y', 'yah', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "#470 creates a list of new stopwords and then adds them to the set provided by nltk\n",
    "#Note: it is case sensitive\n",
    "#Input is the nltk stopword list (\"stopWords\")\n",
    "\n",
    "def add_new_stopwords(sw):\n",
    "    newStopWords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "    newStopWords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "    newStopWords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "    newStopWords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "    newStopWords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "    newStopWords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "    newStopWords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "    newStopWords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "    newStopWords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "    newStopWords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "    newStopWords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "    newStopWords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "    newStopWords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "    newStopWords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "    newStopWords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "    newStopWords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "    newStopWords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "    newStopWords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "    newStopWords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "    newStopWords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "    newStopWords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "    newStopWords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "    newStopWords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "    newStopWords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "    newStopWords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "    newStopWords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "    newStopWords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "    newStopWords += ['nevertheless', 'next', 'nine', 'nobody', 'none'] #removed 'no'\n",
    "    newStopWords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "    newStopWords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "    newStopWords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "    newStopWords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "    newStopWords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "    newStopWords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "    newStopWords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "    newStopWords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "    newStopWords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "    newStopWords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "    newStopWords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "    newStopWords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "    newStopWords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "    newStopWords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "    newStopWords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "    newStopWords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "    newStopWords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "    newStopWords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "    newStopWords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "    newStopWords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "    newStopWords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "    newStopWords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "    newStopWords += ['yours', 'yourself', 'yourselves'] #provided by Codecademy??\n",
    "\n",
    "    # additional stopwords:\n",
    "    newStopWords += ['[Screenshot]', '[screenshot]', 'Screenshot', '[Screenshot]Great', '[SCREENSHOT]', 'screenshot', \n",
    "                 'The', 'the', 'SMART', 'yah', 'got', 'nutty', 'moving', 'weeks', 'Got', 'So', 'today', 'Been', 'or']\n",
    "\n",
    "    newStopWords += ['I', 'it', 'It'] # pronouns\n",
    "\n",
    "    newStopWords += ['AMD', 'NVDA','NVDA', 'TSLA', 'GOOG', 'BA', 'FB', 'GOOGL', 'INTC', 'intel', 'Intel', 'CSCO', 'MU', \n",
    "                 'SMH', 'TSM','AAPL', 'TSLA', 'CSCO', 'POETF', 'PHOTONICS', 'DD', 'ARWR', 'T', 'INFI', 'AMC', 'ARK',\n",
    "                'GME', 'NIO', 'QS'] # Stock symbols or names\n",
    "\n",
    "    newStopWords += ['Readytogo123', 'Maddog68','Stocktwits'] # nouns\n",
    "\n",
    "    newStopWords += ['.', '?', '!', ';', ',', \"'\"] # punctuation\n",
    "\n",
    "    newStopWords += ['&', '#', '%', '$', '@'] # symbols\n",
    "\n",
    "    newStopWords += ['41.75', '530.05', '39', 'Two', 'two',] # numbers\n",
    "\n",
    "    #adds them to the stopWords list provided by nltk\n",
    "    for i in newStopWords:\n",
    "        sw.add(i) #stopWords is defined as a \"set\" in #450 when inputed as english words from nltk;\n",
    "        # sets cannot be ordered so it must be converted back to a list to be ordered or alphabetized. A set has no duplicate elements.\n",
    "\n",
    "    print(len(sw))\n",
    "    #print(stopWords)\n",
    "\n",
    "    #converts the set to a list\n",
    "    stopWords_list = list(sw)\n",
    "\n",
    "    #sorts the stopword list\n",
    "    stopWords_list.sort(key = lambda k : k.lower())\n",
    "    print(stopWords_list)\n",
    "    \n",
    "    return stopWords_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', '$', '%', '&', \"'\", ',', '.', '39', '41.75', '530.05', ';', '?', '@', '[Screenshot]', '[SCREENSHOT]', '[screenshot]', '[Screenshot]Great', 'a', 'AAPL', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'ain', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'AMC', 'AMD', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'aren', \"aren't\", 'ARK', 'around', 'ARWR', 'as', 'at', 'BA', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'Been', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldn', \"couldn't\", 'couldnt', 'cry', 'CSCO', 'd', 'DD', 'de', 'describe', 'detail', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'FB', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'GME', 'go', 'GOOG', 'GOOGL', 'got', 'Got', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'hasnt', 'have', 'haven', \"haven't\", 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'I', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'INFI', 'INTC', 'Intel', 'intel', 'interest', 'into', 'is', 'isn', \"isn't\", 'It', 'it', \"it's\", 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'll', 'ltd', 'm', 'ma', 'Maddog68', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mightn', \"mightn't\", 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'moving', 'MU', 'much', 'must', 'mustn', \"mustn't\", 'my', 'myself', 'name', 'namely', 'needn', \"needn't\", 'neither', 'never', 'nevertheless', 'next', 'nine', 'NIO', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'nutty', 'NVDA', 'o', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'PHOTONICS', 'please', 'POETF', 'put', 'QS', 'rather', 're', 'Readytogo123', 's', 'same', 'Screenshot', 'screenshot', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'SMART', 'SMH', 'So', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'Stocktwits', 'such', 'system', 't', 'T', 'take', 'ten', 'than', 'that', \"that'll\", 'The', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'today', 'together', 'too', 'top', 'toward', 'towards', 'TSLA', 'TSM', 'twelve', 'twenty', 'two', 'Two', 'un', 'under', 'until', 'up', 'upon', 'us', 've', 'very', 'via', 'was', 'wasn', \"wasn't\", 'we', 'weeks', 'well', 'were', 'weren', \"weren't\", 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'won', \"won't\", 'would', 'wouldn', \"wouldn't\", 'y', 'yah', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# this also works; from same link as above\\nWordsToBeRem = ['no', 'mill']\\nfor remword in list(stopWords):\\n    if remword in WordsToBeRem:\\n        stopWords_list.remove(remword)\\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#480 This removes words from the list of stopwords and writes list to csv file\n",
    "# https://stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python#:~:text=one%20more%20easy%20way%20to%20remove%20words%20from,%3D%20words%20-%20stopwords%20final_list%20%3D%20list%20%28final_list%29\n",
    "#new_words = list(filter(lambda w: w not in stop_words, initial_words))\n",
    "\n",
    "def remove_from_stopwords(sw):\n",
    "    WordsToBeRem = ['no']\n",
    "    stopWords = list(filter(lambda w: w not in WordsToBeRem, sw)) #sw has been sorted in #470\n",
    "\n",
    "    #converts the stopword list to a df and then outputs the df to a csv file\n",
    "    df_stopwords = pd.DataFrame(stopWords, columns = ['stopwords'])\n",
    "    df_stopwords.to_csv('stopwords.csv', index = False)\n",
    "\n",
    "    print(stopWords)\n",
    "    \n",
    "    return stopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It did remove the words from the stopWords list!\n"
     ]
    }
   ],
   "source": [
    "#490 Checks to see of the words were removed from the stopWords list.\n",
    "#inputs: stopword list: output from def remove_from_stopwords(sw); the word to be removed\n",
    "\n",
    "def check_stopwords(sw, WordToBeRem):\n",
    "    \n",
    "    r = 0\n",
    "\n",
    "    for w in sw:\n",
    "        #print(w)\n",
    "        if w in WordToBeRem:\n",
    "            print('The word ', w , ' is still in the stopWords list!')\n",
    "            r += 1\n",
    "\n",
    "    if r == 0:\n",
    "        print('It did remove the words from the stopWords list!')\n",
    "    \n",
    "    #print(len(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T20:51:14Z   \n",
      "1   INTC  2021-03-05T20:06:56Z   \n",
      "2   INTC  2021-03-05T19:57:20Z   \n",
      "3   INTC  2021-03-05T19:52:43Z   \n",
      "4   INTC  2021-03-05T19:44:47Z   \n",
      "\n",
      "                                                body followers sentiment  \\\n",
      "0                              common follow ur sibs        48   Bullish   \n",
      "1                                 ITT ADBE OPTT GLBS       575   Bullish   \n",
      "2  Should thankful bull market Dow couple reachin...        21   Bullish   \n",
      "3  ButterFingerDROPs sell October Still probably ...        77      None   \n",
      "4           Trading easy Buy Short signals real time       162      None   \n",
      "\n",
      "   raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "0        0.0000           0.0               1.0                0        No  \n",
      "1        0.0000           0.0               1.0                0        No  \n",
      "2        0.6996           1.0               1.0                0        No  \n",
      "3        0.0000           0.0               0.0                0        No  \n",
      "4        0.4404           1.0               0.0                0        No  \n"
     ]
    }
   ],
   "source": [
    "#510 Removes stopwords from all the \"body\" text (tweets); to do this it must tokenize the string which means it must parse \n",
    "# the string into individual words. It then compares the words with the words in the stopwords list and if there is not \n",
    "# match it puts the word into the \"wordsFiltered\" list. It keeps appending to the list until all of the words are checked.\n",
    "# It then joins the individual words back into a string.\n",
    "\n",
    "#There is a difference between \"deep\" copy and \"shallow\" copy. \"Deep\" copy make a copy where the index and data are\n",
    "# separate from the original. \"Shallow\" copy is like a pointer where the two df share a common index and data\n",
    "#dfAPIScrubbed = dfAPI #This is a shallow copy\n",
    "\n",
    "def rem_stopwords(df, stopWords):\n",
    "\n",
    "    dfScrubbed = dfAPI.copy() #This is a deep copy. dfAPI.copy(deep = True); deep = True is default\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "    \n",
    "        data = df.iloc[i,2]\n",
    "        words = word_tokenize(data)\n",
    "        wordsFiltered = []\n",
    "\n",
    "        for w in words:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "    \n",
    "        joinedWordsFiltered = ' '.join(wordsFiltered)\n",
    "    \n",
    "        dfScrubbed.iloc[i,2] = joinedWordsFiltered # replaces the recorded in dfAPIScrubbed with the stopWords removed\n",
    "        # from the 'body'\n",
    "    \n",
    "        i += 1\n",
    "    \n",
    "    #print(wordsFiltered)\n",
    "\n",
    "    print(dfScrubbed.head())\n",
    "\n",
    "    #print(joinedWordsFiltered)\n",
    "    \n",
    "    return dfScrubbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AMD common follow ur sibs $INTC $MU\n",
      "common follow ur sibs\n"
     ]
    }
   ],
   "source": [
    "# compares the first record (index = 0) raw data (\"body\" column) with scrubbed (stopwords removed) data\n",
    "#inputs: df - original df; dfs - scrubbed df (stopwords removed)\n",
    "def compare_scrubbed(df, dfs):\n",
    "    print(df.iloc[0,2])\n",
    "    print(dfs.iloc[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of compound_bin scores that are different than the modified_rating: 749\n",
      "The total number of records is: 1291\n"
     ]
    }
   ],
   "source": [
    "#530 Compares the compound_bin[6] (nltk) to the modified_rating[8] (modified_rating is where the \"none\" or '0.0' rating\n",
    "#is re-assessed. this was done  to account for tweets where the person had a sentiment but did not bother \n",
    "#to input a sentiment)\n",
    "\n",
    "i = 0\n",
    "counter = 0\n",
    "\n",
    "while i < len(dfAPIScrubbed):\n",
    "    #if dfAPIScrubbed.iloc[i, 6] / 10 - dfAPIScrubbed.iloc[i, 7] != 0:\n",
    "    if dfAPIScrubbed.iloc[i, 6] - int((dfAPIScrubbed.iloc[i, 8])) != 0: # column 6 is 'compound_bin'; 8 is 'modified_rating'\n",
    "\n",
    "        #print(i, int(dfAPIScrubbed.iloc[i, 6]), int((dfAPIScrubbed.iloc[i, 8])))\n",
    "        counter += 1\n",
    "    i += 1\n",
    "print('The number of compound_bin scores that are different than the modified_rating:', counter)\n",
    "print('The total number of records is:', len(dfAPIScrubbed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the index number.:100\n",
      "While yesterday’s market action looks ominous  I would like to remind people that the federal government will approve another 1.9 billion of stimulus  more than $2.5B in 2 months.  The fed will continue to buy treasuries and for that reason you should not care about the 10 year note.  Much of this stimulus will end up back in the market.  The smartmoney have this completely wrong.  ..Despite the huge drop yesterday  I am still up 36% in 2021.  If you can afford to  take advantage of the sales.  Here are my main holdings and the performance since 4th qtr. 2020; not including INTC. ..Buy the fear!..SBH 66.6%.$PRTY 187.7%.$PERI 184.2%.AOSL 163.7%.ACLS 66.2%.$PRPL 44.0%.$FNF 23.3%.$INTC 23.9%\n",
      "While yesterday ’ market action looks ominous like remind people federal government approve 1.9 billion stimulus 2.5B 2 months fed continue buy treasuries reason care 10 year note Much stimulus end market smartmoney completely wrong .. Despite huge drop yesterday 36 2021 If afford advantage sales Here main holdings performance 4th qtr 2020 including .. Buy fear .. SBH 66.6 PRTY 187.7 PERI 184.2 .AOSL 163.7 .ACLS 66.2 PRPL 44.0 FNF 23.3 23.9\n",
      "Original nltk sentiment: -0.1\n",
      "Scrubbed nltk sentiment: 1.0\n"
     ]
    }
   ],
   "source": [
    "#540 Pulls up the record of interest; you must enter the index number.\n",
    "\n",
    "index = int(input('Enter the index number.:')) \n",
    "\n",
    "print(dfAPI.iloc[index,2])\n",
    "print(dfAPIScrubbed.iloc[index,2])\n",
    "\n",
    "print('Original nltk sentiment:', dfAPI.iloc[index,6] / 10)\n",
    "print('Scrubbed nltk sentiment:', dfAPIScrubbed.iloc[index,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$HPQ amazing beat $spy $NVDA $INTC\n",
      "HPQ amazing beat spy\n"
     ]
    }
   ],
   "source": [
    "# compares the pre-scrubbed body with the post-scrubbed body (stopwords removed)\n",
    "print(dfAPI.iloc[334,2])\n",
    "print(dfAPIScrubbed.iloc[334,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create new model with scrubbed body data (stopwords removed). It can be run with new labels (sentiments) that were generated with nltk Vader or that are from the stocktwits sentiment scores with the \"None\" sentiment ratings re-evaluated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#550 converts the scrubbed_compound scores into a 1 significant figure integer from a float number; rounding up\n",
    "# this is only needed if you are going to uses the 'scrubbed_compound' value as the label.\n",
    "\n",
    "def int_conversion(dfs):\n",
    "    dfs['scrubbed_compound'] =  np.int64((dfs['scrubbed_compound'] + .05) * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     symbol            created_at  \\\n",
      "0      INTC  2021-02-23T18:37:55Z   \n",
      "1      INTC  2021-02-23T18:10:05Z   \n",
      "2      INTC  2021-02-23T17:41:45Z   \n",
      "3      INTC  2021-02-23T17:36:25Z   \n",
      "4      INTC  2021-02-23T16:55:34Z   \n",
      "...     ...                   ...   \n",
      "1275     MU  2021-02-01T20:10:14Z   \n",
      "1276     MU  2021-02-01T20:07:12Z   \n",
      "1277     MU  2021-02-01T19:40:37Z   \n",
      "1278     MU  2021-02-01T19:37:10Z   \n",
      "1279     MU  2021-02-01T19:25:26Z   \n",
      "\n",
      "                                                   body followers sentiment  \\\n",
      "0     .. Wow _Great 📈 gains 2490 15 returns A big th...         1      None   \n",
      "1                        chart looking like tech bubble         1   Bearish   \n",
      "2     Come peek Quick bet pull trigger couple grand ...        33   Bullish   \n",
      "3                      added If gets cheaper write puts       261   Bullish   \n",
      "4     Third Point Sees Enormous Shareholder Value Cr...        12   Bullish   \n",
      "...                                                 ...       ...       ...   \n",
      "1275  As reported recent earnings calls Semiconducto...       126   Bullish   \n",
      "1276  gap hold long managed needed support prior day...        43      None   \n",
      "1277  leading way 3.7 ARKK leading team 3.2 3 MRVL 2...       353   Bullish   \n",
      "1278     Hell yes actually work 9-5 job came meeting 80        55   Bullish   \n",
      "1279                                  OMFG 80 going ...        64      None   \n",
      "\n",
      "      raw_compound  compound_bin  sentiment_number  modified_rating modified?  \\\n",
      "0           0.7243           1.0               0.0              1.0       Yes   \n",
      "1           0.3612           1.0              -1.0             -1.0        No   \n",
      "2           0.7184           1.0               1.0              1.0        No   \n",
      "3           0.0000           0.0               1.0              1.0        No   \n",
      "4           0.5423           1.0               1.0              1.0        No   \n",
      "...            ...           ...               ...              ...       ...   \n",
      "1275        0.3129           1.0               1.0              1.0        No   \n",
      "1276       -0.5023          -1.0               0.0              1.0       Yes   \n",
      "1277       -0.4648          -1.0               1.0              1.0        No   \n",
      "1278       -0.4404          -1.0               1.0              1.0        No   \n",
      "1279        0.0000           0.0               0.0             -1.0       Yes   \n",
      "\n",
      "      scrubbed_compound  scrubbed_compound_bin  \n",
      "0                     8                      8  \n",
      "1                     4                      4  \n",
      "2                     7                      7  \n",
      "3                     0                      0  \n",
      "4                     5                      5  \n",
      "...                 ...                    ...  \n",
      "1275                  1                      1  \n",
      "1276                 -3                     -3  \n",
      "1277                  0                      0  \n",
      "1278                 -3                     -3  \n",
      "1279                  0                      0  \n",
      "\n",
      "[1280 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# 550 converts the 'scrubbed_compound' (column 10) data to either a 1, 0 or -1.  \n",
    "# if nltk sentiment number are >= .1; 0 if -.1 < x < .1 and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "# creates a new column called 'compound_bin' from the raw_compound scores\n",
    "\n",
    "def bin_sentiment(dfs):\n",
    "    dfs['scrubbed_compound_bin'] = dfs['scrubbed_compound'] # creates a new column 'scrubbed_compound_bin' (column 11)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(dfAPI):\n",
    "        if dfs.iloc[i,10] >= 0.1: # column 10 is 'scrubbed_compound'\n",
    "            dfs.iloc[i, 11] =  np.int(dfs.iloc[i, 10] + .9) # column 11 is 'scurbbed_compound_bin'\n",
    "        \n",
    "        if dfs.iloc[i,10] < .1 and dfs.iloc[i, 10] > -.1:\n",
    "            dfs.iloc[i, 11] = 0   \n",
    "        \n",
    "        if dfs.iloc[i,10] <= -.1:\n",
    "            dfs.iloc[i, 11] =  np.int(dfs.iloc[i, 10] - .9)\n",
    "        i += 1\n",
    "    \n",
    "    print(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol            created_at  \\\n",
      "0   INTC  2021-02-23T18:37:55Z   \n",
      "1   INTC  2021-02-23T18:10:05Z   \n",
      "2   INTC  2021-02-23T17:41:45Z   \n",
      "3   INTC  2021-02-23T17:36:25Z   \n",
      "4   INTC  2021-02-23T16:55:34Z   \n",
      "\n",
      "                                                body followers sentiment  \\\n",
      "0  .. Wow _Great 📈 gains 2490 15 returns A big th...         1      None   \n",
      "1                     chart looking like tech bubble         1   Bearish   \n",
      "2  Come peek Quick bet pull trigger couple grand ...        33   Bullish   \n",
      "3                   added If gets cheaper write puts       261   Bullish   \n",
      "4  Third Point Sees Enormous Shareholder Value Cr...        12   Bullish   \n",
      "\n",
      "   raw_compound  compound_bin  sentiment_number  modified_rating modified?  \\\n",
      "0        0.7243           1.0               0.0              1.0       Yes   \n",
      "1        0.3612           1.0              -1.0             -1.0        No   \n",
      "2        0.7184           1.0               1.0              1.0        No   \n",
      "3        0.0000           0.0               1.0              1.0        No   \n",
      "4        0.5423           1.0               1.0              1.0        No   \n",
      "\n",
      "   scrubbed_compound  scrubbed_compound_bin  \n",
      "0             0.8271                    1.0  \n",
      "1             0.3612                    1.0  \n",
      "2             0.7184                    1.0  \n",
      "3             0.0000                    0.0  \n",
      "4             0.5423                    1.0  \n"
     ]
    }
   ],
   "source": [
    "print(dfAPIScrubbed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol            created_at  \\\n",
      "0   INTC  2021-02-23T18:37:55Z   \n",
      "1   INTC  2021-02-23T18:10:05Z   \n",
      "2   INTC  2021-02-23T17:41:45Z   \n",
      "3   INTC  2021-02-23T17:36:25Z   \n",
      "4   INTC  2021-02-23T16:55:34Z   \n",
      "\n",
      "                                                body followers sentiment  \\\n",
      "0  .. Wow _Great 📈 gains 2490 15 returns A big th...         1      None   \n",
      "1                     chart looking like tech bubble         1   Bearish   \n",
      "2  Come peek Quick bet pull trigger couple grand ...        33   Bullish   \n",
      "3                   added If gets cheaper write puts       261   Bullish   \n",
      "4  Third Point Sees Enormous Shareholder Value Cr...        12   Bullish   \n",
      "\n",
      "   raw_compound  compound_bin  sentiment_number  modified_rating modified?  \\\n",
      "0        0.7243           1.0               0.0              1.0       Yes   \n",
      "1        0.3612           1.0              -1.0             -1.0        No   \n",
      "2        0.7184           1.0               1.0              1.0        No   \n",
      "3        0.0000           0.0               1.0              1.0        No   \n",
      "4        0.5423           1.0               1.0              1.0        No   \n",
      "\n",
      "   scrubbed_compound  scrubbed_compound_bin  \n",
      "0                  8                      8  \n",
      "1                  4                      4  \n",
      "2                  7                      7  \n",
      "3                  0                      0  \n",
      "4                  5                      5  \n"
     ]
    }
   ],
   "source": [
    "#610\n",
    "print(dfAPIScrubbed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#620\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of file 1 is: 1291\n",
      "The length of file 2 is: 1280\n",
      "The length of the combined dataframe is: 2571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'load = input(\\'Is this the first time processing the raw stocktwits data (enter \"n\"/\"N\" or \"y\"/\"Y\")? \\')\\nif load == \\'n\\' or load == \\'N\\' or load == \\'no\\' or load == \\'No\\':\\n    dfAPI = getData(filename)\\n    print(\\'Loaded filename:\\', filename)\\nelse:\\n    \\n    print(\\'ok\\')\\n    '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 650 Loads and combines two different dataframes in dfAPI; this is to combine two input datasets where the 'none'\n",
    "#values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "\n",
    "def combine_dfs()\n",
    "\n",
    "    filename1 = \"tech stockTwit 03112021 adjusted-Copy1.csv\"\n",
    "    filename2 = \"tech stockTwit 02232021 adjusted-Copy1.csv\"\n",
    "\n",
    "    dfAPI1 = getData(filename1)\n",
    "    dfAPI2 = getData(filename2)\n",
    "\n",
    "    dfAPI = dfAPI1.append(dfAPI2)\n",
    "\n",
    "    print('The length of file 1 is:', len(dfAPI1))\n",
    "    print('The length of file 2 is:', len(dfAPI2))\n",
    "\n",
    "    print('The length of the combined dataframe is:', len(dfAPI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The csv file was written. File name:  tech stockTwit 03112021 dup advert stopwords.csv\n"
     ]
    }
   ],
   "source": [
    "# Writes a csv file\n",
    "#input: df that is to be saved as a csv; output file name (eg 'tech stockTwit 03112021 dup advert stopwords.csv'\n",
    "\n",
    "def write_csv(df, filename_output):\n",
    "    df.to_csv(filename_output, index = False)\n",
    "    #dfAPI.to_csv(filename_output, index = False)\n",
    "    print('The csv file was written. File name: ', filename_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
