{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names(letter):\n",
    "    names = ['Anne', 'Amy', 'Bob', 'David', 'Carrie', 'Barbara', 'Zach']\n",
    "    b_names = [ name for name in names if name.startswith(letter) ]\n",
    "    print(b_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anne', 'Amy']\n"
     ]
    }
   ],
   "source": [
    "names('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 12]\n"
     ]
    }
   ],
   "source": [
    "number = [1,2,3,5,4,6,8,9,11,12]\n",
    "multiples_of_4 = list(filter(lambda a: (a%4 == 0) ,number))\n",
    "print(multiples_of_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiples():\n",
    "    number = [1,2,3,5,4,6,8,\n",
    "              9,11,12]\n",
    "    multiples_of_4 = list(filter(lambda a: (a%4 == 0) ,number))\n",
    "    print(multiples_of_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 12]\n"
     ]
    }
   ],
   "source": [
    "multiples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import time\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(name):\n",
    "    df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "    df1 = pd.read_csv(name, header = 0)\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dir_files(relevant_path):\n",
    "    # https://clay-atlas.com/us/blog/2019/10/27/python-english-tutorial-solved-unicodeescape-error-escape-syntaxerror/?doing_wp_cron=1618286551.1528689861297607421875\n",
    "    #need to change \\ to /\n",
    "\n",
    "    import os\n",
    "    \n",
    "    included_extensions = ['csv']\n",
    "    file_names = [fn for fn in os.listdir(relevant_path) # uses os.listdir to display only .csv files\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "    print('Path: ', relevant_path)\n",
    "\n",
    "    for f in file_names:\n",
    "        print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize1(df, stop_words):\n",
    "    print('\\nLemmatizing ...')\n",
    "    import nltk #not in original code\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    \n",
    "    ''' import nltk #not in original code\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')'''\n",
    "\n",
    "    nltk.download('wordnet') #not in original code\n",
    "    \n",
    "    #print('stop_words = \\n', stop_words)\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    \n",
    "    %time df['body'] = df['body'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "\n",
    "    \n",
    "    print('stop_words = \\n', stop_words)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270', '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74', '1600', '993', '392', '98', '00', '1601', 'amd', 'aapl']\n",
      "Wall time: 113 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>created_at</th>\n",
       "      <th>body</th>\n",
       "      <th>followers</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T21:01:03Z</td>\n",
       "      <td>$INTC Big Trade - $16 800.270 shares $60.74</td>\n",
       "      <td>862</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T21:01:03Z</td>\n",
       "      <td>Large Print $INTC Size: Price: 60.74 Time: Amo...</td>\n",
       "      <td>5502</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T21:00:02Z</td>\n",
       "      <td>Huge Print $INTC Size: Price: 60.74 Time: Amou...</td>\n",
       "      <td>5502</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T20:51:14Z</td>\n",
       "      <td>$AMD common follow ur sibs $INTC $MU</td>\n",
       "      <td>48</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T20:06:56Z</td>\n",
       "      <td>$ITT $INTC $ADBE $OPTT $GLBS . .</td>\n",
       "      <td>575</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol            created_at  \\\n",
       "0   INTC  2021-03-05T21:01:03Z   \n",
       "1   INTC  2021-03-05T21:01:03Z   \n",
       "2   INTC  2021-03-05T21:00:02Z   \n",
       "3   INTC  2021-03-05T20:51:14Z   \n",
       "4   INTC  2021-03-05T20:06:56Z   \n",
       "\n",
       "                                                body followers sentiment  \n",
       "0        $INTC Big Trade - $16 800.270 shares $60.74       862      None  \n",
       "1  Large Print $INTC Size: Price: 60.74 Time: Amo...      5502      None  \n",
       "2  Huge Print $INTC Size: Price: 60.74 Time: Amou...      5502      None  \n",
       "3               $AMD common follow ur sibs $INTC $MU        48   Bullish  \n",
       "4                   $ITT $INTC $ADBE $OPTT $GLBS . .       575   Bullish  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#adds new stopwords to list\n",
    "new_stop_words = ['intc', 'nvda', 'tsla', 'mu', 'msft', 'tsm', 'adbe', 'unh', '39', ' 270',\n",
    "                  '270000', '4033477', '244', '16', '399', '800', '270', '000', '60', '74',\n",
    "                 '1600', '993', '392', '98', '00', '1601', 'amd', 'aapl']\n",
    "for w in new_stop_words:\n",
    "    stop_words.append(w)\n",
    "\n",
    "print(stop_words)\n",
    "\n",
    "#removes the stopwords from the column body_Processed\n",
    "%time df['body'] = df['body'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.02 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>created_at</th>\n",
       "      <th>body</th>\n",
       "      <th>followers</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T21:01:03Z</td>\n",
       "      <td>$INTC Big Trade - $16 800.270 share $60.74</td>\n",
       "      <td>862</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T21:01:03Z</td>\n",
       "      <td>Large Print $INTC Size: Price: 60.74 Time: Amo...</td>\n",
       "      <td>5502</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T21:00:02Z</td>\n",
       "      <td>Huge Print $INTC Size: Price: 60.74 Time: Amou...</td>\n",
       "      <td>5502</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T20:51:14Z</td>\n",
       "      <td>$AMD common follow ur sib $INTC $MU</td>\n",
       "      <td>48</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INTC</td>\n",
       "      <td>2021-03-05T20:06:56Z</td>\n",
       "      <td>$ITT $INTC $ADBE $OPTT $GLBS . .</td>\n",
       "      <td>575</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol            created_at  \\\n",
       "0   INTC  2021-03-05T21:01:03Z   \n",
       "1   INTC  2021-03-05T21:01:03Z   \n",
       "2   INTC  2021-03-05T21:00:02Z   \n",
       "3   INTC  2021-03-05T20:51:14Z   \n",
       "4   INTC  2021-03-05T20:06:56Z   \n",
       "\n",
       "                                                body followers sentiment  \n",
       "0         $INTC Big Trade - $16 800.270 share $60.74       862      None  \n",
       "1  Large Print $INTC Size: Price: 60.74 Time: Amo...      5502      None  \n",
       "2  Huge Print $INTC Size: Price: 60.74 Time: Amou...      5502      None  \n",
       "3                $AMD common follow ur sib $INTC $MU        48   Bullish  \n",
       "4                   $ITT $INTC $ADBE $OPTT $GLBS . .       575   Bullish  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the text\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "import nltk #not in original code\n",
    "nltk.download('wordnet') #not in original code\n",
    "\n",
    "%time df['body'] = df['body'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df):\n",
    "    print('\\nLemmatizing ...')\n",
    "    \n",
    "    import nltk #not in original code\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    nltk.download('wordnet') #not in original code\n",
    "    \n",
    "    #print('stop_words = \\n', stop_words)\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    \n",
    "    %time df['body'] = df['body'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\n",
    "\n",
    "    print('stop_words = \\n', stop_words)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a list of the csv files to choose from: \n",
      "\n",
      "Path:  C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files\n",
      "2021-04-10 ARKG search stocktwits.csv\n",
      "2021-04-11 ROST search stocktwits.csv\n",
      "2021-04-11 TSLA search stocktwits.csv\n",
      "2021-04-11 V search stocktwits.csv\n",
      "2021-04-12 ACAD search stocktwits.csv\n",
      "2021-04-12 EBAY search stocktwits.csv\n",
      "2021-04-12 FB search stocktwits.csv\n",
      "2021-04-12 INTC search stocktwits.csv\n",
      "2021-04-12 OSTK search stocktwits.csv\n",
      "2021-04-12 V search stocktwits.csv\n",
      "2021-04-12 WKHS search stocktwits.csv\n",
      "ARKG search stocktwits-Copy1.csv\n",
      "preprocessed edited tech stockTwit 03112021.csv\n",
      "preprocessed r_a tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a r_e_o tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a r_stopwords r_emoj tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a r_stopwords r_emoj v_a tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a r_stopwords tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a tech stockTwit 03112021.csv\n",
      "preprocessed r_d tech stockTwit 03112021.csv\n",
      "preprocessed r_emoj tech stockTwit 03112021.csv\n",
      "preprocessed r_e_o tech stockTwit 03112021.csv\n",
      "preprocessed r_stopwords r_emoj tech stockTwit 03112021.csv\n",
      "preprocessed r_stopwords tech stockTwit 03112021.csv\n",
      "preprocessed tech stockTwit 03112021-Copy1.csv\n",
      "preprocessed tech stockTwit 03112021.csv\n",
      "scraped_tweets_names_dates.csv\n",
      "stopwords.csv\n",
      "tech stockTwit 03112021 adjusted Rev1.csv\n",
      "tech stockTwit 03112021-Copy1 ORIGINAL DO NOT USE.csv\n",
      "tech stockTwit 03112021.csv\n",
      "\n",
      "What file do you want to use? \n",
      "tech stockTwit 03112021.csv\n",
      "Imported the csv file.\n",
      "\n",
      "Lemmatizing ...\n",
      "Wall time: 193 ms\n",
      "stop_words = \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pstri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files'\n",
    "\n",
    "print('Here is a list of the csv files to choose from: \\n')\n",
    "list_dir_files(relevant_path) # gives all of the file options in the relevant path.\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "name = input('\\nWhat file do you want to use? \\n')\n",
    "df = getData(relevant_path + '/' + name) #returns df; reads csv file into df\n",
    "print('Imported the csv file.')\n",
    "\n",
    "import nltk #not in original code\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#df = lemmatize1(df, stop_words)\n",
    "df = lemmatize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 120 Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "# It counts every 20 edits and gives the user the option to quit. If the user chooses to quit\n",
    "# it breaks from the while look and writes the df to a csv file so all work is saved up to that point.\n",
    "# upon start up it ask if thie is the first time processing the raw data. If no it loads the csv file into\n",
    "# the dataframe and starts where the previous session left off. If \"modified?\" is \"Yes and \"sentiment\" is \"None\"\n",
    "# it skips the record. Therefore it will re-start at the first \"modified?\" is \"No\" and \"sentiment\" is \"None\"\n",
    "def edit(df):\n",
    "\n",
    "    import copy\n",
    "        \n",
    "    i = 0\n",
    "    counter = 0    # counter to see if user want to stop\n",
    "\n",
    "    while i < len(df):\n",
    "    #while i < 6:\n",
    "\n",
    "        if df.loc[i,('sentiment')] == 'None' and df.loc[i,('modified?')] == 'No': # Column 9 is 'modified?'\n",
    "            print('\\nindex number:', i, '\\n', df.loc[i, ('body')])\n",
    "            #print('This is the body of the tweet:\\n', df..log[i,('body')])\n",
    "            rating = int(input('Enter your rating (1, 0 or -1.):')) \n",
    "            df.loc[i,('modified_rating')] = copy.deepcopy(rating) # writes inputed number to the 'modified_rating'\n",
    "            df.loc[i,('modified?')] = 'Yes' # sets \"modified?\" equal to 'Yes' to identify which records have been modified; so that it can start at the next record at start up\n",
    "        \n",
    "            counter += 1\n",
    "        \n",
    "        elif df.loc[i,('sentiment')] == 'Bearish':\n",
    "\n",
    "            df.loc[i,('modified_rating')] = df.loc[i,('sentiment_number')] #copies the stocktwits 'sentiment_number' (7) to the 'modified_rating(8)\n",
    "        \n",
    "        elif df.loc[i,('sentiment')] == 'Bullish':\n",
    "        \n",
    "            df.loc[i,('modified_rating')] = df.loc[i,('sentiment_number')] #copies the stocktwits 'sentiment_number' (7) to the 'modified_rating(8)\n",
    "\n",
    "        if counter == 20: # represents 20 edits\n",
    "            quit = input('Do you want to quit? (Enter either a \"y\" or \"Y\") ')\n",
    "            if quit == 'y' or quit == 'Y':\n",
    "                print('You are exiting.')\n",
    "                break\n",
    "            else:\n",
    "                counter = 0 # resets the counter to 0 so there must be another 20 records reviewed and modified \n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    #df.to_csv(filename, index = False)\n",
    "    #print('The csv file was written. File name: ', filename)\n",
    "    \n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
