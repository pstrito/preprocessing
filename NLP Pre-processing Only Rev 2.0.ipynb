{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "import os\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "#10* initializes the dataframe \"df\" and imports the csv into df; \n",
    "#20* calls getdata to import the csv into the dataframe, 'dfAPI'\n",
    "#30 removes any duplicate records; duplicate records imply bot records\n",
    "#40 finds certain words in the strings ('body') and deletes the entire record.  \n",
    "#50* Vader sentiment analyzer\n",
    "#60* creates a new column called 'compound_bin' from the raw_compound scores\n",
    "#70* converts the 'raw_compound' data to either a 1, 0 or -1. 1 if nltk sentiment number are >= .1; 0 if -.1 < x < .1 \n",
    "#80* Converts sentiment ratings into numerical values and put the value into 'sentiment_number'.\n",
    "#90 Determines the percent correct and incorrect for the Vader sentiment values vs the stocktwits sentiment values\n",
    "#100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "#110 This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "#115 Provides statistics on sentiments; bullish, none or bearish.\n",
    "#120 Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "#130 Loads a csv file into the df dfAPI and print out the first 21 records\n",
    "#140 This will change the modified rating to the nltk rating only when they are opposite to see if it improves \n",
    "the accuracy number \n",
    "#440 sets up stopword removal; returns stopWords\n",
    "#470 creates a list of new stopwords and then adds them to the set provided by nltk\n",
    "Note: it is case sensitive; Input is the nltk stopword list (\"stopWords\")\n",
    "#490 Checks to see of the words were removed from the stopWords list.\n",
    "inputs: stopword list: output from def remove_from_stopwords(sw); the word to be removed\n",
    "#510 Removes stopwords from all the \"body\" text (tweets); to do this it must tokenize the string which means it must parse \n",
    "the string into individual words. It then compares the words with the words in the stopwords list and if there is not \n",
    "match it puts the word into the \"wordsFiltered\" list. It keeps appending to the list until all of the words are checked.\n",
    "It then joins the individual words back into a string.\n",
    "There is a difference between \"deep\" copy and \"shallow\" copy. \"Deep\" copy make a copy where the index and data are\n",
    "separate from the original. \"Shallow\" copy is like a pointer where the two df share a common index and data\n",
    "dfScrubbed = df #This is a shallow copy\n",
    "#550 converts the scrubbed_compound scores into a 1 significant figure integer from a float number; rounding up\n",
    "this is only needed if you are going to uses the 'scrubbed_compound' value as the label.\n",
    "#550 converts the 'scrubbed_compound' (column 10) data to either a 1, 0 or -1.  \n",
    "if nltk sentiment number are >= .1; 0 if -.1 < x < .1 and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "creates a new column called 'compound_bin' from the raw_compound scores\n",
    "#640 compares the first record (index = 0) raw data (\"body\" column) with scrubbed (stopwords removed) data\n",
    "inputs: df - original df; dfs - scrubbed df (stopwords removed)\n",
    "#650 Loads and combines two different dataframes in df; this is to combine two input datasets where the 'none'\n",
    "values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "#660 Writes a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "\n",
    "# 10 initializes the dataframe \"df\" and imports the csv into df; \n",
    "# the argument is the name/address of the file.\n",
    "# https://stackoverflow.com/questions/33440805/pandas-dataframe-read-csv-on-bad-data\n",
    "def getData(name):\n",
    "    df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "    df1 = pd.read_csv(name, header = 0)\n",
    "    return df1\n",
    "\n",
    "# 30 removes any duplicate records; duplicate records imply bot records\n",
    "def remove_duplicates(df):\n",
    "    df = df.drop_duplicates()\n",
    "    len(df)\n",
    "    return df\n",
    "\n",
    "# 40 finds certain words in the strings ('body') and deletes the entire record.\n",
    "#Note: When the record is deleted the df is re-indexed. The index for the while statement is not so the result is\n",
    "#that the record right after the deleted record is skipped. To remedy the problem the index (i) for the while statement \n",
    "#is decremented by one.\n",
    "#Also, the filtering terms are not case sensitive.\n",
    "def filter_records(df):\n",
    "    import fnmatch\n",
    "\n",
    "    data = []\n",
    "    counter = 0\n",
    "    advert = ['* sec *', '* daily News *', '*Huge Print*', '* Form *', '*SweepCast*', '*Large Print*', \n",
    "          '*Huge Print*', '*8-K*', '*SmartOptions*', '*Big Trade*', '*SEC Form*', '*Notice of Exempt*', \n",
    "          '*created_at*', '*stock news*', '*Trading Zones*', '*Entry:*', '*New Article*', '*ooc.bz*', \n",
    "          '*http*', 'Huge Trade', 'Trading is easy', 'www.', '#wallstreetbets', 'wallstreetbets',\n",
    "          'Huge Trade', '#unitedtraders', 'stockbeep.com', 'Big Trade'] # words or phrases whose records are to be removed; It is not case sensitive.\n",
    "\n",
    "    for a in advert:\n",
    "        i = 0\n",
    "        df = df.reset_index(drop = True) # resets the index before each iteration; removes the gaps; resets len(df)\n",
    "        while i < len(df):\n",
    "            dat = df.iloc[i,2] # 2 represents the 'body' column\n",
    "            data = [dat] # sets the string from the df into a list for the fnmatch.filter\n",
    "            #print('index = ', i)\n",
    "            filtered = fnmatch.filter(data, a) # compares the information in the 'body' column with the 'advert' list; it places the matched items in the 'filtered' variable.\n",
    "            #https://www.geeksforgeeks.org/fnmatch-unix-filename-pattern-matching-python/\n",
    "\n",
    "            if len(filtered) != 0: #if returns a True then record needs to be removed\n",
    "                counter += 1\n",
    "                #print('index:', i, df.iloc[i,2]) # prints the index number and record\n",
    "                #print(filtered, '\\n') # prints the entire record where there was a match (not wildcards were used)    \n",
    "                #print('before drop the next record is:', df.iloc[i+1, 2], 'i+1 = ', i + 1)\n",
    "            \n",
    "                df = df.drop(df.index[i]) # drops (deletes) the record\n",
    "            \n",
    "                #print('after the record is dropped:', df.iloc[i,2], 'i = ', i)\n",
    "                \n",
    "                #Note: When the record is dropped there is a change in the 'index' number. after the drop index number\n",
    "                #5 becomes index number 4. Since the counter increments one more time it skips the record right after\n",
    "                #the record that was just checked. That is why it takes multiple runs to remove all of the target\n",
    "                #records. To correct this decrement the index, i, by one\n",
    "                \n",
    "                i -= 1\n",
    "    \n",
    "            i += 1\n",
    "\n",
    "    df = df.reset_index(drop = True) # resets the index; removes the gaps   \n",
    "    len(df)\n",
    "    return df\n",
    "\n",
    "#50 Vader sentiment analyzer\n",
    "def vader_sentiment(df):\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "    f = lambda tweet: vader.polarity_scores(tweet)['compound']\n",
    "\n",
    "    df['raw_compound'] = df['body'].apply(f)\n",
    "\n",
    "    print('The number of clean records in the df are: ', len(df) , '\\n')\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 60 creates a new column called 'compound_bin' from the raw_compound scores. This creates a column that the raw \n",
    "#where the translated raw compound scores will be placed (either a -1, 0, 1.)\n",
    "def compound_binning(df):\n",
    "    df['compound_bin'] = df['raw_compound'] \n",
    "    \n",
    "    #del df['Unnamed: 0'] # deletes the column named 'Unnamed: 0'\n",
    "    \n",
    "    print(df.head())\n",
    "    \n",
    "    # 70 converts the 'raw_compound' data to either a 1, 0 or -1. 1 if nltk sentiment number are >= .1; 0 if -.1 < x < .1 \n",
    "    #and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,5] >= 0.1: # column 5 is 'raw_compound'\n",
    "            df.iloc[i, 6] =  np.int(df.iloc[i, 5] + .9) # column 6 is 'compound_bin'\n",
    "        \n",
    "        if df.iloc[i,5] < .1 and df.iloc[i, 5] > -.1:\n",
    "            df.iloc[i, 6] = 0   \n",
    "        \n",
    "        if df.iloc[i,5] <= -.1:\n",
    "            df.iloc[i, 6] =  np.int(df.iloc[i, 5] - .9)\n",
    "        i += 1\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 80 Converts sentiment ratings into numerical values and put the value into 'sentiment_number'.\n",
    "#Stocktwits sentiment rating (bullish or Bearish) is used as the standard;\n",
    "#Stocktwits sentiment rating of 'None' is not used as a standard because people could have simply elected to not enter it.\n",
    "#https://www.dataquest.io/blog/tutorial-add-column-pandas-dataframe-based-on-if-else-condition/\n",
    "def convert_sentiment_to_numerical(df):\n",
    "    import numpy as np\n",
    "\n",
    "    conditions = [(df['sentiment'] == 'Bullish'),\n",
    "                  (df['sentiment'] == 'None'),\n",
    "                  (df['sentiment'] == 'Bearish')]\n",
    "\n",
    "    values = [1.0, 0.0, -1.0]\n",
    "\n",
    "    df['sentiment_number'] = np.select(conditions, values)\n",
    "\n",
    "    df['modified_rating'] = 0 # adds a column \"modified_rating\" and sets it equal to 0\n",
    "    df['modified?'] = 'No' # adds a column \"modified?\" and sets it equal to 'No'\n",
    "\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 90 Determines the percent correct and incorrect for the Vader sentiment values vs the stocktwits sentiment values\n",
    "def vader_correct(df):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    total = len(df)\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if df.iloc[i, 6] == df.iloc[i, 7]: # column 6 is 'compound_bin' and column 7 is 'sentiment_number'\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1 \n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    print('The Vader percent correct to stocktwits raw data is:', int(100 * correct/total), '%')\n",
    "    print('The Vader percent incorrect to stocktwits raw data is:', int(100 * incorrect/total), '%')\n",
    "\n",
    "    #return df\n",
    "\n",
    "# 100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "def none_count_raw(df):\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "        \n",
    "# 110 This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "#the 'None' proportions more equal. It also prints the ratios of each sentiment response to the total number\n",
    "#of responses.\n",
    "def remove_every_other(df):\n",
    "    i = 0\n",
    "    counter = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'None':\n",
    "            if i % 2 == 0: #identifies every even index where the sentiment is \"None\"\n",
    "                df = df.drop(df.index[i]) #drops (deletes) the record\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    df = df.reset_index(drop = True) #resets the index to be continuous \n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('\\nThe total number of records is: ', len(df))\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'Bullish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bullish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bullish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "            \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'Bearish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bearish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bearish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '% \\n')\n",
    "            \n",
    "    return df    \n",
    "\n",
    "# 115 Provides statistics on sentiments; bullish, none or bearish.\n",
    "def stats(df):\n",
    "    \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'None':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The total number of records is: ', len(df))\n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'Bullish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bullish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bullish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "            \n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,4] == 'Bearish':\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "\n",
    "    print('The number of \"Bearish\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"Bearish\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "            \n",
    "# 120 Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "# It counts every 20 edits and gives the user the option to quit. If the user chooses to quit\n",
    "# it breaks from the while look and writes the df to a csv file so all work is saved up to that point.\n",
    "# upon start up it ask if thie is the first time processing the raw data. If no it loads the csv file into\n",
    "# the dataframe and starts where the previous session left off. If \"modified?\" is \"Yes and \"sentiment\" is \"None\"\n",
    "# it skips the record. Therefore it will re-start at the first \"modified?\" is \"No\" and \"sentiment\" is \"None\"\n",
    "def edit(df):\n",
    "\n",
    "    import copy\n",
    "        \n",
    "    i = 0\n",
    "    counter = 0    # counter to see if user want to stop\n",
    "\n",
    "    while i < len(df):\n",
    "    #while i < 6:\n",
    "\n",
    "        if df.iloc[i,4] == 'None' and df.iloc[i,9] == 'No':\n",
    "            print('\\nindex number:', i, '\\n', df.iloc[i, 2])\n",
    "            #print('This is the body of the tweet:\\n', df.iloc[i, 2])\n",
    "            rating = int(input('Enter your rating (1, 0 or -1.):')) \n",
    "            df.iloc[i,8] = copy.deepcopy(rating) # writes inputed number to the 'modified_rating'\n",
    "            df.iloc[i,9] = 'Yes' # sets \"modified?\" equal to 'Yes' to identify which records have been modified; so that it can start at the next record at start up\n",
    "        \n",
    "            counter += 1\n",
    "        \n",
    "        elif df.iloc[i,4] == 'Bearish':\n",
    "        #elif df.iloc[i,4] == 'Bearish' and df.iloc[i,9] == 'No': # the second condition is not needed\n",
    "\n",
    "            df.iloc[i,8] = df.iloc[i,7] #copies the stocktwits 'sentiment_number' to the 'modified_rating'\n",
    "        \n",
    "        elif df.iloc[i,4] == 'Bullish':\n",
    "        #elif df.iloc[i,4] == 'Bullish' and df.iloc[i,9] == 'No': # the second condition is not needed\n",
    "        \n",
    "            df.iloc[i,8] = df.iloc[i,7] #copies the stocktwits 'sentiment_number' to the 'modified_rating'\n",
    "\n",
    "        if counter == 20: # represents 20 edits\n",
    "            quit = input('Do you want to quit? (Enter either a \"y\" or \"Y\") ')\n",
    "            if quit == 'y' or quit == 'Y':\n",
    "                print('You are exiting.')\n",
    "                break\n",
    "            else:\n",
    "                counter = 0 # resets the counter to 0 so there must be another 20 records reviewed and modified \n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    #df.to_csv(filename, index = False)\n",
    "    #print('The csv file was written. File name: ', filename)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 140 This will change the modified rating to the nltk rating only when they are opposite to see if it improves \n",
    "#the accuracy number \n",
    "def change_opp_nltk(df):\n",
    "    \n",
    "    filename = 'tech stockTwit 02232021 opposite compound_bin vs modified_rating.csv'\n",
    "    \n",
    "    print('The name of the csv file that will be written to is: ', filename)\n",
    "    \n",
    "    correct_name = input('Is this the correct filename? (enter \"N\" or \"n\" for no)')\n",
    "          \n",
    "    if correct_name == 'N' or correct_name == 'n':\n",
    "          new_name = input('What is the correct name?')\n",
    "          filename = new_name\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    import copy\n",
    "\n",
    "    counter = 0    # counter to see if user want to stop\n",
    "\n",
    "    while i < len(df):\n",
    "\n",
    "        if df.iloc[i,6] == -1 and df.iloc[i, 8] == 1:\n",
    "            df.iloc[i,8] = copy.deepcopy(df.iloc[i, 6]) # change \"modified rating\" to \"compound_bin\"       \n",
    "        \n",
    "        elif df.iloc[i,6] == 1 and df.iloc[i, 8] == -1:\n",
    "            df.iloc[i,8] = copy.deepcopy(df.iloc[i, 6]) # change \"modified rating\" to \"compound_bin\"     \n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    df.to_csv(filename, index = False)\n",
    "    print('The csv file was written. File name: ', filename)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 180 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "def none_count(df):\n",
    "    i = 0\n",
    "    sentiment_number = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        if df.iloc[i,8] == 0.0:\n",
    "            sentiment_number += 1\n",
    "        i +=1\n",
    "        \n",
    "    '''\n",
    "    while i < len(test_labels):\n",
    "        if test_labels[i] == 0.0:\n",
    "            sentiment_number += 1\n",
    "        i += 1\n",
    "    '''\n",
    "    \n",
    "    print('The number of \"None\" stocktwits sentiment values is:', sentiment_number)\n",
    "    print('The percentage of \"None\" values is:', (int(sentiment_number/len(df) * 1000)/10), '%')\n",
    "    \n",
    "#440 sets up stopword removal; returns stopWords\n",
    "def set_up_nltk_stopword_removal():\n",
    "    #from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "\n",
    "    print(len(stopWords))\n",
    "    return stopWords\n",
    "\n",
    "#470 creates a list of new stopwords and then adds them to the set provided by nltk\n",
    "#Note: it is case sensitive\n",
    "#Input is the nltk stopword list (\"stopWords\")\n",
    "def add_new_stopwords(sw):\n",
    "    newStopWords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "    newStopWords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "    newStopWords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "    newStopWords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "    newStopWords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "    newStopWords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "    newStopWords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "    newStopWords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "    newStopWords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "    newStopWords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "    newStopWords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "    newStopWords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "    newStopWords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "    newStopWords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "    newStopWords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "    newStopWords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "    newStopWords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "    newStopWords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "    newStopWords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "    newStopWords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "    newStopWords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "    newStopWords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "    newStopWords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "    newStopWords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "    newStopWords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "    newStopWords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "    newStopWords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "    newStopWords += ['nevertheless', 'next', 'nine', 'nobody', 'none'] #removed 'no'\n",
    "    newStopWords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "    newStopWords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "    newStopWords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "    newStopWords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "    newStopWords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "    newStopWords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "    newStopWords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "    newStopWords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "    newStopWords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "    newStopWords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "    newStopWords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "    newStopWords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "    newStopWords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "    newStopWords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "    newStopWords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "    newStopWords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "    newStopWords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "    newStopWords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "    newStopWords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "    newStopWords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "    newStopWords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "    newStopWords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "    newStopWords += ['yours', 'yourself', 'yourselves'] #provided by Codecademy??\n",
    "\n",
    "    # additional stopwords:\n",
    "    newStopWords += ['[Screenshot]', '[screenshot]', 'Screenshot', '[Screenshot]Great', '[SCREENSHOT]', 'screenshot', \n",
    "                 'The', 'the', 'SMART', 'yah', 'got', 'nutty', 'moving', 'weeks', 'Got', 'So', 'today', 'Been', 'or']\n",
    "\n",
    "    newStopWords += ['I', 'it', 'It'] # pronouns\n",
    "\n",
    "    newStopWords += ['AMD', 'NVDA','NVDA', 'TSLA', 'GOOG', 'BA', 'FB', 'GOOGL', 'INTC', 'intel', 'Intel', 'CSCO', 'MU', \n",
    "                 'SMH', 'TSM','AAPL', 'TSLA', 'CSCO', 'POETF', 'PHOTONICS', 'DD', 'ARWR', 'T', 'INFI', 'AMC', 'ARK',\n",
    "                'GME', 'NIO', 'QS', 'INTC'] # Stock symbols or names\n",
    "\n",
    "    newStopWords += ['Readytogo123', 'Maddog68','Stocktwits', 'Big Trade'] # nouns\n",
    "\n",
    "    newStopWords += ['.', '?', '!', ';', ',', \"'\"] # punctuation\n",
    "\n",
    "    newStopWords += ['&', '#', '%', '$', '@'] # symbols\n",
    "\n",
    "    newStopWords += ['41.75', '530.05', '39', 'Two', 'two',] # numbers\n",
    "\n",
    "    #adds them to the stopWords list provided by nltk\n",
    "    for i in newStopWords:\n",
    "        sw.add(i) #stopWords is defined as a \"set\" in #450 when inputed as english words from nltk;\n",
    "        # sets cannot be ordered so it must be converted back to a list to be ordered or alphabetized. A set has no duplicate elements.\n",
    "\n",
    "    print('The length of the stopword list is: ', len(sw))\n",
    "    #print(stopWords)\n",
    "\n",
    "    #converts the set to a list\n",
    "    stopWords_list = list(sw)\n",
    "\n",
    "    #sorts the stopword list\n",
    "    stopWords_list.sort(key = lambda k : k.lower())\n",
    "    #print(stopWords_list)\n",
    "    \n",
    "    return stopWords_list\n",
    "\n",
    "#480 This removes words from the list of stopwords and writes list to csv file\n",
    "# https://stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python#:~:text=one%20more%20easy%20way%20to%20remove%20words%20from,%3D%20words%20-%20stopwords%20final_list%20%3D%20list%20%28final_list%29\n",
    "#new_words = list(filter(lambda w: w not in stop_words, initial_words))\n",
    "def remove_from_stopwords(sw, relevant_path):\n",
    "    WordsToBeRem = ['no']\n",
    "    stopWords = list(filter(lambda w: w not in WordsToBeRem, sw)) #It will retain anyword in sw that is not in WordsToBeRemoved\n",
    "\n",
    "    #converts the stopword list to a df so that it can then be written to a csv file\n",
    "    df_stopwords = pd.DataFrame(stopWords, columns = ['stopwords'])\n",
    "    name_of_csv_file = relevant_path + '/' + 'stopwords.csv'\n",
    "    df_stopwords.to_csv(name_of_csv_file, index = False) #writes stopwords to csv file\n",
    "\n",
    "    #print(stopWords)\n",
    "    \n",
    "    return stopWords\n",
    "\n",
    "#490 Checks to see of the words were removed from the stopWords list.\n",
    "#inputs: stopword list (sw) and the word to be removed from the so (WordToBeRem):\n",
    "def check_stopwords(sw, WordToBeRem):\n",
    "    \n",
    "    r = 0\n",
    "\n",
    "    for w in sw:\n",
    "        #print(w)\n",
    "        if w == WordToBeRem:\n",
    "            print('The word ', w , ' is still in the stopWords list!')\n",
    "            r += 1\n",
    "\n",
    "    if r == 0:\n",
    "        print('It did remove the words from the stopWords list!')\n",
    "    \n",
    "    #print(len(stopWords))\n",
    "\n",
    "#510 Removes stopwords from all the \"body\" text (tweets); to do this it must tokenize the string which means it must parse \n",
    "# the string into individual words. It then compares the words with the words in the stopwords list and if there is not \n",
    "# match it puts the word into the \"wordsFiltered\" list. It keeps appending to the list until all of the words are checked.\n",
    "# It then joins the individual words back into a string.\n",
    "#There is a difference between \"deep\" copy and \"shallow\" copy. \"Deep\" copy make a copy where the index and data are\n",
    "# separate from the original. \"Shallow\" copy is like a pointer where the two df share a common index and data\n",
    "#dfScrubbed = df #This is a shallow copy\n",
    "def rem_stopwords(df, stopWords):\n",
    "    \n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    \n",
    "    dfScrubbed = df.copy() #This is a deep copy. df.copy(deep = True); deep = True is default\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "    \n",
    "        data = df.iloc[i,2]\n",
    "        words = word_tokenize(data) # separates the string into a individual words.\n",
    "        wordsFiltered = []\n",
    "\n",
    "        for w in words:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w) # makes a new word list without the stopwords\n",
    "    \n",
    "        joinedWordsFiltered = ' '.join(wordsFiltered)\n",
    "    \n",
    "        dfScrubbed.iloc[i,2] = joinedWordsFiltered # replaces the recorded in dfScrubbed with the stopWords removed\n",
    "        # from the 'body'\n",
    "    \n",
    "        i += 1\n",
    "    \n",
    "    #print(wordsFiltered)\n",
    "    \n",
    "    #### method removes empty body rows and reindexes\n",
    "    dfScrubbed = remove_empty_body_rows(dfScrubbed)\n",
    "    \n",
    "    #### checks to see if there are any empty records left\n",
    "    print('Are there any empty body records?')\n",
    "    empty = np.where(pd.isnull(dfScrubbed['body'])) #checks to see if there are any empty records in the column 'body'\n",
    "    print(empty)\n",
    "    \n",
    "    print(dfScrubbed.head())\n",
    "    \n",
    "    return dfScrubbed\n",
    "\n",
    "#550 converts the scrubbed_compound scores into a 1 significant figure integer from a float number; rounding up\n",
    "# this is only needed if you are going to uses the 'scrubbed_compound' value as the label.\n",
    "def int_conversion(dfs):\n",
    "    dfs['scrubbed_compound'] =  np.int64((dfs['scrubbed_compound'] + .05) * 10)\n",
    "\n",
    "# 550 converts the 'scrubbed_compound' (column 10) data to either a 1, 0 or -1.  \n",
    "# if nltk sentiment number are >= .1; 0 if -.1 < x < .1 and -1 if <= -.1 and over-rights the value in compound_bin\n",
    "# creates a new column called 'compound_bin' from the raw_compound scores\n",
    "def bin_sentiment(dfs):\n",
    "    dfs['scrubbed_compound_bin'] = dfs['scrubbed_compound'] # creates a new column 'scrubbed_compound_bin' (column 11)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if dfs.iloc[i,10] >= 0.1: # column 10 is 'scrubbed_compound'\n",
    "            dfs.iloc[i, 11] =  np.int(dfs.iloc[i, 10] + .9) # column 11 is 'scurbbed_compound_bin'\n",
    "        \n",
    "        if dfs.iloc[i,10] < .1 and dfs.iloc[i, 10] > -.1:\n",
    "            dfs.iloc[i, 11] = 0   \n",
    "        \n",
    "        if dfs.iloc[i,10] <= -.1:\n",
    "            dfs.iloc[i, 11] =  np.int(dfs.iloc[i, 10] - .9)\n",
    "        i += 1\n",
    "    \n",
    "    print(dfs)\n",
    "\n",
    "#640 compares the first record (index = 0) raw data (\"body\" column) with scrubbed (stopwords removed) data\n",
    "#inputs: df - original df; dfs - scrubbed df (stopwords removed)\n",
    "def compare_scrubbed(df, dfs):\n",
    "    print(df.iloc[0,2])\n",
    "    print(dfs.iloc[0,2])\n",
    "\n",
    "# 650 Loads and combines two different dataframes in df; this is to combine two input datasets where the 'none'\n",
    "#values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "def combine_dfs(df1, df2):\n",
    "\n",
    "    df = df1.append(df2)\n",
    "\n",
    "    print('The length of file 1 is:', len(df1))\n",
    "    print('The length of file 2 is:', len(df2))\n",
    "\n",
    "    print('The length of the combined dataframe is:', len(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 660 Writes a csv file\n",
    "#input: df that is to be saved as a csv; output file name (eg 'tech stockTwit 03112021 dup advert stopwords.csv'\n",
    "def write_csv(df, filename_output, relevant_path):\n",
    "    \n",
    "    df.to_csv(relevant_path + '/' + filename_output, index = False, encoding = 'utf-8')\n",
    "    print('The csv file was written. File name: ', filename_output)\n",
    "    \n",
    "# displays a list of file with on a csv suffix       \n",
    "def list_dir_files(relevant_path):\n",
    "    # https://clay-atlas.com/us/blog/2019/10/27/python-english-tutorial-solved-unicodeescape-error-escape-syntaxerror/?doing_wp_cron=1618286551.1528689861297607421875\n",
    "    #need to change \\ to /\n",
    "\n",
    "    # uses os.listdir to display only .csv files\n",
    "    import os\n",
    "    \n",
    "    included_extensions = ['csv']\n",
    "    file_names = [fn for fn in os.listdir(relevant_path)\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "    print('Path: ', relevant_path)\n",
    "\n",
    "    for f in file_names:\n",
    "        print(f)\n",
    "\n",
    "# removes duplicate headers\n",
    "def remove_duplicate_headers(df):\n",
    "    column = 'symbol'\n",
    "    df.drop(df[df['symbol'] == column].index, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# removes specific rows and resets the index\n",
    "def remove_empty_body_rows(df):\n",
    "    df.dropna(subset=['body'], inplace=True) #drops empty body records\n",
    "    df = df.reset_index(drop = True) # resets the index\n",
    "    return df\n",
    "\n",
    "\n",
    "#### checks to see if there are any empty records left\n",
    "def empty_records_check(df):\n",
    "    print('Are there any empty body records?')\n",
    "    empty = np.where(pd.isnull(df['body'])) #checks to see if there are any empty records in the column 'body'\n",
    "    \n",
    "    if empty[0].size == 0:\n",
    "        print('There are no empty records! \\n', empty)\n",
    "    else:\n",
    "        print('There are empty records ...\\n', empty)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a list of the csv files to choose from: \n",
      "\n",
      "Path:  C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files\n",
      "2021-04-10 ARKG search stocktwits.csv\n",
      "2021-04-11 ROST search stocktwits.csv\n",
      "2021-04-11 TSLA search stocktwits.csv\n",
      "2021-04-11 V search stocktwits.csv\n",
      "2021-04-12 ACAD search stocktwits.csv\n",
      "2021-04-12 EBAY search stocktwits.csv\n",
      "2021-04-12 FB search stocktwits.csv\n",
      "2021-04-12 INTC search stocktwits.csv\n",
      "2021-04-12 OSTK search stocktwits.csv\n",
      "2021-04-12 V search stocktwits.csv\n",
      "2021-04-12 WKHS search stocktwits.csv\n",
      "ARKG search stocktwits-Copy1.csv\n",
      "preprocessed r_a tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a r_e_o tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a r_stopwords tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a tech stockTwit 03112021.csv\n",
      "preprocessed r_d tech stockTwit 03112021.csv\n",
      "preprocessed r_e_o tech stockTwit 03112021.csv\n",
      "preprocessed r_stopwords tech stockTwit 03112021.csv\n",
      "preprocessed tech stockTwit 03112021-Copy1.csv\n",
      "preprocessed tech stockTwit 03112021.csv\n",
      "scraped_tweets_names_dates.csv\n",
      "stopwords.csv\n",
      "tech stockTwit 03112021 adjusted Rev1.csv\n",
      "tech stockTwit 03112021-Copy1 ORIGINAL DO NOT USE.csv\n",
      "tech stockTwit 03112021.csv\n",
      "\n",
      "What file do you want to use? tech stockTwit 03112021.csv\n",
      "Imported the csv file.\n",
      "\n",
      "This is the first time this file has been preprocessed.\n",
      "\n",
      "Performing Vader sentiment analysis... \n",
      "\n",
      "The number of clean records in the df are:  2417 \n",
      "\n",
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T21:01:03Z   \n",
      "1   INTC  2021-03-05T21:01:03Z   \n",
      "2   INTC  2021-03-05T21:00:02Z   \n",
      "3   INTC  2021-03-05T20:51:14Z   \n",
      "4   INTC  2021-03-05T20:06:56Z   \n",
      "\n",
      "                                                body followers sentiment  \\\n",
      "0  $INTC Big Trade - $16 399 800.270 000 shares a...       862      None   \n",
      "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None   \n",
      "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None   \n",
      "3               $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "\n",
      "   raw_compound  \n",
      "0        0.2960  \n",
      "1        0.0000  \n",
      "2        0.3182  \n",
      "3        0.0000  \n",
      "4        0.0000  \n",
      "Produced Vader sentiment values.\n",
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T21:01:03Z   \n",
      "1   INTC  2021-03-05T21:01:03Z   \n",
      "2   INTC  2021-03-05T21:00:02Z   \n",
      "3   INTC  2021-03-05T20:51:14Z   \n",
      "4   INTC  2021-03-05T20:06:56Z   \n",
      "\n",
      "                                                body followers sentiment  \\\n",
      "0  $INTC Big Trade - $16 399 800.270 000 shares a...       862      None   \n",
      "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None   \n",
      "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None   \n",
      "3               $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "\n",
      "   raw_compound  compound_bin  \n",
      "0        0.2960        0.2960  \n",
      "1        0.0000        0.0000  \n",
      "2        0.3182        0.3182  \n",
      "3        0.0000        0.0000  \n",
      "4        0.0000        0.0000  \n",
      "     symbol            created_at  \\\n",
      "0      INTC  2021-03-05T21:01:03Z   \n",
      "1      INTC  2021-03-05T21:01:03Z   \n",
      "2      INTC  2021-03-05T21:00:02Z   \n",
      "3      INTC  2021-03-05T20:51:14Z   \n",
      "4      INTC  2021-03-05T20:06:56Z   \n",
      "...     ...                   ...   \n",
      "2495     MU  2021-02-24T12:44:23Z   \n",
      "2496     MU  2021-02-24T12:42:03Z   \n",
      "2497     MU  2021-02-24T12:38:21Z   \n",
      "2498     MU  2021-02-24T12:10:44Z   \n",
      "2499     MU  2021-02-24T12:10:09Z   \n",
      "\n",
      "                                                   body followers sentiment  \\\n",
      "0     $INTC Big Trade - $16 399 800.270 000 shares a...       862      None   \n",
      "1     Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None   \n",
      "2     Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None   \n",
      "3                  $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "4                   $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "...                                                 ...       ...       ...   \n",
      "2495    $MU 90 w ay to easy，let&#39;s see 100 this week         1      None   \n",
      "2496  $AMD $NVDA $INTC $MU $QCOM..BUY STHC reverse m...       191   Bullish   \n",
      "2497  $MU Premarket looking promising  bears can get...        55   Bullish   \n",
      "2498                                 $MU DXI up over 3%        55   Bullish   \n",
      "2499  $MU Looking very good again  buy with conviction.        55   Bullish   \n",
      "\n",
      "      raw_compound  compound_bin  \n",
      "0           0.2960           1.0  \n",
      "1           0.0000           0.0  \n",
      "2           0.3182           1.0  \n",
      "3           0.0000           0.0  \n",
      "4           0.0000           0.0  \n",
      "...            ...           ...  \n",
      "2495        0.0000           0.0  \n",
      "2496        0.7249           1.0  \n",
      "2497        0.4574           1.0  \n",
      "2498        0.0000           0.0  \n",
      "2499        0.4927           1.0  \n",
      "\n",
      "[2417 rows x 7 columns]\n",
      "Completed the Vader compound binning.\n",
      "     symbol            created_at  \\\n",
      "0      INTC  2021-03-05T21:01:03Z   \n",
      "1      INTC  2021-03-05T21:01:03Z   \n",
      "2      INTC  2021-03-05T21:00:02Z   \n",
      "3      INTC  2021-03-05T20:51:14Z   \n",
      "4      INTC  2021-03-05T20:06:56Z   \n",
      "...     ...                   ...   \n",
      "2495     MU  2021-02-24T12:44:23Z   \n",
      "2496     MU  2021-02-24T12:42:03Z   \n",
      "2497     MU  2021-02-24T12:38:21Z   \n",
      "2498     MU  2021-02-24T12:10:44Z   \n",
      "2499     MU  2021-02-24T12:10:09Z   \n",
      "\n",
      "                                                   body followers sentiment  \\\n",
      "0     $INTC Big Trade - $16 399 800.270 000 shares a...       862      None   \n",
      "1     Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None   \n",
      "2     Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None   \n",
      "3                  $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "4                   $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "...                                                 ...       ...       ...   \n",
      "2495    $MU 90 w ay to easy，let&#39;s see 100 this week         1      None   \n",
      "2496  $AMD $NVDA $INTC $MU $QCOM..BUY STHC reverse m...       191   Bullish   \n",
      "2497  $MU Premarket looking promising  bears can get...        55   Bullish   \n",
      "2498                                 $MU DXI up over 3%        55   Bullish   \n",
      "2499  $MU Looking very good again  buy with conviction.        55   Bullish   \n",
      "\n",
      "      raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "0           0.2960           1.0               0.0                0        No  \n",
      "1           0.0000           0.0               0.0                0        No  \n",
      "2           0.3182           1.0               0.0                0        No  \n",
      "3           0.0000           0.0               1.0                0        No  \n",
      "4           0.0000           0.0               1.0                0        No  \n",
      "...            ...           ...               ...              ...       ...  \n",
      "2495        0.0000           0.0               0.0                0        No  \n",
      "2496        0.7249           1.0               1.0                0        No  \n",
      "2497        0.4574           1.0               1.0                0        No  \n",
      "2498        0.0000           0.0               1.0                0        No  \n",
      "2499        0.4927           1.0               1.0                0        No  \n",
      "\n",
      "[2417 rows x 10 columns]\n",
      "Converted the Stocktwits sentiments to a numberical value (1,0,-1).\n",
      "\n",
      "All finished with the Vader sentiment analysis.\n",
      "\n",
      "Do you want to remove duplicates? [Press enter if no] \n",
      "Do you want to remove advertisements? [Press enter if no] \n",
      "Do you want to remove every other neutral sentiment record: [Press enter if no] \n",
      "Do you want to compare the Vader sentiment numbers with the Stocktwits sentiment ratings? [Press enter if no] \n",
      "Do you want to count the \"None\" sentiment values for the Stocktwits sentiments before any edits? [Press enter if no] \n",
      "Do you want to see the statistics on the Stocktwits sentiments? [Press enter if no] \n",
      "Do you want to edit the \"None\" records? [Press enter if no] y\n",
      "\n",
      "index number: 0 \n",
      " $INTC Big Trade - $16 399 800.270 000 shares at $60.74\n",
      "Enter your rating (1, 0 or -1.):0\n",
      "\n",
      "index number: 1 \n",
      " Large Print $INTC Size: 270000 Price: 60.74 Time: 1601 Amount: $16 399 800.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your rating (1, 0 or -1.):0\n",
      "\n",
      "index number: 2 \n",
      " Huge Print $INTC Size: 4033477 Price: 60.74 Time: 1600 Amount: $244 993 392.98\n",
      "Enter your rating (1, 0 or -1.):0\n",
      "\n",
      "index number: 6 \n",
      " @ButterFingerDROPs $INTC had its sell off back in October. Still probably the most undervalued semi stock out there.\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 7 \n",
      " $INTC  Trading is easy with Buy and Short signals in real time.\n",
      "Enter your rating (1, 0 or -1.):0\n",
      "\n",
      "index number: 8 \n",
      " $AMD At this rate  this will be left behind by $INTC as well  just like how $MU flew by us -:(\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 10 \n",
      " $INTC  didnt even flinch on  this market sell off. $AMD $NVDA $SPY\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 11 \n",
      " $AMD $INTC $NVDA repost..https://m.hexus.net/tech/news/industry/147502-tsmc-aims-double-5nm-production-capacity-year/\n",
      "Enter your rating (1, 0 or -1.):0\n",
      "\n",
      "index number: 12 \n",
      " #russell1000  Earnings Ratings #mega.stocks: $VZ  $INTC  $BAC  $UNH  $KO.https://www.finscreener.org/earnings/earnings-ratings/russell1000\n",
      "Enter your rating (1, 0 or -1.):0\n",
      "\n",
      "index number: 14 \n",
      " $INTC $intc i hope that they will be bankrupted very soon. This company is trash. Only reason they are up is that citadel is backing them up relentlessly.\n",
      "Enter your rating (1, 0 or -1.):-1\n",
      "\n",
      "index number: 15 \n",
      " $AMD I can&#39;t believe $MU and $INTC are having stellar few months while this keeps on bleeding profusely for the last 4 months -:(\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 17 \n",
      " $VZ and $INTC LEAPS I added on dip are saving my ass just a little bit. Just a little bit. Maybe in should knock on wood (just not Cathie Wood... yeesh).\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 18 \n",
      " LiquidTheta® Trade Alert (Delayed/Actionable)..Ticker: $INTC..Buy: Aug 20 2021 $60.00 Calls..Entry Price: $5.35- $5.60..TP1: $6.42.TP2: $7.22.TP3: $8.02..Stop Loss: $4.63 (Optional)..Highly Profitable Options Signals Delivered to Your Inbox.https://www.liquidtheta.com\n",
      "Enter your rating (1, 0 or -1.):0\n",
      "\n",
      "index number: 19 \n",
      " @TraderLeibniz @Stock__Twists @Jamz83 @Uncle_Covid @AllJackedUp @M_89 @denseanddumb @Oliwood @Jabba_Hutt @grifmaster $INTC just had a little pop right now. :-)\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 21 \n",
      " @FuutesRipping $INTC holding up very well in this market. just FYI and something for you to keep an eye on.\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 23 \n",
      " @TraderLeibniz @Uncle_Covid @Stock__Twists @Jamz83 @AllJackedUp @M_89 @denseanddumb @Oliwood @Jabba_Hutt @grifmaster $INTC is holding up well in this market.\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 24 \n",
      " $INTC &amp; $GOOG  $GOOGL are probably the safest places to be right now.  Its not like they can tank any further. Especially Intel (although Intel has surprised me before given how much the POS has dumped in the past).\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 25 \n",
      " $INTC this has to be a meg options pin. No other explanation\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 26 \n",
      " $INTC this is up...wow...\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "\n",
      "index number: 28 \n",
      " $INTC boomer stock says fuck the broader market\n",
      "Enter your rating (1, 0 or -1.):1\n",
      "Do you want to quit? (Enter either a \"y\" or \"Y\") y\n",
      "You are exiting.\n",
      "Do you want to see how many \"None\" records there are after the edits? [Press enter if no] \n",
      "Do you want to flip the Vader sentiment rating when it is the opposite of the Stocktwits sentiment rating? [Press enter if no] \n",
      "Do you want to see the number of \"None\" sentiments after the edit? [Press enter if no] \n",
      "Do you want to remove the Stopwords? [Press enter if no] \n",
      "Test empty records before writing the csv file\n",
      "Are there any empty body records?\n",
      "There are no empty records! \n",
      " (array([], dtype=int64),)\n",
      "    symbol            created_at  \\\n",
      "110   INTC  2021-03-03T13:38:58Z   \n",
      "111   INTC  2021-03-03T13:28:48Z   \n",
      "112   INTC  2021-03-03T13:06:30Z   \n",
      "113   INTC  2021-03-03T12:33:43Z   \n",
      "114   INTC  2021-03-03T12:26:41Z   \n",
      "115   INTC  2021-03-03T12:26:22Z   \n",
      "116   INTC  2021-03-03T11:59:19Z   \n",
      "117   INTC  2021-03-03T11:53:16Z   \n",
      "118   INTC  2021-03-03T11:41:57Z   \n",
      "119   INTC  2021-03-03T11:27:07Z   \n",
      "\n",
      "                                                  body followers sentiment  \\\n",
      "110  Intel To Pay $2.18B Penalty To VLSI Tech For P...      1819      None   \n",
      "111  $INTC Intel To Pay $2.18B Penalty To VLSI Tech...     10423      None   \n",
      "112       $INTC premarket signals  blood bath coming.?       263      None   \n",
      "113  $SPY $TGT $SNAP $INTC $UVXY A word on &quot;fi...      4272      None   \n",
      "114  $INTC most likely knew it would get sued for p...         2   Bullish   \n",
      "115  $INTC In the recent reporting quarter: 7.44% o...      1788      None   \n",
      "116  $INTC apple already have 5nm chips   while int...         3   Bearish   \n",
      "117  $INTC The 2 billion fine will be litigated rep...       223      None   \n",
      "118  $INTC $AMD my bad! Zen 5 also on track..so Zen...         0   Bearish   \n",
      "119                                         $AMD $INTC       228      None   \n",
      "\n",
      "     raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "110       -0.7579          -1.0               0.0              0.0        No  \n",
      "111       -0.7579          -1.0               0.0              0.0        No  \n",
      "112        0.0000           0.0               0.0              0.0        No  \n",
      "113        0.0000           0.0               0.0              0.0        No  \n",
      "114        0.4497           1.0               1.0              0.0        No  \n",
      "115       -0.2960          -1.0               0.0              0.0        No  \n",
      "116       -0.4215          -1.0              -1.0              0.0        No  \n",
      "117        0.2023           1.0               0.0              0.0        No  \n",
      "118       -0.5848          -1.0              -1.0              0.0        No  \n",
      "119        0.0000           0.0               0.0              0.0        No  \n",
      "Do you want to write a csv file? [Press enter if no] y\n",
      "The csv file was written. File name:  preprocessed edited tech stockTwit 03112021.csv\n",
      "The file was written with the filename of:  preprocessed edited tech stockTwit 03112021.csv \n",
      "\n",
      "The filename is: \n",
      " C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files/preprocessed edited tech stockTwit 03112021.csv\n",
      "csv file read into df to see if all of the empty records are removed.\n",
      "Are there any empty body records?\n",
      "There are no empty records! \n",
      " (array([], dtype=int64),)\n",
      "Are there any empty body records?\n",
      "There are no empty records! \n",
      " (array([], dtype=int64),)\n",
      "The csv file was written. File name:  preprocessed edited tech stockTwit 03112021.csv\n",
      "Do you want to combine two files? [Press enter if no] n\n",
      "\n",
      "All done ....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "yes_resp = ['yes', 'YES', 'y', 'Y', 'Yes']\n",
    "no_resp = ['no', 'NO', 'n', 'N', 'No']\n",
    "\n",
    "\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files'\n",
    "\n",
    "print('Here is a list of the csv files to choose from: \\n')\n",
    "list_dir_files(relevant_path)\n",
    "name = input('\\nWhat file do you want to use? ')\n",
    "df = getData(relevant_path + '/' + name) #returns df; reads csv file into df\n",
    "print('Imported the csv file.')\n",
    "\n",
    "df = remove_duplicate_headers(df)\n",
    "\n",
    "if 'raw_compound' not in df.columns: #checks to see if this file have been prepocessed before by seeing if the column 'raw_compond' exists\n",
    "    \n",
    "    print('\\nThis is the first time this file has been preprocessed.\\n')\n",
    "    print('Performing Vader sentiment analysis... \\n')\n",
    "    \n",
    "    df = vader_sentiment(df) #returns df; adds column with Vader sentiment values ('raw_compound') from the 'body' column.\n",
    "    print('Produced Vader sentiment values.')\n",
    "\n",
    "    df = compound_binning(df) #returns df; adds a column where the raw_compound scores are translated into 1, 0 or -1 'compound_bin'\n",
    "    print('Completed the Vader compound binning.')\n",
    "\n",
    "    df = convert_sentiment_to_numerical(df) #returns df\n",
    "    print('Converted the Stocktwits sentiments to a numberical value (1,0,-1).')\n",
    "    print('\\nAll finished with the Vader sentiment analysis.\\n')\n",
    "\n",
    "else:\n",
    "    print('\\nThis file has been preprocessed before. There is no need to run the VADER analysis.\\n')\n",
    "    \n",
    "#remove duplicates\n",
    "r_d = input('Do you want to remove duplicates? [Press enter if no] ')\n",
    "if r_d in yes_resp:\n",
    "    df = remove_duplicates(df) #return df; removes duplicates\n",
    "    remove_dupl = 'r_d '\n",
    "else:\n",
    "    remove_dupl = ''\n",
    "    \n",
    "#remove advertisements\n",
    "r_a = input('Do you want to remove advertisements? [Press enter if no] ')\n",
    "if r_a in yes_resp:\n",
    "    df = filter_records(df) #returns df; removes addvertisements\n",
    "    remove_advertisements = 'r_a '\n",
    "else:\n",
    "    remove_advertisements = ''\n",
    "\n",
    "# 110 OPTIONAL: This removes every other \"None\" record to reduce the total number of \"None\" rating. This is to make\n",
    "#the 'None' proportions more equal. It also prints the ratios of each sentiment response to the total number\n",
    "#of responses.\n",
    "r_e_o = input('Do you want to remove every other neutral sentiment record: [Press enter if no] ')\n",
    "if r_e_o in yes_resp:\n",
    "    df = remove_every_other(df) #returns df\n",
    "    rem_every_other = 'r_e_o '\n",
    "else:\n",
    "    rem_every_other = ''\n",
    "    \n",
    "# 90 OPTIONAL Compares the Vader sentiment numbers with the Stocktwits sentiment ratings.\n",
    "v_c = input('Do you want to compare the Vader sentiment numbers with the Stocktwits sentiment ratings? [Press enter if no] ')\n",
    "if v_c in yes_resp:\n",
    "    vader_correct(df) \n",
    "\n",
    "# 100 OPTIONAL: Counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "c_n_s = input('Do you want to count the \"None\" sentiment values for the Stocktwits sentiments before any edits? [Press enter if no] ')\n",
    "if c_n_s in yes_resp:\n",
    "    none_count_raw(df) \n",
    "\n",
    "# 115 OPTIONAL: Provides statistics on Stocktwits sentiments; bullish, none or bearish.\n",
    "s_o_s = input('Do you want to see the statistics on the Stocktwits sentiments? [Press enter if no] ')\n",
    "if s_o_s in yes_resp:\n",
    "    stats(df) \n",
    "\n",
    "# 120 OPTIONAL: Allows user to manually input value when stocktwits sentiment value is \"None\"\n",
    "# It counts every 20 edits and gives the user the option to quit. If the user chooses to quit\n",
    "# it breaks from the while look and writes the df to a csv file so all work is saved up to that point.\n",
    "# upon start up it ask if thie is the first time processing the raw data. If no it loads the csv file into\n",
    "# the dataframe and starts where the previous session left off. If \"modified?\" is \"Yes and \"sentiment\" is \"None\"\n",
    "# it skips the record. Therefore it will re-start at the first \"modified?\" is \"No\" and \"sentiment\" is \"None\"\n",
    "\n",
    "e_n = input('Do you want to edit the \"None\" records? [Press enter if no] ')\n",
    "if e_n in yes_resp:\n",
    "    df = edit(df) #returns df\n",
    "    ed = 'edited '\n",
    "else:\n",
    "    ed = ''\n",
    "\n",
    "# 180 OPTIONAL: counts how many \"None\" sentiment values are there for the stocktwits sentiment values after the edit\n",
    "n_r_a_e = input('Do you want to see how many \"None\" records there are after the edits? [Press enter if no] ')\n",
    "if n_r_a_e in yes_resp:\n",
    "    none_count(df) \n",
    "\n",
    "# 140 OPTIONAL: This will change the modified rating to the nltk rating only when they are opposite to see if it improves \n",
    "#the accuracy number \n",
    "# flip vader rating if opposite to stocktwits sentiment\n",
    "f_v_r = input('Do you want to flip the Vader sentiment rating when it is the opposite of the Stocktwits sentiment rating? [Press enter if no] ')\n",
    "if f_v_r in yes_resp:\n",
    "    df = change_opp_nltk(df) #returns df\n",
    "\n",
    "# 180 OPTIONAL: counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "n_c_a_e = input('Do you want to see the number of \"None\" sentiments after the edit? [Press enter if no] ')\n",
    "if n_c_a_e in yes_resp:\n",
    "    none_count(df) \n",
    "\n",
    "# stopword removal from tweets\n",
    "s_w_r = input('Do you want to remove the Stopwords? [Press enter if no] ')\n",
    "if s_w_r in yes_resp:\n",
    "    \n",
    "    swords = 'r_stopwords ' # this is for the filename for the csv file\n",
    "    \n",
    "    #440 imports the nltk stopword list that holds the stopwords that will be removed from the text ('body.') \n",
    "    sw = set_up_nltk_stopword_removal() \n",
    "\n",
    "    #470 creates a list of new stopwords and then adds them to the set provided by nltk\n",
    "    #Note  it is case sensitive\n",
    "    #Input is the nltk stopword list (\"stopWords\")\n",
    "    sw = add_new_stopwords(sw) \n",
    "\n",
    "    #480 This removes words from the list of stopwords and writes list to csv file\n",
    "    # https //stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python# ~ text=one%20more%20easy%20way%20to%20remove%20words%20from,%3D%20words%20-%20stopwords%20final_list%20%3D%20list%20%28final_list%29\n",
    "    #new_words = list(filter(lambda w  w not in stop_words, initial_words))\n",
    "    sw = remove_from_stopwords(sw, relevant_path) \n",
    "    #return stopWords\n",
    "\n",
    "    #490 Checks to see of the words were removed from the stopWords list.\n",
    "    #inputs  stopword list (sw) which is the output from remove_from_stopwords(sw); the word to be removed \"no\" \n",
    "    check_stopwords(sw, 'no') \n",
    "\n",
    "    #510 Removes stopwords from all the \"body\" text (tweets); to do this it must tokenize the string which means it must parse \n",
    "    # the string into individual words. It then compares the words with the words in the stopwords list and if there is not \n",
    "    # match it puts the word into the \"wordsFiltered\" list. It keeps appending to the list until all of the words are checked.\n",
    "    # It then joins the individual words back into a string.\n",
    "    #There is a difference between \"deep\" copy and \"shallow\" copy. \"Deep\" copy make a copy where the index and data are\n",
    "    # separate from the original. \"Shallow\" copy is like a pointer where the two df share a common index and data\n",
    "    #dfAPIScrubbed = dfAPI #This is a shallow copy\n",
    "    \n",
    "    dfs = rem_stopwords(df, sw) #removes the stopwords and empty body records and returns dfScrubbed\n",
    "\n",
    "    #550 converts the scrubbed_compound scores into a 1 significant figure integer from a float number; rounding up\n",
    "    # this is only needed if you are going to uses the 'scrubbed_compound' value as the label.\n",
    "    #int_conversion(dfScrubbed) #return df\n",
    "\n",
    "    # compares the first record (index = 0) raw data (\"body\" column) with scrubbed (stopwords removed) data\n",
    "    #inputs  df - original df; dfs - scrubbed df (stopwords removed)\n",
    "    c_o_w_s_r = input('Do you want to compare the original tweet with the stopwords removed tweet? ')\n",
    "    if c_o_w_s_r in yes_resp:\n",
    "        compare_scrubbed(df, dfs) \n",
    "        \n",
    "else:\n",
    "    dfs = df\n",
    "    swords = ''\n",
    "\n",
    "#### checks to see if there are any empty records\n",
    "print('Test empty records before writing the csv file')\n",
    "empty_records_check(dfs)\n",
    "\n",
    "dfs = remove_empty_body_rows(dfs)\n",
    "\n",
    "print(dfs.iloc[110:120,])\n",
    "\n",
    "# Writes a csv file; input  df that is to be saved as a csv; output file name is combination of types of editing\n",
    "w_csv = input('Do you want to write a csv file? [Press enter if no] ')\n",
    "if w_csv in yes_resp:\n",
    "    processed = 'preprocessed '\n",
    "    filename_output = processed + remove_dupl + remove_advertisements + rem_every_other + swords + ed + name\n",
    "    \n",
    "    if name == filename_output: #Checks to see if the file already exists\n",
    "        os.remove(filename_output) #If the file already exists it deletes the original file\n",
    "        print('The old file was deleted.\\n')\n",
    "    \n",
    "    write_csv(dfs, filename_output, relevant_path) #Writes the df to a new file\n",
    "    print('The file was written with the filename of: ', filename_output, '\\n')\n",
    "\n",
    "    # NOTE TO SELF - When there is a record that has spaces only, it is encoded as a 'NaN' or empty record\n",
    "    #when encoded as a utf-8 csv file. It will cause the postprocessing Vader app to crash. Importing the csv file\n",
    "    #and then removing the 'NaN' and then rewriting the csv file should take care of the problem.\n",
    "\n",
    "final_name =  relevant_path + '/' + filename_output\n",
    "print('The filename is: \\n', final_name)\n",
    "dftest = getData(final_name)\n",
    "print('csv file read into df to see if all of the empty records are removed.')\n",
    "empty_records_check(dftest)\n",
    "df_final = remove_empty_body_rows(dftest)\n",
    "empty_records_check(df_final)\n",
    "\n",
    "os.remove(final_name) #If the file already exists it deletes the original file\n",
    "write_csv(df_final, filename_output, relevant_path) #Writes the df to a new file\n",
    "\n",
    "\n",
    "\n",
    "# combines two dfs\n",
    "c_t_dfs = input('Do you want to combine two files? [Press enter if no] ')\n",
    "if c_t_dfs in yes_resp:\n",
    "    \n",
    "    print('Here is a list of the csv files to choose from: \\n')\n",
    "    list_dir_files(relevant_path)\n",
    "    first_name = input('\\nWhat is the first file you want to combine? ')\n",
    "    df = getData(relevant_path + '/' + first_name) #returns df; reads csv file into df\n",
    "    print('Imported the csv file.')\n",
    "    \n",
    "    second_name = input('What is the second file you want to add? ')\n",
    "    df2 = getData(relevant_path + '/' + second_name)\n",
    "\n",
    "    # 650 Loads and combines two different dataframes in dfAPI; this is to combine two input datasets where the 'none'\n",
    "    #values have been modified; this is to see if increased records will increase the accuracy of the model.\n",
    "    df = combine_dfs(df1, df2)\n",
    "    \n",
    "    w_csv = input('Do you want to write a csv file? [Press enter if no] ')\n",
    "    if w_csv in yes_resp:\n",
    "        duo_name = input('What do you want to name the file? [Do not include a suffix.]')\n",
    "        duo_name = duo_name + '.csv'\n",
    "        write_csv(df, duo_name, relevant_path) #Writes the df to a new file\n",
    "        print('The file was written with the filename of: ', duo_name, '\\n')\n",
    "\n",
    "print('\\nAll done ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T21:01:03Z   \n",
      "1   INTC  2021-03-05T21:01:03Z   \n",
      "2   INTC  2021-03-05T21:00:02Z   \n",
      "3   INTC  2021-03-05T20:51:14Z   \n",
      "4   INTC  2021-03-05T20:06:56Z   \n",
      "\n",
      "                                                body followers sentiment  \\\n",
      "0  $INTC Big Trade - $16 399 800.270 000 shares a...       862      None   \n",
      "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None   \n",
      "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None   \n",
      "3               $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "\n",
      "   raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "0        0.2960           1.0               0.0                0        No  \n",
      "1        0.0000           0.0               0.0                0        No  \n",
      "2        0.3182           1.0               0.0                0        No  \n",
      "3        0.0000           0.0               1.0                0        No  \n",
      "4        0.0000           0.0               1.0                0        No  \n"
     ]
    }
   ],
   "source": [
    "dftest = getData('preprocessed tech stockTwit 03112021.csv')\n",
    "print(dftest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     symbol            created_at  \\\n",
      "0      INTC  2021-03-05T20:51:14Z   \n",
      "1      INTC  2021-03-05T20:06:56Z   \n",
      "2      INTC  2021-03-05T19:57:20Z   \n",
      "3      INTC  2021-03-05T19:52:43Z   \n",
      "4      INTC  2021-03-05T19:36:13Z   \n",
      "...     ...                   ...   \n",
      "1286     MU  2021-02-24T13:22:35Z   \n",
      "1287     MU  2021-02-24T12:48:31Z   \n",
      "1288     MU  2021-02-24T12:38:21Z   \n",
      "1289     MU  2021-02-24T12:10:44Z   \n",
      "1290     MU  2021-02-24T12:10:09Z   \n",
      "\n",
      "                                                   body followers sentiment  \\\n",
      "0                  $AMD common follow ur sibs $INTC $MU        48   Bullish   \n",
      "1                   $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish   \n",
      "2     $INTC Should be thankful we are in this bull m...        21   Bullish   \n",
      "3     @ButterFingerDROPs $INTC had its sell off back...        77      None   \n",
      "4     $AMD At this rate  this will be left behind by...        11      None   \n",
      "...                                                 ...       ...       ...   \n",
      "1286  $MU bought on 89$.My only regret i miss averag...         2   Bullish   \n",
      "1287  $MU today is going to be another back breaker🚀...        37   Bullish   \n",
      "1288  $MU Premarket looking promising  bears can get...        55   Bullish   \n",
      "1289                                 $MU DXI up over 3%        55   Bullish   \n",
      "1290  $MU Looking very good again  buy with conviction.        55   Bullish   \n",
      "\n",
      "      raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "0           0.0000           0.0               1.0                0        No  \n",
      "1           0.0000           0.0               1.0                0        No  \n",
      "2           0.6996           1.0               1.0                0        No  \n",
      "3           0.0000           0.0               0.0                0        No  \n",
      "4           0.5574           1.0               0.0                0        No  \n",
      "...            ...           ...               ...              ...       ...  \n",
      "1286       -0.5267          -1.0               1.0                0        No  \n",
      "1287        0.0000           0.0               1.0                0        No  \n",
      "1288        0.4574           1.0               1.0                0        No  \n",
      "1289        0.0000           0.0               1.0                0        No  \n",
      "1290        0.4927           1.0               1.0                0        No  \n",
      "\n",
      "[1291 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#df = df.reset_index(drop = True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of \"None\" stocktwits sentiment values is: 481\n",
      "The percentage of \"None\" values is: 37.2 %\n"
     ]
    }
   ],
   "source": [
    "# 100 counts how many \"None\" sentiment values are there for the stocktwits sentiment value\n",
    "none_count_raw(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you want to test? Y\n",
      "yes I do\n"
     ]
    }
   ],
   "source": [
    "yes_resp = ['yes', 'YES', 'y', 'Y', 'Yes']\n",
    "no_resp = ['no', 'NO', 'n', 'N', 'No']\n",
    "\n",
    "test = input('do you want to test? ')\n",
    "if test in yes_resp:\n",
    "    print('yes I do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b d output.csv\n"
     ]
    }
   ],
   "source": [
    "name1 = 'output.csv'\n",
    "remove_dupl = 'a '\n",
    "remove_advertisements = 'b '\n",
    "remove_every_other = ''\n",
    "ed = 'd '\n",
    "\n",
    "filename_output = remove_dupl + remove_advertisements + remove_every_other + ed + name1\n",
    "\n",
    "print(filename_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in - no\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "#how to determine if column exists\n",
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame([[10, 20, 30, 40], [7, 14, 21, 28], [55, 15, 8, 12]],\n",
    "                  columns=['Apple', 'Orange', 'Banana', 'Pear'],\n",
    "                  index=['Basket1', 'Basket2', 'Basket3'])\n",
    " \n",
    "if 'apple' not in df.columns:\n",
    "    print(\"in - no\")\n",
    "else:\n",
    "    print(\"notin - yes\")\n",
    " \n",
    " \n",
    "if set(['Apple','Orange']).issubset(df.columns):\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a list of the csv files to choose from: \n",
      "\n",
      "Path:  C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Preprocessing\n",
      "preprocessed tech stockTwit 03112021.csv\n",
      "stopwords.csv\n",
      "tech stockTwit 03112021-Copy1 ORIGINAL DO NOT USE.csv\n",
      "tech stockTwit 03112021.csv\n",
      "\n",
      "What file do you want to use? tech stockTwit 03112021.csv\n",
      "Imported the csv file.\n",
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T21:01:03Z   \n",
      "1   INTC  2021-03-05T21:01:03Z   \n",
      "2   INTC  2021-03-05T21:00:02Z   \n",
      "3   INTC  2021-03-05T20:51:14Z   \n",
      "4   INTC  2021-03-05T20:06:56Z   \n",
      "\n",
      "                                                body followers sentiment  \n",
      "0  $INTC Big Trade - $16 399 800.270 000 shares a...       862      None  \n",
      "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None  \n",
      "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None  \n",
      "3               $AMD common follow ur sibs $INTC $MU        48   Bullish  \n",
      "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish  \n",
      "The index is:  30\n",
      "The index is:  61\n",
      "The index is:  92\n",
      "The index is:  123\n",
      "The index is:  154\n",
      "The index is:  185\n",
      "The index is:  216\n",
      "The index is:  247\n",
      "The index is:  278\n",
      "The index is:  309\n",
      "The index is:  340\n",
      "The index is:  371\n",
      "The index is:  402\n",
      "The index is:  433\n",
      "The index is:  464\n",
      "The index is:  495\n",
      "The index is:  526\n",
      "The index is:  557\n",
      "The index is:  588\n",
      "The index is:  619\n",
      "The index is:  650\n",
      "The index is:  681\n",
      "The index is:  712\n",
      "The index is:  743\n",
      "The index is:  774\n",
      "The index is:  805\n",
      "The index is:  836\n",
      "The index is:  867\n",
      "The index is:  898\n",
      "The index is:  929\n",
      "The index is:  960\n",
      "The index is:  991\n",
      "The index is:  1022\n",
      "The index is:  1053\n",
      "The index is:  1084\n",
      "The index is:  1115\n",
      "The index is:  1146\n",
      "The index is:  1177\n",
      "The index is:  1208\n",
      "The index is:  1239\n",
      "The index is:  1270\n",
      "The index is:  1301\n",
      "The index is:  1332\n",
      "The index is:  1363\n",
      "The index is:  1394\n",
      "The index is:  1425\n",
      "The index is:  1456\n",
      "The index is:  1487\n",
      "The index is:  1518\n",
      "The index is:  1548\n",
      "The index is:  1574\n",
      "The index is:  1603\n",
      "The index is:  1625\n",
      "The index is:  1645\n",
      "The index is:  1670\n",
      "The index is:  1686\n",
      "The index is:  1703\n",
      "The index is:  1723\n",
      "The index is:  1742\n",
      "The index is:  1773\n",
      "The index is:  1804\n",
      "The index is:  1835\n",
      "The index is:  1866\n",
      "The index is:  1897\n",
      "The index is:  1924\n",
      "The index is:  1951\n",
      "The index is:  1981\n",
      "The index is:  2012\n",
      "The index is:  2043\n",
      "The index is:  2066\n",
      "The index is:  2097\n",
      "The index is:  2128\n",
      "The index is:  2159\n",
      "The index is:  2190\n",
      "The index is:  2221\n",
      "The index is:  2252\n",
      "The index is:  2283\n",
      "The index is:  2314\n",
      "The index is:  2345\n",
      "The index is:  2376\n",
      "The index is:  2407\n",
      "The index is:  2438\n",
      "The index is:  2469\n",
      "starting to remove headers\n",
      "done removing headers\n",
      "They are all gone!\n",
      "  symbol            created_at  \\\n",
      "0   INTC  2021-03-05T21:01:03Z   \n",
      "1   INTC  2021-03-05T21:01:03Z   \n",
      "2   INTC  2021-03-05T21:00:02Z   \n",
      "3   INTC  2021-03-05T20:51:14Z   \n",
      "4   INTC  2021-03-05T20:06:56Z   \n",
      "\n",
      "                                                body followers sentiment  \n",
      "0  $INTC Big Trade - $16 399 800.270 000 shares a...       862      None  \n",
      "1  Large Print $INTC Size: 270000 Price: 60.74 Ti...      5502      None  \n",
      "2  Huge Print $INTC Size: 4033477 Price: 60.74 Ti...      5502      None  \n",
      "3               $AMD common follow ur sibs $INTC $MU        48   Bullish  \n",
      "4                $ITT $INTC $ADBE $OPTT $GLBS  .  .        575   Bullish  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_duplicate_headers(df):\n",
    "    column = 'symbol'\n",
    "    df.drop(df[df['symbol'] == column].index, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('Here is a list of the csv files to choose from: \\n')\n",
    "list_dir_files()\n",
    "name = input('\\nWhat file do you want to use? ')\n",
    "df = getData(name) #returns df; reads csv file into df\n",
    "print('Imported the csv file.')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "i = 0\n",
    "while i < len(df):\n",
    "    if df.iloc[i , 0] == \"symbol\":\n",
    "        print('The index is: ', i)\n",
    "    i += 1\n",
    "\n",
    "print('starting to remove headers')\n",
    "df = remove_duplicate_headers(df)\n",
    "print('done removing headers')\n",
    "\n",
    "\n",
    "i = 0\n",
    "while i < len(df):\n",
    "    if df.iloc[i , 0] == \"symbol\":\n",
    "        print('The index is: ', i)\n",
    "    i += 1\n",
    "else:\n",
    "    print('They are all gone!')\n",
    "    \n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed r_a tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a r_e_o tech stockTwit 03112021.csv\n",
      "preprocessed r_d r_a tech stockTwit 03112021.csv\n",
      "preprocessed r_d tech stockTwit 03112021.csv\n",
      "preprocessed r_e_o tech stockTwit 03112021.csv\n",
      "preprocessed r_stopwords tech stockTwit 03112021.csv\n",
      "preprocessed tech stockTwit 03112021-Copy1.csv\n",
      "tech stockTwit 03112021 adjusted Rev1.csv\n",
      "What file do you want: preprocessed r_stopwords tech stockTwit 03112021.csv\n",
      "    symbol            created_at  \\\n",
      "0     INTC  2021-03-05T21:01:03Z   \n",
      "1     INTC  2021-03-05T21:01:03Z   \n",
      "2     INTC  2021-03-05T21:00:02Z   \n",
      "3     INTC  2021-03-05T20:51:14Z   \n",
      "4     INTC  2021-03-05T20:06:56Z   \n",
      "..     ...                   ...   \n",
      "115   INTC  2021-03-03T12:26:22Z   \n",
      "116   INTC  2021-03-03T11:59:19Z   \n",
      "117   INTC  2021-03-03T11:53:16Z   \n",
      "118   INTC  2021-03-03T11:41:57Z   \n",
      "119   INTC  2021-03-03T11:27:07Z   \n",
      "\n",
      "                                                  body  followers sentiment  \\\n",
      "0          Big Trade - 16 399 800.270 000 shares 60.74        862      None   \n",
      "1    Large Print Size : 270000 Price : 60.74 Time :...       5502      None   \n",
      "2    Huge Print Size : 4033477 Price : 60.74 Time :...       5502      None   \n",
      "3                                common follow ur sibs         48   Bullish   \n",
      "4                                   ITT ADBE OPTT GLBS        575   Bullish   \n",
      "..                                                 ...        ...       ...   \n",
      "115  In recent reporting quarter : 7.44 institution...       1788      None   \n",
      "116  apple 5nm chips struggling launching 10nm micr...          3   Bearish   \n",
      "117  2 billion fine litigated repeatedly end paying...        223      None   \n",
      "118  bad Zen 5 track .. Zen 4 completed.Zen 5 compl...          0   Bearish   \n",
      "119                                                NaN        228      None   \n",
      "\n",
      "     raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "0          0.2960           1.0               0.0                0        No  \n",
      "1          0.0000           0.0               0.0                0        No  \n",
      "2          0.3182           1.0               0.0                0        No  \n",
      "3          0.0000           0.0               1.0                0        No  \n",
      "4          0.0000           0.0               1.0                0        No  \n",
      "..            ...           ...               ...              ...       ...  \n",
      "115       -0.2960          -1.0               0.0                0        No  \n",
      "116       -0.4215          -1.0              -1.0                0        No  \n",
      "117        0.2023           1.0               0.0                0        No  \n",
      "118       -0.5848          -1.0              -1.0                0        No  \n",
      "119        0.0000           0.0               0.0                0        No  \n",
      "\n",
      "[120 rows x 10 columns]\n",
      "before:\n",
      "empty\n",
      "after:\n",
      "    symbol            created_at  \\\n",
      "0     INTC  2021-03-05T21:01:03Z   \n",
      "1     INTC  2021-03-05T21:01:03Z   \n",
      "2     INTC  2021-03-05T21:00:02Z   \n",
      "3     INTC  2021-03-05T20:51:14Z   \n",
      "4     INTC  2021-03-05T20:06:56Z   \n",
      "..     ...                   ...   \n",
      "115   INTC  2021-03-03T12:26:22Z   \n",
      "116   INTC  2021-03-03T11:59:19Z   \n",
      "117   INTC  2021-03-03T11:53:16Z   \n",
      "118   INTC  2021-03-03T11:41:57Z   \n",
      "119   INTC  2021-03-03T11:27:07Z   \n",
      "\n",
      "                                                  body  followers sentiment  \\\n",
      "0          Big Trade - 16 399 800.270 000 shares 60.74        862      None   \n",
      "1    Large Print Size : 270000 Price : 60.74 Time :...       5502      None   \n",
      "2    Huge Print Size : 4033477 Price : 60.74 Time :...       5502      None   \n",
      "3                                common follow ur sibs         48   Bullish   \n",
      "4                                   ITT ADBE OPTT GLBS        575   Bullish   \n",
      "..                                                 ...        ...       ...   \n",
      "115  In recent reporting quarter : 7.44 institution...       1788      None   \n",
      "116  apple 5nm chips struggling launching 10nm micr...          3   Bearish   \n",
      "117  2 billion fine litigated repeatedly end paying...        223      None   \n",
      "118  bad Zen 5 track .. Zen 4 completed.Zen 5 compl...          0   Bearish   \n",
      "119                                                           228      None   \n",
      "\n",
      "     raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "0          0.2960           1.0               0.0                0        No  \n",
      "1          0.0000           0.0               0.0                0        No  \n",
      "2          0.3182           1.0               0.0                0        No  \n",
      "3          0.0000           0.0               1.0                0        No  \n",
      "4          0.0000           0.0               1.0                0        No  \n",
      "..            ...           ...               ...              ...       ...  \n",
      "115       -0.2960          -1.0               0.0                0        No  \n",
      "116       -0.4215          -1.0              -1.0                0        No  \n",
      "117        0.2023           1.0               0.0                0        No  \n",
      "118       -0.5848          -1.0              -1.0                0        No  \n",
      "119        0.0000           0.0               0.0                0        No  \n",
      "\n",
      "[120 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def remove_duplicate_headers(df):\n",
    "    column = 'symbol'\n",
    "    df.drop(df[df['symbol'] == column].index, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Post Processing'\n",
    "included_extensions = ['csv']\n",
    "file_names = [fn for fn in os.listdir(relevant_path)\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "for f in file_names:\n",
    "    print(f)\n",
    "    \n",
    "name = input('What file do you want: ')\n",
    "df = getData(relevant_path + '/' + name)\n",
    "\n",
    "print(df.head(120))\n",
    "\n",
    "print('before:')\n",
    "empty = np.where(pd.isnull(df['body']))\n",
    "print('empty')\n",
    "\n",
    "df = remove_duplicate_headers(df)\n",
    "\n",
    "df = df.fillna(value ={'body':' '}) #replaces any empty 'body' records with a space\n",
    "\n",
    "print('after:')\n",
    "np.where(pd.isnull(df['body']))\n",
    "\n",
    "print(df.head(120))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files/preprocessed r_stopwords tech stockTwit 03112021.csv\n",
      "csv file read into df to see if all of the empty records are removed.\n",
      "There are no empty records: \n",
      " (array([], dtype=int64),)\n",
      "    symbol            created_at  \\\n",
      "110   INTC  2021-03-03T13:38:58Z   \n",
      "111   INTC  2021-03-03T13:28:48Z   \n",
      "112   INTC  2021-03-03T13:06:30Z   \n",
      "113   INTC  2021-03-03T12:33:43Z   \n",
      "114   INTC  2021-03-03T12:26:41Z   \n",
      "115   INTC  2021-03-03T12:26:22Z   \n",
      "116   INTC  2021-03-03T11:59:19Z   \n",
      "117   INTC  2021-03-03T11:53:16Z   \n",
      "118   INTC  2021-03-03T11:41:57Z   \n",
      "119   INTC  2021-03-02T21:46:58Z   \n",
      "120   INTC  2021-03-02T21:43:00Z   \n",
      "121   INTC  2021-03-02T21:35:49Z   \n",
      "122   INTC  2021-03-02T21:32:28Z   \n",
      "123   INTC  2021-03-02T21:21:17Z   \n",
      "124   INTC  2021-03-02T21:07:09Z   \n",
      "\n",
      "                                                  body  followers sentiment  \\\n",
      "110  To Pay 2.18B Penalty To VLSI Tech For Patent I...       1819      None   \n",
      "111  To Pay 2.18B Penalty To VLSI Tech For Patent I...      10423      None   \n",
      "112                premarket signals blood bath coming        263      None   \n",
      "113  SPY TGT SNAP UVXY A word quot fighting trends ...       4272      None   \n",
      "114  likely knew sued patent infringement knew bene...          2   Bullish   \n",
      "115  In recent reporting quarter : 7.44 institution...       1788      None   \n",
      "116  apple 5nm chips struggling launching 10nm micr...          3   Bearish   \n",
      "117  2 billion fine litigated repeatedly end paying...        223      None   \n",
      "118  bad Zen 5 track .. Zen 4 completed.Zen 5 compl...          0   Bearish   \n",
      "119  https : //www.tomshardware.com/news/intel-orde...         11   Bearish   \n",
      "120  Your daily News digest https : //wsfriend.com/...        769      None   \n",
      "121  If beat em steal NXPI .https : //www.tomshardw...         30      None   \n",
      "122  LiquidTheta® Trade Alert ( Delayed/Actionable ...       1440      None   \n",
      "123       kdsdawg Keep coming Post page reference NLST        143   Bullish   \n",
      "124  .Every patents enforced Many times patents stolen        197      None   \n",
      "\n",
      "     raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "110       -0.7579          -1.0               0.0                0        No  \n",
      "111       -0.7579          -1.0               0.0                0        No  \n",
      "112        0.0000           0.0               0.0                0        No  \n",
      "113        0.0000           0.0               0.0                0        No  \n",
      "114        0.4497           1.0               1.0                0        No  \n",
      "115       -0.2960          -1.0               0.0                0        No  \n",
      "116       -0.4215          -1.0              -1.0                0        No  \n",
      "117        0.2023           1.0               0.0                0        No  \n",
      "118       -0.5848          -1.0              -1.0                0        No  \n",
      "119        0.0000           0.0              -1.0                0        No  \n",
      "120        0.0000           0.0               0.0                0        No  \n",
      "121       -0.4939          -1.0               0.0                0        No  \n",
      "122        0.4215           1.0               0.0                0        No  \n",
      "123        0.2732           1.0               1.0                0        No  \n",
      "124       -0.5367          -1.0               0.0                0        No  \n",
      "\n",
      "AFTER DROP: \n",
      " (array([], dtype=int64),) \n",
      "\n",
      "    symbol            created_at  \\\n",
      "110   INTC  2021-03-03T13:38:58Z   \n",
      "111   INTC  2021-03-03T13:28:48Z   \n",
      "112   INTC  2021-03-03T13:06:30Z   \n",
      "113   INTC  2021-03-03T12:33:43Z   \n",
      "114   INTC  2021-03-03T12:26:41Z   \n",
      "115   INTC  2021-03-03T12:26:22Z   \n",
      "116   INTC  2021-03-03T11:59:19Z   \n",
      "117   INTC  2021-03-03T11:53:16Z   \n",
      "118   INTC  2021-03-03T11:41:57Z   \n",
      "119   INTC  2021-03-02T21:46:58Z   \n",
      "120   INTC  2021-03-02T21:43:00Z   \n",
      "121   INTC  2021-03-02T21:35:49Z   \n",
      "122   INTC  2021-03-02T21:32:28Z   \n",
      "123   INTC  2021-03-02T21:21:17Z   \n",
      "124   INTC  2021-03-02T21:07:09Z   \n",
      "\n",
      "                                                  body  followers sentiment  \\\n",
      "110  To Pay 2.18B Penalty To VLSI Tech For Patent I...       1819      None   \n",
      "111  To Pay 2.18B Penalty To VLSI Tech For Patent I...      10423      None   \n",
      "112                premarket signals blood bath coming        263      None   \n",
      "113  SPY TGT SNAP UVXY A word quot fighting trends ...       4272      None   \n",
      "114  likely knew sued patent infringement knew bene...          2   Bullish   \n",
      "115  In recent reporting quarter : 7.44 institution...       1788      None   \n",
      "116  apple 5nm chips struggling launching 10nm micr...          3   Bearish   \n",
      "117  2 billion fine litigated repeatedly end paying...        223      None   \n",
      "118  bad Zen 5 track .. Zen 4 completed.Zen 5 compl...          0   Bearish   \n",
      "119  https : //www.tomshardware.com/news/intel-orde...         11   Bearish   \n",
      "120  Your daily News digest https : //wsfriend.com/...        769      None   \n",
      "121  If beat em steal NXPI .https : //www.tomshardw...         30      None   \n",
      "122  LiquidTheta® Trade Alert ( Delayed/Actionable ...       1440      None   \n",
      "123       kdsdawg Keep coming Post page reference NLST        143   Bullish   \n",
      "124  .Every patents enforced Many times patents stolen        197      None   \n",
      "\n",
      "     raw_compound  compound_bin  sentiment_number  modified_rating modified?  \n",
      "110       -0.7579          -1.0               0.0                0        No  \n",
      "111       -0.7579          -1.0               0.0                0        No  \n",
      "112        0.0000           0.0               0.0                0        No  \n",
      "113        0.0000           0.0               0.0                0        No  \n",
      "114        0.4497           1.0               1.0                0        No  \n",
      "115       -0.2960          -1.0               0.0                0        No  \n",
      "116       -0.4215          -1.0              -1.0                0        No  \n",
      "117        0.2023           1.0               0.0                0        No  \n",
      "118       -0.5848          -1.0              -1.0                0        No  \n",
      "119        0.0000           0.0              -1.0                0        No  \n",
      "120        0.0000           0.0               0.0                0        No  \n",
      "121       -0.4939          -1.0               0.0                0        No  \n",
      "122        0.4215           1.0               0.0                0        No  \n",
      "123        0.2732           1.0               1.0                0        No  \n",
      "124       -0.5367          -1.0               0.0                0        No  \n"
     ]
    }
   ],
   "source": [
    "dftest = getData(relevant_path + '/' + filename_output)\n",
    "\n",
    "print(relevant_path + '/' + filename_output)\n",
    "print('csv file read into df to see if all of the empty records are removed.')\n",
    "\n",
    "empty = np.where(pd.isnull(dftest['body'])) #checks to see if there are any empty records in the column 'body'; empty is a tuple where the first element is the array, the second is dtype of the array\n",
    "if empty[0].size == 0:\n",
    "    print('There are no empty records: \\n', empty)\n",
    "else:\n",
    "    print('There are empty records: \\n', empty, '\\n')\n",
    "    \n",
    "print(dftest.iloc[110:125,])\n",
    "\n",
    "dftest.dropna(subset=['body'], inplace=True) #drops empty body records\n",
    "dftest = dftest.reset_index(drop = True) # resets the index\n",
    "\n",
    "empty = np.where(pd.isnull(dftest['body'])) #checks to see if there are any empty records in the column 'body'; empty is a tuple where the first element is the array, the second is dtype of the array\n",
    "print('\\nAFTER DROP: \\n', empty, '\\n')\n",
    "\n",
    "print(dftest.iloc[110:125,])\n",
    "\n",
    "# removes specific rows and resets the index\n",
    "def remove_empty_body_rows(df):\n",
    "    df.dropna(subset=['body'], inplace=True) #drops empty body records\n",
    "    df = df.reset_index(drop = True) # resets the index\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTC,2021-03-03T11:27:07Z,,228,None,0.0,0.0,0.0,0,No"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
